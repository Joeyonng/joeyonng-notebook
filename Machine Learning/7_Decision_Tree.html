<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Joeyonng’s Notebook - 40&nbsp; Decision Tree (DT)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../Machine Learning/K-means.html" rel="next">
<link href="../Machine Learning/6_Support_Vector_Machine.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="https://joeyonng.github.io/joeyonng/">
    <span class="navbar-title">Joeyonng</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" rel="" target="" aria-current="page">
 <span class="menu-text">Notebook</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://joeyonng.github.io/joeyonng/Notes/" rel="" target="">
 <span class="menu-text">Pages</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://joeyonng.github.io/joeyonng/" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://joeyonng.github.io/joeyonng-backyard/" rel="" target="">
 <span class="menu-text">Backyard</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Machine Learning/1_Linear_Discriminant.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="../Machine Learning/7_Decision_Tree.html"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Decision Tree (DT)</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notations and Facts</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Linear Algebra</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/01_Fields_and_Spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Fields and Spaces</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/02_Vectors_and_Matrices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Vectors and Matrices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/03_Span_and_Linear_Independence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Span and Linear Independence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/04_Basis_and_Dimension.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Basis and Dimension</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/05_Linear_Map_and_Rank.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Map and Rank</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/06_Inner_Product_and_Norm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Inner Product and Norm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/07_Orthogonality_and_Orthogonal_Matrix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Orthogonality and Orthogonal Matrix</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/08_Complementary_Subspaces_and_Projection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Complementary Subspaces and Projection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Orthogonal Complement and Decomposition</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/10_Singular_Value_Decomposition_and_Pseudoinverse.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">SVD and Pseudoinverse</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/11_Orthogonal_and_Affine_Projection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Orthogonal and Affine Projection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/12_Determinants_and_Eigensystems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Determinants and Eigensystems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/13_Similarity_and_Diagonalization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Similarity and Diagonalization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Normal and Positive Definite Matrices</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Probability and Statistics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/01_Probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/02_Random_Variables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/03_Expectation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Expectation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/04_Common_Distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Common Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/05_Moment_Generating_Functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Moment Generating Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/06_Concentration_Inequalities_I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Concentration Inequalities I</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/07_Convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Convergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/08_Limit_Theorems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Limit Theorems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/09_Maximum_Likelihood_Estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/10_Bayesian_Estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Bayesian Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/11_Expectation_Maximization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Expectation-maximization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Probability and Statistics/12_Concentration_Inequalities2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Concentration Inequalities II</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Learning Theory</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/1_Statistical_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/2_Bayesian_Classifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Bayesian Classifier</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/3_Effective_Class_Size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Effective Class Size</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/4_Empirical_Risk_Minimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Empirical Risk Minimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/5_Uniform_Convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Uniform Convergence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/6_PAC_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">PAC Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/7_Rademacher_Complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Rademacher Complexity</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Machine Learning/1_Linear_Discriminant.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Linear Discriminant</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Machine Learning/2_Perceptron.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Perceptron</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Machine Learning/3_Logistic_Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Machine Learning/4_Multi_Layer_Perceptron.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Multi-layer Perceptron</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Machine Learning/5_Boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Machine Learning/6_Support_Vector_Machine.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Support Vector Machine</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Machine Learning/7_Decision_Tree.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Decision Tree (DT)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Machine Learning/K-means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">K-means</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Decision Tree (DT)</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="preliminary" class="level2">
<h2 class="anchored" data-anchor-id="preliminary">Preliminary</h2>
</section>
<section id="tree-basics" class="level2">
<h2 class="anchored" data-anchor-id="tree-basics">Tree basics</h2>
<p>Decision tree is composed of <strong>nodes</strong> and <strong>edges</strong>.</p>
<ul>
<li><p>Each node corresponds to a subset of the original dataset.</p></li>
<li><p>The root node is the original training dataset provided to train the decision tree.</p></li>
<li><p>The path from the root node to a node specifies how the subset of the dataset is partitioned from the original training dataset.</p></li>
</ul>
<p>In the following sections, we will use the following notations.</p>
<ul>
<li><p>A node: <span class="math inline">t</span></p></li>
<li><p>A decision tree is a set of nodes: <span class="math inline">T</span></p></li>
<li><p>The original training set: <span class="math inline">\mathbf{X}</span></p></li>
<li><p>The subset of the dataset that corresponds to node <span class="math inline">t</span>: <span class="math inline">\mathbf{X}_{t}</span></p></li>
</ul>
</section>
<section id="impurity-function" class="level2">
<h2 class="anchored" data-anchor-id="impurity-function">Impurity function</h2>
<p>The <strong>impurity function</strong> <span class="math inline">F (\mathbf{y})</span> measures the impureness of a set of labels <span class="math inline">\mathbf{y}</span>.</p>
<ul>
<li><p><span class="math inline">F (\mathbf{y})</span> achieves maximum when the labels in <span class="math inline">\mathbf{y}</span> are in uniform distribution.</p></li>
<li><p><span class="math inline">F (\mathbf{y})</span> achieves minimum when there is only one unique label in <span class="math inline">\mathbf{y}</span>.</p></li>
</ul>
<p>We use <span class="math inline">F (\mathbf{X})</span> in the following context to compute the impureness of the labels in the dataset <span class="math inline">\mathbf{X}</span> using the impurity function <span class="math inline">F</span>.</p>
<section id="classification" class="level3">
<h3 class="anchored" data-anchor-id="classification">Classification</h3>
<p>Given a dataset <span class="math inline">\mathbf{X}</span> with <span class="math inline">C</span> unique labels, <span class="math inline">\mathbb{P} (c)</span> is the probability of label <span class="math inline">c</span> in the dataset, which is computed by dividing the number of instances with label <span class="math inline">c</span> by the total number instances in <span class="math inline">\mathbf{X}</span>.</p>
<ul>
<li><p>When there is only one class in <span class="math inline">\mathbf{X}</span>, the dataset is pure and thus impurity functions should return 0.</p></li>
<li><p>On the contrary, if all possible labels are in <span class="math inline">\mathbf{X}</span> and the numbers of all unique labels are the same, <span class="math inline">\mathbf{X}</span> achieves the maximum impureness.</p></li>
</ul>
<section id="gini-impurity-index-gini-impurity" class="level4">
<h4 class="anchored" data-anchor-id="gini-impurity-index-gini-impurity">Gini Impurity Index (Gini Impurity)</h4>
<p><strong>Gini impurity</strong> measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.</p>
<p><span class="math display">
\begin{aligned}
F (\mathbf{X})
&amp; = \sum_{c \in C} \mathbb{P} (c) (1 - \mathbb{P} (c))
\\
&amp; = \sum_{c \in C} \mathbb{P} (c) - \mathbb{P} (c)^{2}
\\
&amp; = 1 - \sum_{c \in C} \mathbb{P} (c)^2.
\\
\end{aligned}
</span></p>
</section>
<section id="shannon-entropy-entropy" class="level4">
<h4 class="anchored" data-anchor-id="shannon-entropy-entropy">Shannon Entropy (Entropy)</h4>
<p><strong>Entropy</strong> is defined as the expectation of the log value</p>
<p><span class="math display">
F (\mathbf{X}) = \sum_{c \in C} \mathbb{P} (c) \log \mathbb{P} (c).
</span></p>
<p>Entropy can be thought as the difference measured by KL Divergence between the probability distribution of the unique labels represented in the current dataset <span class="math inline">\mathbf{X}</span> and the distribution of the most impure dataset. Therefore, The larger the entropy, the more far away from a uniform distribution is the distribution of the labels represented by <span class="math inline">\mathbf{X}</span>.</p>
</section>
</section>
<section id="regression" class="level3">
<h3 class="anchored" data-anchor-id="regression">Regression</h3>
<p>Given a dataset <span class="math inline">\mathbf{X}</span> with continuous labels <span class="math inline">\{y_{1}, y_{2}, \dots, y_{n}\}</span>, impurity functions can be defined a similar way.</p>
<ul>
<li><p>If the labels in <span class="math inline">\mathbf{X}</span> are very similar (low variance), the impurity functions should return a value closed to 0.</p></li>
<li><p>If the labels in <span class="math inline">\mathbf{X}</span> are very different from each other (high variance), impurity functions should return a very large value.</p></li>
</ul>
<section id="mean-squared-error" class="level4">
<h4 class="anchored" data-anchor-id="mean-squared-error">Mean squared error</h4>
<p>A dataset’s impurity can be simply measured by the mean squared error.</p>
<p><span class="math display"> F (\mathbf{X}) = \frac{ 1 }{ N } \sum_{i}^{n} (y_{i} - \bar{y})^{2} </span></p>
<p>where <span class="math inline">\bar{y}</span> is the mean value of the labels in dataset <span class="math inline">\mathbf{X}</span>.</p>
</section>
</section>
</section>
<section id="splitting-criteria" class="level2">
<h2 class="anchored" data-anchor-id="splitting-criteria">Splitting criteria</h2>
<p>A <strong>split</strong> is a way that divides a feature space into different groups and is used in the tree building process to split a node to children nodes.</p>
<ul>
<li><p>Binary split (2-way split): split a feature space into 2 groups. A node will have 2 sub-nodes.</p></li>
<li><p>k-way split: split a feature space into k groups. A node will have k sub-nodes.</p></li>
</ul>
<p>The most important question of building a decision tree is how to choose the best split from a set of valid splits to split a node (dataset) into different child nodes (sub-datasets).</p>
<ul>
<li><p>A <strong>splitting criteria</strong> is a function that measures the impurity difference between the dataset before splitting and the datasets after splitting.</p></li>
<li><p>The best split <span class="math inline">s</span> for the node <span class="math inline">t</span> should be the one that has the maximum splitting criteria <span class="math inline">\Delta F(\mathbf{X}_{t}, s)</span>.</p></li>
</ul>
<p>Given a set of datasets <span class="math inline">\mathbf{X}_{1}, \dots, \mathbf{X}_{k}</span> created by applying split <span class="math inline">s</span> to the dataset <span class="math inline">\mathbf{X}_{t}</span>, the splitting criteria is defined as:</p>
<p><span class="math display">
\Delta F (\mathbf{X}_{t}, s) = F (\mathbf{X}_{t}) - \frac{ 1 }{ \lvert \mathbf{X}_{t} \rvert } \sum_{i = 1}^{k} \lvert \mathbf{X}_{i} \rvert F (\mathbf{X}).
</span></p>
<p>If <span class="math inline">F</span> is the entropy function, <span class="math inline">\Delta F(\mathbf{X}, s)</span> is called <strong>Information Gain</strong>.</p>
</section>
<section id="stopping-condition" class="level2">
<h2 class="anchored" data-anchor-id="stopping-condition">Stopping condition</h2>
<p>Each split produces new nodes that recursively become the starting points for new splits.</p>
<ul>
<li><p>A node stops splitting when certain <strong>stopping conditions</strong> are satisfied and such nodes are <strong>leaf nodes</strong>.</p></li>
<li><p>The leaf node doesn’t have children but has a label according to the dataset it corresponds to.</p></li>
</ul>
<p>The basic stopping condition is that the dataset that the leaf node corresponds to has impureness of <span class="math inline">0</span> (single training instance or all training instance have the same label), in which case the splitting stops because there is no need to reduce the impureness.</p>
<section id="early-stopping" class="level3">
<h3 class="anchored" data-anchor-id="early-stopping">Early stopping</h3>
<p>However, always splitting into pureness usually induces overfitting. Thus, there are other stopping conditions that can achieve <strong>early stopping</strong> to avoid overfitting.</p>
<ul>
<li><p>Dataset size is below a threshold.</p></li>
<li><p>Splitting criteria improvement is below a threshold.</p></li>
<li><p>Tree depth is above a threshold.</p></li>
<li><p>Number of nodes is above a threshold.</p></li>
</ul>
</section>
</section>
<section id="label-assignment" class="level2">
<h2 class="anchored" data-anchor-id="label-assignment">Label assignment</h2>
<p>For each node, we can assign a label to the node according to the labels of the dataset it corresponds to.</p>
<ul>
<li><p>Classification: majority label.</p></li>
<li><p>Regression: mean label.</p></li>
</ul>
<p>Every node can have a label assigned, but only leaf nodes actually use labels.</p>
<section id="misclassification-cost" class="level3">
<h3 class="anchored" data-anchor-id="misclassification-cost">Misclassification cost</h3>
<p>Assuming the assigned label of the node <span class="math inline">t</span> is <span class="math inline">y_{t}</span>, the <strong>misclassification cost</strong> <span class="math inline">r(t)</span> of a node <span class="math inline">t</span> for classification is defined as.</p>
<p><span class="math display">
r (t) = 1 - \mathbb{P} (y_{t}),
</span></p>
<p>and for regression is</p>
<p><span class="math display">
r(t) = \frac{1}{N(t)} \sum_{y \in t} (y - y_{t}),
</span></p>
<p>where <span class="math inline">y \in t</span> means all labels in the dataset that node <span class="math inline">t</span> corresponds to.</p>
<p>Then weighted misclassification cost of the node <span class="math inline">t</span> is defined as the product of misclassification cost and the probability of picking an instance that is in the node <span class="math inline">t</span>.</p>
<p><span class="math display">
R (t) = \mathbb{P} (\mathbf{x} \in \mathbf{X}_{t}) r(t) = \frac{\lvert \mathbf{X}_{t} \rvert}{\lvert \mathbf{X} \rvert} r(t).
</span></p>
<p>Then the misclassification cost of the a tree <span class="math inline">T</span> is</p>
<p><span class="math display">
R (T) = \sum_{t \in \hat{T}} R (t),
</span></p>
<p>where <span class="math inline">\hat{T}</span> is the set of the leaf nodes in tree <span class="math inline">T</span>.</p>
<p>The weighted misclassification cost of a node <span class="math inline">t</span> is always higher than the sum of the weighted misclassification costs of the children nodes <span class="math inline">T_{c} = \{t_{1}, t_{2}, \dots, t_{k}\}</span> that <span class="math inline">t</span> splits to</p>
<p><span class="math display">
R (t) \geq \sum_{t_{i} \in T_{c}} R (t_{i}).
</span></p>
<p>Thus, if we split one of the leaf nodes of <span class="math inline">T</span> to get a new and larger tree <span class="math inline">T'</span>,</p>
<p><span class="math display">
R (T) \geq R (T'),
</span></p>
<p>which shows that the misclassification cost of a tree will always decrease or stay the same if we continue to split its leaf nodes.</p>
</section>
</section>
<section id="pruning" class="level2">
<h2 class="anchored" data-anchor-id="pruning">Pruning</h2>
<p>Instead of doing early stopping while building the decision tree to avoid overfitting, another way is to do pruning after the tree has build. <strong>Pruning</strong> is the process to make some internal nodes to be leaf nodes, and remove their children from a sufficiently large tree.</p>
<section id="minimal-cost-complexity-pruning" class="level3">
<h3 class="anchored" data-anchor-id="minimal-cost-complexity-pruning">Minimal cost-complexity pruning</h3>
<p>Previously we show that <span class="math inline">R (T)</span> might not a good measure of the performance of a tree in the sense that it always favors a larger tree. Thus we introduce another metric called <strong>cost-complexity</strong> that also considers the size of the tree.</p>
<ul>
<li><p>If we consider each node has a complexity of <span class="math inline">\alpha</span>, the cost-complexity <span class="math inline">R_{\alpha} (t)</span> of a node <span class="math inline">t</span> is</p>
<p><span class="math display">
  R_{\alpha} (t) = R (t) + \alpha  
  </span></p></li>
<li><p>Thus, the cost-complexity <span class="math inline">R_{\alpha} (T)</span> of a tree <span class="math inline">T</span> is</p>
<p><span class="math display">
  R_{\alpha} (T) = \sum_{t \in \hat{T}} R_{\alpha} (t) = \sum_{t \in \hat{T}} (R (t) + \alpha) = R (T) + \alpha \lvert \hat{T} \rvert,
  </span></p>
<p>where <span class="math inline">\hat{T}</span> is the set of leaf nodes of <span class="math inline">T</span>.</p></li>
</ul>
<p>Cost-complexity can be seen as adding a regularization term that penalize the complexity of the tree to the misclassification cost.</p>
<ul>
<li><p><span class="math inline">\alpha</span> is the regularization parameter that balances the training accuracy and tree complexity.</p></li>
<li><p>Given <span class="math inline">\alpha</span>, the goal of pruning of <span class="math inline">T</span> is to get a pruned tree <span class="math inline">\hat{T}</span> (a subtree of <span class="math inline">T</span> that has the same root with <span class="math inline">T</span>) that minimizes <span class="math inline">R_{\alpha} (\hat{T})</span>.</p></li>
</ul>
</section>
<section id="weakest-link-cutting" class="level3">
<h3 class="anchored" data-anchor-id="weakest-link-cutting">Weakest-link cutting</h3>
<p><strong>Weakest-link cutting</strong> is an efficient way of doing the minimal cost-complexity pruning.</p>
<p>If the tree <span class="math inline">T</span> is pruned by deleting subtree <span class="math inline">T_{t}</span> rooted at the node <span class="math inline">t</span> (replacing tree <span class="math inline">T</span> with node <span class="math inline">t</span>), the cost-complexity difference between pruned tree <span class="math inline">\hat{T}</span> and unpruned tree <span class="math inline">T</span> is</p>
<p><span class="math display">
R_{\alpha} (\hat{T}) - R_{\alpha} (T) = R_{\alpha} (t) - R_{\alpha} (T_{t}).
</span></p>
<ul>
<li><p>If <span class="math inline">\alpha = 0</span>, <span class="math inline">R_{\alpha} (t) - R_{\alpha} (T_{t}) = R (t) - R (T_{t}) \geq 0</span>.</p></li>
<li><p>As <span class="math inline">\alpha</span> becomes larger, <span class="math inline">R_{\alpha} (t) - R_{\alpha} (T_{t})</span> is getting smaller and will eventually becomes <span class="math inline">&lt; 0</span>, since <span class="math inline">\alpha</span> is increasing slower than <span class="math inline">\alpha \lvert \hat{T} \rvert</span>.</p></li>
<li><p>Given a sufficiently large <span class="math inline">\alpha</span>, <span class="math inline">R_{\alpha} (t) - R_{\alpha} (T_{t}) &lt; 0</span>, which means that the cost-complexity of the node <span class="math inline">t</span> is better than its subtree <span class="math inline">T_{t}</span>, and thus <span class="math inline">T_{t}</span> should be pruned.</p></li>
</ul>
<p>Given a tree <span class="math inline">T</span>, the <strong>weakest link</strong> <span class="math inline">\bar{t}</span> is the <em>internal node</em> in <span class="math inline">T</span> that achieves <span class="math inline">R_{\alpha} (\bar{t}) - R_{\alpha} (T_{\bar{t}}) = 0</span> with the smallest <span class="math inline">\alpha</span> value.</p>
<ul>
<li><p>The <span class="math inline">\alpha</span> value that achieves <span class="math inline">R_{\alpha} (t) - R_{\alpha} (T_{t}) = 0</span> can be directly calculated.</p>
<p><span class="math display">
  \begin{aligned}
  R_{\alpha} (t) - R_{\alpha} (T_{t})
  &amp; = 0
  \\
  R (t) + \alpha - (R (T) + \alpha \lvert \hat{T} \rvert)
  &amp; = 0
  \\
  R (t) - R (T) + \alpha (1 + \lvert \hat{T} \rvert)
  &amp; = 0
  \\
  \alpha
  &amp; = \frac{ R (t) - R (T_{t}) }{ \lvert T_{t} \rvert - 1 }
  \\
  \end{aligned}
  </span></p></li>
<li><p>The weakest link is defined as</p>
<p><span class="math display">
  \bar{t} = \arg \min_{t \in T \setminus \hat{T}} \frac{ R (t) - R (T_{t}) }{ \lvert T_{t} \rvert - 1 }
  </span></p>
<p>where <span class="math inline">T \setminus \hat{T}</span> means the set of the internal nodes of <span class="math inline">T</span>.</p></li>
<li><p>If there are more than 1 internal node that achieves <span class="math inline">R_{\alpha} (\bar{t}) - R_{\alpha} (T_{\bar{t}}) = 0</span> with same minimum <span class="math inline">\alpha</span> value, they are all called the weakest links.</p></li>
</ul>
<p>Weakest-link cutting finds the optimal subtree <span class="math inline">\hat{T}</span> of <span class="math inline">T_{max}</span> that minimizes <span class="math inline">R_{\alpha} (\hat{T})</span> with a predefined threshold <span class="math inline">\alpha_{max}</span> in a iterative way.</p>
<ul>
<li><p>We start the pruning process by first removing from <span class="math inline">T_{max}</span> the subtrees <span class="math inline">T_{t}</span> rooted at nodes <span class="math inline">t</span> that have already achieved <span class="math inline">R_{\alpha} (t) - R_{\alpha} (T_{t}) = 0</span>. We denote the resulting tree <span class="math inline">T_{0}</span>.</p></li>
<li><p>In each iteration <span class="math inline">i</span>, the weakest link(s) <span class="math inline">\bar{t}</span> of tree <span class="math inline">T_{i - 1}</span> is identified by calculating</p>
<p><span class="math display">
  \bar{t} = \arg \min_{t \in T_{i - 1} \setminus \hat{T}_{i - 1}} \frac{
      R (t) - R (T_{t})
  }{
      \lvert T_{t} \rvert - 1
  }.
  </span></p>
<p>In the meantime, we can also calculate the <span class="math inline">\alpha_{i}</span> that identifies the weakest links.</p>
<p><span class="math display">
  \alpha_{i} = \min_{t \in T_{i - 1} \setminus \hat{T}_{i - 1}} \frac{
      R (t) - R (T_{t})
  }{
      \lvert T_{t} \rvert - 1
  }.
  </span></p></li>
<li><p>We replace <span class="math inline">T_{\bar{t}}</span> (the subtree rooted at <span class="math inline">\bar{t}</span>) by <span class="math inline">\bar{t}</span> and denote the resulting tree <span class="math inline">T_{i}</span>.</p></li>
<li><p>Continue the iteration until the minimum <span class="math inline">\alpha</span> required to achieve <span class="math inline">R_{\alpha} (\bar{t}) - R_{\alpha} (T_{\bar{t}}) = 0</span> is above a predefined threshold <span class="math inline">\alpha_{max}</span>.</p></li>
</ul>
</section>
</section>
<section id="cart-tree-building" class="level2">
<h2 class="anchored" data-anchor-id="cart-tree-building">CART Tree building</h2>
<section id="identify-all-possible-splits" class="level3">
<h3 class="anchored" data-anchor-id="identify-all-possible-splits">Identify all possible splits</h3>
<p>CART considers binary split of a single feature for each node (each node only splits a one feature and only has 2 children).</p>
<ul>
<li><p>For a categorical feature that has <span class="math inline">k</span> distinct values, CART considers all possible ways to split the <em>k</em> distinct values into 2 groups.</p>
<ul>
<li><p>The maximum ways of splitting is <span class="math inline">2^{k - 1} - 1</span>.</p></li>
<li><p>e.g.&nbsp;If the categorical feature has 4 distinct values: <span class="math inline">\{1, 2, 3, 4\}</span>, then all possible splits are</p>
<table class="table">
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">index</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">left child</td>
<td style="text-align: center;">{1}</td>
<td style="text-align: center;">{2}</td>
<td style="text-align: center;">{3}</td>
<td style="text-align: center;">{4}</td>
<td style="text-align: center;">{1, 2}</td>
<td style="text-align: center;">{1, 3}</td>
<td style="text-align: center;">{1, 4}</td>
</tr>
<tr class="even">
<td style="text-align: center;">right child</td>
<td style="text-align: center;">{2, 3, 4}</td>
<td style="text-align: center;">{1, 3, 4}</td>
<td style="text-align: center;">1, 2, 4}</td>
<td style="text-align: center;">{1, 2, 3}</td>
<td style="text-align: center;">{3, 4}</td>
<td style="text-align: center;">{2, 4}</td>
<td style="text-align: center;">{2, 3}</td>
</tr>
</tbody>
</table></li>
</ul></li>
<li><p>For a numerical feature that has <span class="math inline">k</span> distinct values appeared in the dataset, CART considers all the intervals between 2 consecutive values as the splits.</p>
<ul>
<li><p>The maximum ways of splitting is <span class="math inline">k - 1</span>.</p></li>
<li><p>e.g.&nbsp;If the numerical feature has 6 distinct values: <span class="math inline">\{-5.0, 1.0, 3.0, 5.0, 7.0, 11.0\}</span>, then all possible splits are</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">index</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">left child</td>
<td style="text-align: center;"><span class="math display"> \leq -2.0 </span></td>
<td style="text-align: center;"><span class="math display"> \leq 2.0 </span></td>
<td style="text-align: center;"><span class="math display"> \leq 4.0 </span></td>
<td style="text-align: center;"><span class="math display"> \leq 6.0 </span></td>
<td style="text-align: center;"><span class="math display"> \leq 9.0 </span></td>
</tr>
<tr class="even">
<td style="text-align: center;">right child</td>
<td style="text-align: center;"><span class="math display"> &gt; -2.0 </span></td>
<td style="text-align: center;"><span class="math display"> &gt; 2.0 </span></td>
<td style="text-align: center;"><span class="math display"> &gt; 4.0 </span></td>
<td style="text-align: center;"><span class="math display"> &gt; 6.0 </span></td>
<td style="text-align: center;"><span class="math display"> &gt; 9.0 </span></td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul>
<p>At a given node, CART considers all possible splits of all features and chooses the one that has the maximum splitting criteria.</p>
</section>
<section id="recursive-tree-building" class="level3">
<h3 class="anchored" data-anchor-id="recursive-tree-building">Recursive tree building</h3>
<ol type="1">
<li><p>Identify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either treat it as categorical feature by discretizing it or sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split.</p></li>
<li><p>Calculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split.</p></li>
<li><p>Once a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset.</p></li>
<li><p>The splitting on a node stops when no further splitting can be made (the dataset contains only one class) or satisfies the preset early-stopping conditions.</p></li>
</ol>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li>https://victorzhou.com/blog/intro-to-random-forests/</li>
<li>https://www.math.snu.ac.kr/~hichoi/machinelearning/lecturenotes/CART.pdf</li>
<li>https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote17.html</li>
<li>http://www.odbms.org/wp-content/uploads/2014/07/DecisionTrees.pdf</li>
<li>https://scientistcafe.com/ids/splitting-criteria.html</li>
<li>https://online.stat.psu.edu/stat508/book/export/html/647</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="../Machine Learning/6_Support_Vector_Machine.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Support Vector Machine</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../Machine Learning/K-means.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">K-means</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js" type="text/javascript"></script>
<script type="text/javascript">
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>



</body></html>