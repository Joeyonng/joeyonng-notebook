[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Joeyonng’s Notebook",
    "section": "Welcome",
    "text": "Welcome\nHere is my notebook where I used to share my summaries on general machine learning methods.\nThe goal is to make my notes formal, comprehensive and most importantly, easy-to-read.\nThis repository is far from complete and I will continue to cover more topics as I learn more in my PhD journey.\nFeel free to point any error you found in my notes. Any suggestions will be much appreciated!."
  },
  {
    "objectID": "index.html#update-history",
    "href": "index.html#update-history",
    "title": "Joeyonng’s Notebook",
    "section": "Update History",
    "text": "Update History"
  },
  {
    "objectID": "notations.html#notations",
    "href": "notations.html#notations",
    "title": "Notations and Facts",
    "section": "Notations",
    "text": "Notations\n\nMathematical definitions\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\nx\nScalar\nVariables are scalars (numbers).\n\n\n\\mathbf{x}\nVector\nBold non-capitalized variables are vectors.\n\n\n\\hat{\\mathbf{x}}\nUnit vector\nVectors that have a hat are unit vectors.\n\n\n\\mathbf{X}\nMatrix\nBold capitalized variables are matrices.\n\n\nX\nRandom variable\nCapitalized variables are random variables.\n\n\n\\mathcal{X}\nSet\nCalligraphic variables are sets.\n\n\n\n\n\nMathematical operations\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbf{a} \\cdot \\mathbf{b}\nDot product\nDot product between vector \\mathbf{a} and \\mathbf{b} (same as \\mathbf{a}^{T} \\mathbf{b}).\n\n\n\\mathbf{A}\\mathbf{b}\nMatrix vector product\nMatrix product between matrix \\mathbf{A} and vector \\mathbf{b} (column matrix).\n\n\n\\mathbf{a}^{T}\nVector transpose\nThe transposed vector is a matrix of size 1 \\times d\n\n\n\\mathbf{A}^{T}\nMatrix transpose\nTranspose a matrix.\n\n\n\n\n\nMathematical indexing\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbf{A}_{i, j}\nMatrix element selection\nSelect the scalar at row i and column j of matrix \\mathbf{A}.\n\n\n\\mathbf{A}_{i, *}\nMatrix row selection\nSelect the vector at row i of matrix \\mathbf{A}.\n\n\n\\mathbf{A}_{*, j}\nMatrix column selection\nSelect the vector at column j of matrix \\mathbf{A}.\n\n\n\\mathbf{a}_{i}\nVector element selection\nSelect the scalar at index i of vector \\mathbf{a}.\n\n\n\n\n\nOthers\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbb{1}_{cond}\nConditional operator\nEvaluates to 1 if cond is true, 0 otherwise."
  },
  {
    "objectID": "notations.html#facts",
    "href": "notations.html#facts",
    "title": "Notations and Facts",
    "section": "Facts",
    "text": "Facts\n\nLogarithm\n\nProduct\n \\ln(xy) = \\ln(x) + \\ln(y) \nQuotient\n \\ln \\left( \\frac{x}{y} \\right) = \\ln(x) - \\ln(y) \nLog of power\n \\ln(x^{y}) = y \\ln(x) \nLog reciprocal\n \\ln \\left( \\frac{1}{x} \\right) = \\ln(1) - \\ln(x) = 0 - \\ln(x) = -\\ln(x) \n\n\n\nLinear algebra\n\nThe squared norm of vector \\mathbf{x}\n \\lVert \\mathbf{x} \\rVert^{2} = \\mathbf{x} \\cdot \\mathbf{x} = \\mathbf{x}^{T} \\mathbf{x}\nThe squared norm of a vector difference between \\mathbf{a} and \\mathbf{b}\n \\lVert \\mathbf{a} - \\mathbf{b} \\rVert^{2} = (\\mathbf{a} - \\mathbf{b})^{T} (\\mathbf{a} - \\mathbf{b}) = \\mathbf{a}^{T}\\mathbf{a} - 2 \\mathbf{a}^T\\mathbf{b} + \\mathbf{b}^{T}\\mathbf{b} \nThe matrix form of the dot product between two vectors \\mathbf{a} and \\mathbf{b} with a coefficient \\lambda\n \\lambda(\\mathbf{a} \\cdot \\mathbf{b}) = \\mathbf{a}^{T} \\mathbf{\\Lambda} \\mathbf{b} \nwhere \\mathbf{\\Lambda} is a diagonal matrix with value \\lambda.\nThe transpose of the product of two matrices \\mathbf{A} and \\mathbf{B}\n (\\mathbf{A}\\mathbf{B})^{T} = \\mathbf{B}^{T}\\mathbf{A}^{T} \n\n\n\nCalculus\n\nThe derivative of the sigmoid function \\sigma is \\sigma (1 - \\sigma)\n\n\\begin{aligned}\n\\frac{\\partial \\sigma}{\\partial x} & = \\frac{\\partial}{\\partial x} \\frac{1}{1 + e^{-x}} \\\\\n& = \\frac{\\partial}{\\partial x} (1 + e^{-x})^{-1} \\\\\n& = -(1 + e^{-x})^{-2} \\times -e^{-x} \\\\\n& = \\frac{e^{-x}}{(1 + e^{-x})^{2}} \\\\\n& = \\frac{e^{-x}}{1 + e^{-x}} \\frac{1}{1 + e^{-x}} \\\\\n& = \\frac{e^{-x}}{1 + e^{-x}} \\left( \\frac{1 + e^{-x}}{1 + e^{-x}} - \\frac{e^{-x}}{1 + e^{-x}} \\right) \\\\\n& = \\frac{e^{-x}}{1 + e^{-x}} \\left( 1 - \\frac{e^{-x}}{1 + e^{-x}} \\right) \\\\\n& = \\sigma (1 - \\sigma) \\\\\n\\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html",
    "href": "Linear Algebra/01_Fields_and_Spaces.html",
    "title": "1  Fields and Spaces",
    "section": "",
    "text": "2 Fields\nA field is a set \\mathbb{F}, equipped with two operations addition + and multiplication \\cdot, obeying the rules (axioms) listed below.\nA vector space, defined over a field \\mathbb{F}, is a non-empty set \\mathcal{V} (whose members are called vectors), equipped with two operations: vector addition + and scalar multiplication \\cdot, obeying the rules (axioms) listed below.\nGiven a vector space \\mathcal{V} over a field \\mathbb{F}, a subspace \\mathcal{W} of \\mathcal{V} is a non-empty subset of \\mathcal{V} (\\mathcal{W} \\subseteq \\mathcal{V}) that follows the closure axioms.\nFor all u and v in the subspace \\mathcal{W} (\\forall u, v \\in \\mathcal{W}) and for all \\alpha in the field \\mathbb{F} (\\forall \\alpha \\in \\mathbb{F}), we have\nu + v \\in \\mathcal{W},\n\\alpha \\cdot v \\in \\mathcal{W}."
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#axioms-of-fields",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#axioms-of-fields",
    "title": "1  Fields and Spaces",
    "section": "2.1 Axioms of fields",
    "text": "2.1 Axioms of fields\nFor all x, y, and z in the field \\mathbb{F} (\\forall x, y, z \\in \\mathbb{F}), we have:\n\nClosure under addition and multiplication:\n\n  x + y \\in \\mathbb{F},\n  \n\n  x \\cdot y \\in \\mathbb{F}.\n  \nCommutativity of addition and multiplication:\n\n  x + y = y + x,\n  \n\n  x \\cdot y = y \\cdot x.\n  \nAssociativity of addition and multiplication:\n\n  (x + y) + z = x + (y + z),\n  \n\n  (x \\cdot y) \\cdot z = x \\cdot (y \\cdot z).\n  \nDistributive property of multiplication:\n\n  (x + y) \\cdot z = x \\cdot z + y \\cdot z.\n  \nThere is an element in \\mathbb{F} called “zero” 0 \\in \\mathbb{F} such that\n\n  x + 0 = x,\n  \nand there is another element in \\mathbb{F} called “one” 1 \\in \\mathbb{F}, 1 \\neq 0, such that\n\n  x \\cdot 1 = x.\n  \nFor each x \\in \\mathbb{F}, there is an element in \\mathbb{F} called addictive inverse x_{I} of x such that\n\n  x + x_{I} = 0,\n  \nand if x \\neq 0, there is an element in \\mathbb{F} called multiplicative inverse x^{-1} of x such that\n\n  x \\cdot x^{-1} = 1."
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#properties-of-fields",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#properties-of-fields",
    "title": "1  Fields and Spaces",
    "section": "2.2 Properties of fields",
    "text": "2.2 Properties of fields\n\nZero and one are unique: there is only one “zero” and one “one” in any field \\mathbb{F}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove “zero” is unique by contradiction.\nSuppose there are two different “zero”s 0_{0} and 0_{1}.\nDue to the definition of “zero”,\n\n  \\begin{aligned}\n  0_{1} + 0_{0}\n  & = 0_{1}\n  & [0_{0} \\text{ is \"zero\"}]\n  \\\\\n  0_{0} + 0_{1}\n  & = 0_{0}\n  & [0_{1} \\text{ is \"zero\"}]\n  \\\\\n  \\end{aligned}\n  \nDue to the Commutativity axiom,\n\n  \\begin{aligned}\n  0_{1} + 0_{0}\n  & = 0_{0} + 0_{1}\n  \\\\\n  0_{1}\n  & = 0_{0}\n  \\\\\n  \\end{aligned}\n  \nwhich contradicts to the fact that 0_{0} and 0_{1} (1_{0} and 1_{1}) are different. Thus, there is only a unique “zero” in \\mathbb{F}.\nThe proof that the “one” in any \\mathbb{F} is unique is the same as above by replacing every addition + with multiplication \\cdot and 0_{0}, 0_{1} with 1_{0}, 1_{1}.\n\n\n\nAddictive and multiplicative inverse of every element are unique: there is only one addictive inverse and multiplicative inverse of every element (other than “zero” for multiplicative inverse) in any field \\mathbb{F}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the addictive inverse is unique by contradiction.\nSuppose there are two different addictive inverses of x: x_{1} and x_{2}.\nBy following the definition of addictive inverse,\n\n  (x + x_{1}) + x_{2} = 0 + x_{2} = x_{2}.\n  \nAlso,\n\n  \\begin{aligned}\n  (x + x_{1}) + x_{2}\n  & = x_{2}\n  \\\\\n  x + (x_{1} + x_{2})\n  & = x_{2}\n  & [\\text{associativity}]\n  \\\\\n  x + (x_{2} + x_{1})\n  & = x_{2}\n  & [\\text{commutativity}]\n  \\\\\n  (x + x_{2}) + x_{1}\n  & = x_{2}\n  & [\\text{associativity}]\n  \\\\\n  x_{1}\n  & = x_{2}\n  \\end{aligned}\n  \nwhich contradicts to the fact that x_{1} and x_{2} are different. Thus, the addictive inverse of every element in any field is unique.\nThe proof that the multiplicative inverse of every element in any field is unique is the same as above by replacing every addition + with multiplication \\cdot.\n\n\n\nFor every x in \\mathbb{F}, x \\cdot 0 = 0\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider\n\n  \\begin{aligned}\n  x \\cdot 0 + x \\cdot 0\n  & = x \\cdot (0 + 0)\n  & [\\text{distributive property}]\n  \\\\\n  & = x \\cdot 0\n  & [\\text{definition of } 0]\n  \\end{aligned}\n  \nBy the definition of addictive inverse, there must exist an addictive inverse y of x \\cdot 0 in \\mathbb{F} such that\n\n  \\begin{aligned}\n  x \\cdot 0 + y\n  & = 0\n  \\\\\n  (x \\cdot 0 + x \\cdot 0) + y\n  & = 0\n  \\\\\n  x \\cdot 0 + (x \\cdot 0 + y)\n  & = 0\n  & [\\text{associativity}]\n  \\\\\n  x \\cdot 0 + 0\n  & = 0\n  & [\\text{addictive inverse}]\n  \\\\\n  x \\cdot 0\n  & = 0\n  & [\\text{definition of 0}]\n  \\end{aligned}\n  \n\n\n\nx_{I} = 1_{I} \\cdot x\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n  \\begin{aligned}\n  (x + x_{I}) + 1_{I} \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  (x_{I} + x) + 1_{I} \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + (x + 1_{I} \\cdot x)\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + (1 \\cdot x + 1_{I} \\cdot x)\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + (1  + 1_{I}) \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + 0 \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + 0\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I}\n  & = 1_{I} \\cdot x\n  \\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#axioms-of-vector-spaces",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#axioms-of-vector-spaces",
    "title": "1  Fields and Spaces",
    "section": "3.1 Axioms of vector spaces",
    "text": "3.1 Axioms of vector spaces\nFor all u, v, and w in the vector space \\mathcal{V} (\\forall u, v, w \\in \\mathcal{V}) and for all \\alpha and \\beta in the field \\mathbb{F}, we have\n\nClosure under vector addition and scalar multiplication\n\n\nu + v \\in \\mathcal{V},\n\n\n\\alpha \\cdot v \\in \\mathcal{V}.\n\n\nCommutativity of vector addition:\n\n  u + v = v + u.\n  \nNote that there is no requirement of the commutativity of scalar multiplication in the definition of the vector space.\nAssociativity of vector addition and scalar multiplication:\n\n  (u + v) + \\mathbf{w} = u + (v + \\mathbf{w}),\n  \n\n  (\\alpha \\cdot \\beta) \\cdot v = \\alpha \\cdot (\\beta \\cdot v).\n  \nNote that in the left hand side of the second equation, the first dot is field multiplication while the second one is the scalar multiplication, but the in the right hand side both dots are scalar multiplication.\nDistributive property of scalar multiplication:\n\n  \\alpha \\cdot (u + v) = \\alpha \\cdot u + \\alpha \\cdot v,\n  \n\n  (\\alpha + \\beta) \\cdot v = \\alpha \\cdot v + \\beta \\cdot v.\n  \nThere is an element in \\mathcal{V} called “zero” vector 0 \\in \\mathcal{V} such that\n\n  v + 0 = v,\n  \nand the definition of “one” in the field 1 \\in \\mathbb{F} is applied in the vector space\n\n  1 \\cdot v = v.\n  \nNote that there is no requirement for the existence of “one” vector in the definition of the vector space.\nFor each v \\in \\mathcal{V}, there is an element in \\mathcal{V} called addictive inverse vector v_{I} of v such that\n\n  v + v_{I} = 0.\n  \nNote that there is no requirement for the scalar multiplicative inverse in the definition of the vector space."
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#linear-combination",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#linear-combination",
    "title": "1  Fields and Spaces",
    "section": "3.2 Linear combination",
    "text": "3.2 Linear combination\nLet \\mathcal{V} be a vector space over the field \\mathbb{F}. Given a set of vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} and a set of field elements \\alpha_{1}, \\dots, \\alpha_{n} \\in \\mathbb{F}, the vector u is a linear combination of v_{1}, \\dots, v_{n} with \\alpha_{1}, \\dots, \\alpha_{n} as coefficients if\n\nu = \\sum_{i=1}^{n} \\alpha_{i} v_{i}."
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#properties-of-subspace",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#properties-of-subspace",
    "title": "1  Fields and Spaces",
    "section": "4.1 Properties of subspace",
    "text": "4.1 Properties of subspace\nLet \\mathcal{U} and \\mathcal{V} be the subspaces of a vector space over \\mathbb{F}.\n\n0 \\in \\mathcal{W}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the subspace \\mathcal{W} cannot be empty, there is at least an element v \\in \\mathcal{W}.\nSince 1 \\in \\mathbb{F} and 1_{I} \\in \\mathbb{F},\n\n  1_{I} \\cdot v = v_{I} \\in \\mathcal{W},\n  \naccording to the axiom of the closure under scalar multiplication.\nAccording to the axiom of the closure under vector addition,\n\n  v + v_{I} = 0 \\in \\mathcal{W}.\n  \nThus, “zero” vector must be in \\mathcal{W}.\n\n\n\n\\mathcal{W} is also a vector space.\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy virtue of the closure axioms, all axioms of the vector space are obeyed by \\mathcal{W}.\nSince all elements in \\mathcal{W} are also in the vector space \\mathcal{V}, the axioms of\n\nclosure\ncommutativity\nassociativity\ndistributive property\n1 \\cdot v = v\n\nin \\mathcal{W} follow directly from those in \\mathcal{V}\nThe existence of “zero” vector and addictive inverse are proved in the proof above.\n\n\n\nThe subspace\n\n  \\mathcal{W} + \\mathcal{U} = \\left\\{\n      w + u \\mid w \\in \\mathcal{W}, u \\in \\mathcal{U}\n  \\right\\}\n  \nis also a subspace.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince 0 \\in \\mathcal{W} and 0 \\in \\mathcal{U}, then 0 \\in \\mathcal{W} + \\mathcal{U}. Thus, \\mathcal{W} + \\mathcal{U} is non-empty.\nSuppose a, b \\in \\mathcal{W} + \\mathcal{U}, then there exists elements a_{1} \\in \\mathcal{W}, a_{2} \\in \\mathcal{U} such that a = a_{1} + a_{2} and b_{1} \\in \\mathcal{W}, b_{2} \\in \\mathcal{U} such that b = b_{1} + b_{2}.\nBecause of closure under addition,\n\n  a_{1} + b_{1} \\in \\mathcal{W},\n  \n\n  a_{2} + b_{2} \\in \\mathcal{U}.\n  \nThus, according to the definition of the set addition\n\n  a + b = (a_{1} + b_{1}) + (a_{2} + b_{2}) \\in \\mathcal{W} + \\mathcal{U}.\n  \nAgain, suppose x \\in \\mathcal{W} + \\mathcal{U}, then there exists elements x_{1} \\in \\mathcal{W}, x_{2} \\in \\mathcal{U} such that x = x_{1} + x_{2}.\nBecause of closure under scalar multiplication,\n\n  \\alpha \\cdot x_{1} \\in \\mathcal{W} \\quad \\forall \\alpha \\in \\mathbb{F},\n  \n\n  \\alpha \\cdot x_{2} \\in \\mathcal{U} \\quad \\forall \\alpha \\in \\mathbb{F}.\n  \nThus, according to the definition of the set addition\n\n  \\alpha \\cdot x = \\alpha \\cdot x_{1} + \\alpha \\cdot x_{2} \\in \\mathcal{W} + \\mathcal{U} \\quad \\forall \\alpha \\in \\mathbb{F}.\n  \nThus, \\mathcal{W} + \\mathcal{U} is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n\n\n\nThe subspace\n\n  \\mathcal{W} \\cap \\mathcal{U} = \\left\\{\n      v \\mid v \\in \\mathcal{W}, v \\in \\mathcal{U}\n  \\right\\}\n  \nis also a subspace.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince 0 \\in \\mathcal{W} and 0 \\in \\mathcal{U}, then 0 \\in \\mathcal{W} \\cap \\mathcal{U}. Thus, \\mathcal{W} \\cap \\mathcal{U} is non-empty.\nSuppose a, b \\in \\mathcal{W} \\cap \\mathcal{U}, then because of the closure axiom of the subspaces\n\n  a + b \\in \\mathcal{W},\n  \\\\\n  a + b \\in \\mathcal{U}.\n  \nThus, by definition,\n\n  a + b \\in \\mathcal{W} \\cap \\mathcal{U}.\n  \nAgain, suppose x \\in \\mathcal{W} \\cap \\mathcal{U}, then because of the closure axiom of the subspaces,\n\n  \\alpha \\cdot x \\in \\mathcal{W}, \\forall \\alpha \\in \\mathbb{F},\n  \\\\\n  \\alpha \\cdot x \\in \\mathcal{U}, \\forall \\alpha \\in \\mathbb{F}.\n  \nThus, by definition, \\alpha \\cdot x \\in \\mathcal{W} \\cap \\mathcal{U}, \\forall \\alpha \\in \\mathbb{F}.\nThus, \\mathcal{W} \\cap \\mathcal{U} is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n\n\n\nThe subspace\n\n  \\mathcal{W} \\cup \\mathcal{U} = \\left\\{\n      v \\mid v \\in \\mathcal{W} \\mathop{\\text{or}} v \\in \\mathcal{U}\n  \\right\\}\n  \nis a subspace if and only if \\mathcal{W} \\subseteq \\mathcal{U} or \\mathcal{U} \\subseteq \\mathcal{W}\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst we prove if \\mathcal{U} \\subseteq \\mathcal{W}, then \\mathcal{U} \\cup \\mathcal{W} is a subspace.\nSince \\mathcal{U} \\subseteq \\mathcal{W},\n\n  \\mathcal{U} \\cup \\mathcal{W} = \\mathcal{W}\n  \nand thus is a subspace.\nThe same argument can be made for \\mathcal{W} \\subseteq \\mathcal{U}.\nThen we prove if \\mathcal{U} \\cup \\mathcal{W} is a subspace, then one subspace is contained in the other by contradiction.\nSuppose \\mathcal{U} \\cup \\mathcal{W} is a subspace, but \\mathcal{U} \\nsubseteq \\mathcal{W} and \\mathcal{W} \\nsubseteq \\mathcal{U}. This means there exists at least two vectors u and w such that\n\n  u \\in \\mathcal{U}, u \\notin \\mathcal{W}\n  \n\n  w \\in \\mathcal{W}, w \\notin \\mathcal{U}.\n  \nSince u, w \\in \\mathcal{U} \\cup \\mathcal{W} and closure under addition property, the vector\n\n  v = u + w\n  \nis also in \\mathcal{U} \\cup \\mathcal{W}, which means it must also be in \\mathcal{U} or \\mathcal{W}.\nIf v \\in \\mathcal{U}, because there must exist an addictive inverse of u \\in \\mathcal{U}, then\n\n  \\begin{aligned}\n  v\n  & = u + w\n  \\\\\n  v + u_{I}\n  & = u + u_{I} + w\n  \\\\\n  w\n  & = v + u_{I}\n  \\\\\n  \\end{aligned}\n  \nwhich shows that w \\in \\mathcal{U} and contradicts to our assumption.\nThe same contradiction can also be found when v \\in \\mathcal{W}."
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#example-subspaces-of-2-dimensional-real-value-column-vectors",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#example-subspaces-of-2-dimensional-real-value-column-vectors",
    "title": "1  Fields and Spaces",
    "section": "4.2 Example: subspaces of 2-dimensional real-value column vectors",
    "text": "4.2 Example: subspaces of 2-dimensional real-value column vectors\nThe 2-dimensional real-value column vector space \\mathbb{R}^{2} has 3 types of subspaces\n\nThe subspace of the zero vector only,\n\n\\mathcal{W} = \\left\\{ 0 \\right\\}.\n\nThe subspace of the vector space itself,\n\n\\mathcal{W} = \\mathbb{R}^{2}.\n\nAny “line” that goes through zero vector"
  },
  {
    "objectID": "Linear Algebra/02_Vectors_and_Matrices.html",
    "href": "Linear Algebra/02_Vectors_and_Matrices.html",
    "title": "2  Vectors and Matrices",
    "section": "",
    "text": "3 Vectors\nLet \\mathbb{F} be a field. The vector space \\mathbb{F}^{n} is defined as the set of all tuples (ordered lists) that have n field elements of \\mathbb{F}\n\\mathbb{F}^{n} =\n\\left\\{\n\\begin{bmatrix}\n    x_{1} \\\\\n    \\vdots \\\\\n    x_{n} \\\\\n\\end{bmatrix},\nx_{i} \\in \\mathbb{F}\n\\right\\},\nwhere each element member is called a n dimensional column vector.\nLet \\mathbb{F} be a field. The vector space \\mathbb{F}^{m \\times n} is defined as the set of all tables that have m rows and n columns of field elements of \\mathbb{F}\n\\mathbb{F}^{m \\times n} =\n\\left\\{\n\\begin{bmatrix}\n    x_{1, 1}, & \\dots & x_{1, n} \\\\\n    \\vdots, & \\dots & \\vdots \\\\\n    x_{m, 1}, & \\dots & x_{m, n} \\\\\n\\end{bmatrix},\nx_{i, j} \\in \\mathbb{F}\n\\right\\},\nwhere each element member is called m \\times n dimensional matrix.\nThe definition of vector addition, scalar multiplication, zero, and addictive inverse are the same as n dimensional column vectors.\n(matrix-vector-multiplication)=\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}, the matrix-vector multiplication\n\\mathbf{y} = \\mathbf{A} \\mathbf{x} =\n\\begin{bmatrix}\n    y_{1} = a_{1, 1} \\cdot x_{1} + \\dots a_{1, n} \\cdot x_{n} \\\\\n    \\vdots \\\\\n    y_{m} = a_{m, 1} \\cdot x_{1} + \\dots a_{m, n} \\cdot x_{n} \\\\\n\\end{bmatrix}\nis a function that maps from vector space x \\in \\mathbb{F}^{n} to y \\in \\mathbb{F}^{m}.\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}, we can define several subspaces of \\mathbb{F}^{n} or \\mathbb{F}^{m} using the matrix-vector multiplication of matrix \\mathbf{A}."
  },
  {
    "objectID": "Linear Algebra/02_Vectors_and_Matrices.html#matrix-vector-multiplication",
    "href": "Linear Algebra/02_Vectors_and_Matrices.html#matrix-vector-multiplication",
    "title": "2  Vectors and Matrices",
    "section": "4.1 Matrix-vector multiplication",
    "text": "4.1 Matrix-vector multiplication\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}, the matrix-vector multiplication is a function that maps from vector space \\mathbf{x} \\in \\mathbb{F}^{n} to \\mathbf{y} \\in \\mathbb{F}^{m}\n\n\\mathbf{y} = \\mathbf{A} \\mathbf{x} =\n\\begin{bmatrix}\n    y_{1} = a_{1, 1} \\cdot x_{1} + \\dots a_{1, n} \\cdot x_{n} \\\\\n    \\vdots \\\\\n    y_{m} = a_{m, 1} \\cdot x_{1} + \\dots a_{m, n} \\cdot x_{n} \\\\\n\\end{bmatrix}.\n\nIf we view \\mathbf{A} as n columns of \\mathbb{F}^{m} vectors,\n\n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{a}_{1} & \\dots & \\mathbf{a}_{n} \\\\\n\\end{bmatrix}\n\nthe vector \\mathbf{y} can be interpreted as the linear combinations of columns of \\mathbf{A} with elements of \\mathbf{x} as coefficients\n\n\\mathbf{y} = \\sum_{i=1}^{n} x_{i} \\mathbf{a}_{i}.\n\n(matrix-multiplication)="
  },
  {
    "objectID": "Linear Algebra/02_Vectors_and_Matrices.html#matrix-multiplication",
    "href": "Linear Algebra/02_Vectors_and_Matrices.html#matrix-multiplication",
    "title": "2  Vectors and Matrices",
    "section": "4.2 Matrix multiplication",
    "text": "4.2 Matrix multiplication\nGiven two matrices \\mathbf{A} \\in \\mathbb{F}^{m \\times n} and \\mathbf{B} \\in \\mathbb{F}^{n \\times r}, the matrix multiplication is a function that applies vector matrix multiplication on the matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n} and the vector \\mathbf{b}_{i} \\in \\mathbb{F}^{n} (ith column of \\mathbf{B})\n\n\\mathbf{C} = \\mathbf{A} \\mathbf{B} =\n\\begin{bmatrix}\n\\mathbf{c}_{1} = \\mathbf{A} \\mathbf{b}_{1} & \\dots & \\mathbf{c}_{r} = \\mathbf{A} \\mathbf{b}_{r} \\\\\n\\end{bmatrix},\n\nwhich results in a vector \\mathbf{c}_{i} \\in \\mathbb{F}^{m} as the ith column of the \\mathbf{C}.\n\nThe column i of \\mathbf{C} is a linear combination of columns of \\mathbf{A} using the elements of the column i of \\mathbf{B} as coefficients.\nThe row i of \\mathbf{C} is a linear combination of rows of \\mathbf{B} using the elements of row i of \\mathbf{A} as coefficients."
  },
  {
    "objectID": "Linear Algebra/02_Vectors_and_Matrices.html#null-space",
    "href": "Linear Algebra/02_Vectors_and_Matrices.html#null-space",
    "title": "2  Vectors and Matrices",
    "section": "5.1 Null space",
    "text": "5.1 Null space\nThe null space of the matrix \\mathbf{A} is the set\n\nN (\\mathbf{A}) = \\left\\{\n    \\mathbf{x} \\in \\mathbb{F}^{n} \\mid \\mathbf{A} \\mathbf{x} = \\mathbf{0} \\in \\mathbb{F}^{m}\n\\right\\},\n\nwhich is the set of the vectors in \\mathbb{F}^{n} that is mapped to 0 \\in \\mathbb{F}^{m} by matrix \\mathbf{A}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathbf{A} 0 = 0 \\in \\mathbb{F}^{n}, \\mathbf{x} = 0 \\in N (A). Thus N (A) is not empty.\nConsider \\mathbf{x}_{1}, \\mathbf{x}_{2} \\in N (A). According to the definition of the N (A), \\mathbf{A} \\mathbf{x}_{1} = 0 and \\mathbf{A} \\mathbf{x}_{2} = 0.\nThen,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x}_{1} + \\mathbf{A} \\mathbf{x}_{2}\n& = 0\n\\\\\n\\mathbf{A} (\\mathbf{x}_{1} + \\mathbf{x}_{2})\n& = 0.\n\\end{aligned}\n\nThus N (\\mathbf{A}) is closed under vector addition\n\n\\mathbf{x}_{1} + \\mathbf{x}_{2} \\in N (\\mathbf{A}).\n\nAlso, for all \\alpha \\in \\mathbb{F},\n\n\\begin{aligned}\n\\alpha \\cdot \\mathbf{A} \\mathbf{x}_{1}\n& = \\alpha \\cdot 0\n\\\\\n\\mathbf{A} (\\alpha \\cdot \\mathbf{x}_{1})\n& = 0.\n\\end{aligned}\n\nThus N (\\mathbf{A}) is closed under scalar multiplication\n\n\\alpha \\cdot \\mathbf{x}_{1} \\in N (\\mathbf{A}).\n\nThus, N (\\mathbf{A}) is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace."
  },
  {
    "objectID": "Linear Algebra/02_Vectors_and_Matrices.html#range-image-space",
    "href": "Linear Algebra/02_Vectors_and_Matrices.html#range-image-space",
    "title": "2  Vectors and Matrices",
    "section": "5.2 Range (image) space",
    "text": "5.2 Range (image) space\nThe range (image) space of the matrix \\mathbf{A} is the set\n\nR (\\mathbf{A}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\mathbf{A} \\mathbf{x}, \\forall \\mathbf{x} \\in \\mathbb{F}^{n}\n\\right\\},\n\nwhich is the set of vectors in \\mathbb{F}^{m} that can be mapped from \\mathbb{F}^{n} by matrix \\mathbf{A}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathbf{x} = 0 \\in \\mathbb{F}^{n}, \\mathbf{y} = 0 \\in R (A). Thus R (A) is not empty.\nConsider \\mathbf{y}_{1}, \\mathbf{y}_{2} \\in R (A). According to the definition of the R (A), there exists an \\mathbf{x}_{1} \\in \\mathbb{F}^{m} for \\mathbf{y}_{1} and an \\mathbf{x}_{2} \\in \\mathbb{F}^{m} for \\mathbf{y}_{2}.\nThen,\n\n\\begin{aligned}\n\\mathbf{y}_{1} + \\mathbf{y}_{2}\n& = \\mathbf{A} \\mathbf{x}_{1} + \\mathbf{A} \\mathbf{x}_{2}\n\\\\\n& = \\mathbf{A} (\\mathbf{x}_{1} + \\mathbf{x}_{2})\n\\end{aligned}\n\nSince by the closure under addition property,\n\n\\mathbf{x}_{1} + \\mathbf{x}_{2} \\in \\mathbb{F}^{m}\n\nand thus R (\\mathbf{A}) is closed under vector addition\n\n\\mathbf{y}_{1} + \\mathbf{y}_{2} \\in R (\\mathbf{A}).\n\nAlso, for all \\alpha \\in \\mathbb{F},\n\n\\begin{aligned}\n\\alpha \\cdot \\mathbf{y}_{1}\n& = \\alpha \\cdot \\mathbf{A} \\mathbf{x}_{1}\n\\\\\n& = \\mathbf{A} (\\alpha \\cdot \\mathbf{y}_{1})\n\\end{aligned}\n\nAgain, since by the closure under scalar multiplication property,\n\n\\alpha \\cdot \\mathbf{x}_{1} \\in \\mathbb{F}^{m}, \\forall \\alpha \\in \\mathbb{F},\n\nand thus R (\\mathbf{A}) is closed under scalar multiplication\n\n\\alpha \\cdot \\mathbf{y}_{1} \\in R (\\mathbf{A}).\n\nThus, R (\\mathbf{A}) is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace."
  },
  {
    "objectID": "Linear Algebra/02_Vectors_and_Matrices.html#column-and-row-space",
    "href": "Linear Algebra/02_Vectors_and_Matrices.html#column-and-row-space",
    "title": "2  Vectors and Matrices",
    "section": "5.3 Column and row space",
    "text": "5.3 Column and row space\nThe column space of the matrix \\mathbf{A} is the set of linear combinations of columns of \\mathbf{A}\n\nC (\\mathbf{A}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\sum_{i=1}^{n} \\alpha_{i} \\cdot \\mathbf{a}_{*, i}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\},\n\nand the row space of the matrix \\mathbf{A} is the set of linear combinations of rows of \\mathbf{A}\n\nC (\\mathbf{A}^{T}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\sum_{i=1}^{n} \\alpha_{i} \\cdot \\mathbf{a}_{i, *}^{T}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\},\n\nwhich is the same as the column space of \\mathbf{A}^{T}.\nBy the definition of matrix-vector multiplication, the column space of \\mathbf{A} is the same as the range space of \\mathbf{A}\n\nC (\\mathbf{A}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\sum_{i=1}^{n} \\alpha_{i} \\cdot \\mathbf{a}_{*, i}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\} = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\mathbf{A} \\mathbf{x}, \\forall \\mathbf{x} \\in \\mathbb{F}^{n}\n\\right\\} = R (\\mathbf{A})."
  },
  {
    "objectID": "Linear Algebra/03_Span_and_Linear_Independence.html",
    "href": "Linear Algebra/03_Span_and_Linear_Independence.html",
    "title": "3  Span and Linear Independence",
    "section": "",
    "text": "4 Span\nGiven a vector space \\mathcal{V} over a field \\mathbb{F}, the span of a set of vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} is the set of all possible linear combinations of v_{1}, \\dots, v_{m}\n\\text{span} (v_{1}, \\dots, v_{n}) = \\left\\{\n    \\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\}.\nIf \\mathcal{V} is \\mathbb{F}^{m}, the set vectors v_{1}, \\dots, v_{n} can be viewed as the columns of the matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}. Then\n\\text{span} (v_{1}, \\dots, v_{n}) = R (\\mathbf{A}).\nSince we have proved R (\\mathbf{A}) is a subspace of \\mathcal{V}, the span of a set of vectors is a subspace of \\mathcal{V}.\nGiven a vector space \\mathcal{V} over a field \\mathbb{F}, a set of non-zero vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} is linearly independent when\n\\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i} = 0 \\iff \\alpha_{i} = 0, i = 1, \\dots, n."
  },
  {
    "objectID": "Linear Algebra/03_Span_and_Linear_Independence.html#spanning-set",
    "href": "Linear Algebra/03_Span_and_Linear_Independence.html#spanning-set",
    "title": "3  Span and Linear Independence",
    "section": "4.1 Spanning set",
    "text": "4.1 Spanning set\nFor a set of vectors \\mathcal{S} = v_{1}, \\dots, v_{n}, if the subspace \\mathcal{W} is the span of \\mathcal{S}\n\n\\mathcal{W} = \\text{span} (\\mathcal{S}),\n\nthen the set of vectors \\mathcal{S} is the spanning set of the subspace \\mathcal{W}.S."
  },
  {
    "objectID": "Linear Algebra/03_Span_and_Linear_Independence.html#properties-of-the-spanning-set",
    "href": "Linear Algebra/03_Span_and_Linear_Independence.html#properties-of-the-spanning-set",
    "title": "3  Span and Linear Independence",
    "section": "4.2 Properties of the spanning set",
    "text": "4.2 Properties of the spanning set\nConsider a set of vectors \\mathcal{A} = \\left\\{ v_{1}, \\dots, v_{n} \\right\\} and a subspace \\mathcal{W}.\n(spanning-set-property-1)=\n\nIf \\mathcal{A} is a spanning set of \\mathcal{W}, the new set\n\n  \\mathcal{A} \\cup \\left\\{\n      u_{1}, \\dots, u_{n}\n  \\right\\}\n  \nis still a spanning set of \\mathcal{W} for arbitrary vectors u_{1}, \\dots, u_{n} \\in \\mathcal{W}.\n\n(spanning-set-property-2)=\n\n\\mathcal{A} is a spanning set of \\mathcal{W} if and only if\n\nall vectors in \\mathcal{W} are linear combinations of v_{1}, \\dots, v_{n} and\nv_{1}, \\dots, v_{n} \\in \\mathcal{W}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince this statement has “if and only if”, we first prove in the forward direction.\nIf v_{1}, \\dots, v_{n} is a spanning set of \\mathcal{W}, then by the definition of spanning set, the set of all linear combinations of v_{1}, \\dots, v_{n} is the subspace \\mathcal{W}.\nThus, all vectors in the subspace \\mathcal{W} are the linear combinations of v_{1}, \\dots, v_{n}.\nSince every v_{i}, i = 1, \\dots, n is a linear combination of itself, then v_{1}, \\dots, v_{n} are also in \\mathcal{W}.\nThen we prove in the backward direction.\nSince all vectors in \\mathcal{W} are linear combinations of v_{1}, \\dots, v_{n}, by the definition of span,\n\n  \\mathcal{W} \\subseteq \\text{span} (v_{1}, \\dots, v_{n}).\n  \nSince v_{1}, \\dots, v_{n} \\in \\mathcal{W} and the closure property of the subspace, all linear combinations of v_{1}, \\dots, v_{n} are also in \\mathcal{W}, which means that\n\n  \\text{span} (v_{1}, \\dots, v_{n}) \\subseteq \\mathcal{W}.\n  \nThus,\n\n  \\mathcal{W} = \\text{span} (v_{1}, \\dots, v_{n}).\n  \n\n\n\n\n(spanning-set-property-3)=\n\nIf vectors in set \\mathcal{B} are linear combinations of the vectors in \\mathcal{A}, then\n\n  \\text{span} (\\mathcal{B}) \\subseteq \\text{span} (\\mathcal{A}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction. Suppose that the vectors in set \\mathcal{B} are linear combinations of the vectors in \\mathcal{A}, but there exists an vector v \\in \\text{span} (\\mathcal{B}) but v \\notin \\text{span} (\\mathcal{A}).\nSuppose \\mathcal{A} = \\{ a_{1}, \\dots, a_{n} \\}. Since v \\in \\mathcal{B} is a linear combinations of vectors in \\mathcal{A}, there exists a set of coefficients \\alpha_{1}, \\dots, \\alpha_{n} such that\n\n  v = \\sum_{i = 1}^{n} \\alpha_{i} a_{i}.\n  \nwhich by definition of span means v \\in \\text{span} (\\mathbf{A}).\nHowever, we assume that v \\notin \\text{span} (\\mathbf{A}), which raises a contradiction.\n\n\n\n\n(spanning-set-property-4)=\n\nIf \\text{span} (\\mathcal{A}) = \\mathcal{W} and \\text{span} (\\mathcal{B}) = \\mathcal{U}, then\n\n  \\text{span} (\\mathcal{A} \\cup \\mathcal{B}) = \\mathcal{W} + \\mathcal{U},\n  \nwhere\n\n  \\mathcal{W} + \\mathcal{U} = \\left\\{\n      w + u \\mid w \\in \\mathcal{W}, u \\in \\mathcal{U}\n  \\right\\}.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\mathcal{A} = \\{ a_{1}, \\dots, a_{n} \\}, \\mathcal{B} = \\{ b_{1}, \\dots, b_{m} \\}, then\n\n  \\text{span} (\\mathcal{A} \\cup \\mathcal{B}) = \\left\\{\n      \\sum_{i=1}^{n} \\alpha_{i} a_{i} + \\sum_{i=1}^{n} \\beta_{i} b_{i}, \\forall \\alpha_{i}, \\beta_{i} \\in \\mathbb{F}\n  \\right\\}.\n  \nNote that\n\n  w \\in \\mathcal{W} = \\sum_{i=1}^{n} \\alpha_{i} a_{i}, \\forall \\alpha_{i} \\in \\mathbb{F},\n  \n\n  u \\in \\mathcal{U} = \\sum_{i=1}^{n} \\beta_{i} b_{i}, \\forall \\beta_{i} \\in \\mathbb{F}.\n  \nThus,\n\n  \\text{span} (\\mathcal{A} \\cup \\mathcal{B}) = \\left\\{\n      w + u \\mid w \\in \\mathcal{W}, u \\in \\mathcal{U}\n  \\right\\} = \\mathcal{W} + \\mathcal{U}."
  },
  {
    "objectID": "Linear Algebra/03_Span_and_Linear_Independence.html#properties-of-linear-independence",
    "href": "Linear Algebra/03_Span_and_Linear_Independence.html#properties-of-linear-independence",
    "title": "3  Span and Linear Independence",
    "section": "5.1 Properties of linear independence",
    "text": "5.1 Properties of linear independence\n\nGiven a matrix \\mathbf{A} whose columns are vectors \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\in \\mathbb{F}^{m} the set of vectors \\mathcal{S} = \\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\} is linearly independent when the null space of \\mathbf{A} only contains \\mathbf{0} \\in \\mathbb{F}^{n}.\n\n  N (\\mathbf{A}) = \\left\\{\n      \\mathbf{0}\n  \\right\\}\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition, N (\\mathbf{A}) = \\{ \\mathbf{0} \\} says that the only set of \\alpha_{1}, \\dots, \\alpha_{n} satisfying\n\n  \\sum_{i = 1}^{n} \\alpha_{i} \\mathbf{A}_{*, i} = 0\n  \nis \\alpha_{1}, \\dots, \\alpha_{n} = 0, which is equivalent to saying the columns of \\mathbf{A} are a linearly independent set.\n\n\n\nIf vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} are linearly independent and the subspace \\mathcal{W} = \\text{span} (v_{1}, \\dots, v_{n}), the set of field elements \\{ \\alpha_{1}, \\dots, \\alpha_{n} \\} that is paired with the set of vectors \\{ v_{1}, \\dots, v_{n} \\} to represent any vector u \\in \\mathcal{W}: u = \\sum_{i=1}^{n} \\alpha_{i} v_{i} is unique.\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof by contradiction. Suppose there is another set of field elements that can be paired with v_{1}, \\dots, v_{n} to represent u\n\n  \\left\\{\n      \\beta_{1}, \\dots, \\beta_{n} \\mid u = \\sum_{i=1}^{n} \\beta_{i} \\cdot v_{i}\n  \\right\\}.\n  \nThen,\n\n  \\begin{aligned}\n  \\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i}\n  & = \\sum_{i=1}^{n} \\beta_{i} \\cdot v_{i}\n  \\\\\n  \\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i} - \\sum_{i=1}^{n} \\beta_{i} \\cdot v_{i}\n  & = 0\n  \\\\\n  \\sum_{i=1}^{n} (\\alpha_{i} - \\beta_{i}) \\cdot v_{i}\n  & = 0\n  \\\\\n  \\alpha_{i} - \\beta_{i}\n  & = 0\n  \\\\\n  \\alpha_{i}\n  & = \\beta_{i},\n  \\end{aligned}\n  \nwhich contradicts to the fact \\alpha_{i} and \\beta_{i} should be different."
  },
  {
    "objectID": "Linear Algebra/03_Span_and_Linear_Independence.html#linear-dependence",
    "href": "Linear Algebra/03_Span_and_Linear_Independence.html#linear-dependence",
    "title": "3  Span and Linear Independence",
    "section": "5.2 Linear dependence",
    "text": "5.2 Linear dependence\nGiven a vector space \\mathcal{V} over a field \\mathbb{F}, a set of non-zero vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} is linear dependent when there exists an \\alpha_{i} \\neq 0 such that\n\n\\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i} = 0.\n\n(existence-of-linear-combination)=\nIf a set of vectors \\{ v_{1}, \\dots, v_{n} \\} is linearly dependent, there exists an index j (1 \\leq j \\leq n) such that v_{j} is a linear combination of the rest of the vectors:\n\nv_{j} = \\sum_{i=1, i \\neq j}^{n} \\alpha_{i} \\cdot v_{i}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the set \\{ v_{1}, \\dots, v_{n} \\} is linearly dependent, there exists an \\beta_{j} \\neq 0 (1 \\leq j \\leq n) such that\n\n\\beta_{1} \\cdot v_{1} + \\dots \\beta_{j} \\cdot v_{j} + \\dots \\beta_{n} \\cdot v_{n} = 0.\n\nThus,\n\n\\begin{aligned}\n\\beta_{j} \\cdot v_{j}\n& = - \\sum_{i=1, i \\neq j} \\beta_{i} \\cdot v_{i}\n\\\\\nv_{j}\n& = - \\beta_{j}^{-1} \\cdot \\sum_{i=1, i \\neq j} \\beta_{i} \\cdot v_{i}\n\\\\\nv_{j}\n& = \\sum_{i=1, i \\neq j} - \\beta_{j}^{-1} \\beta_{i} \\cdot v_{i}\n\\\\\nv_{j}\n& = \\sum_{i=1, i \\neq j} \\alpha_{i} \\cdot v_{i}\n& [\\text{rewrite } - \\beta_{j}^{-1} \\beta_{i} = \\alpha_{i}]\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/04_Basis_and_Dimension.html",
    "href": "Linear Algebra/04_Basis_and_Dimension.html",
    "title": "4  Basis and Dimension",
    "section": "",
    "text": "5 Basis\nA set of vectors v_{1}, \\dots, v_{k} is a basis of a subspace \\mathcal{W} of a vector space \\mathcal{V} over a field \\mathbb{F}, if\nThe dimension of a subspace is the common cardinality of its all bases."
  },
  {
    "objectID": "Linear Algebra/04_Basis_and_Dimension.html#properties-of-basis",
    "href": "Linear Algebra/04_Basis_and_Dimension.html#properties-of-basis",
    "title": "4  Basis and Dimension",
    "section": "5.1 Properties of basis",
    "text": "5.1 Properties of basis\n\nIf \\mathcal{B} = \\{ b_{1}, \\dots, b_{n} \\} is a basis of the subspace \\mathcal{W}, \\mathcal{B} is a minimal spanning set for \\mathcal{W}. That is, there is no vector in \\mathcal{B} that can be removed such that \\mathcal{B} is still a spanning set.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove by contradiction. Assume \\mathcal{B} is a basis of the subspace \\mathcal{W} but is not a minimal spanning set, which means that the vectors in \\mathcal{B} are linear independent and \\mathcal{B} contains at least one vector b such that \\hat{\\mathcal{B}} = \\mathcal{B} \\setminus \\{ b \\} is still a spanning set.\nTherefore, there exists a set of coefficients \\alpha_{1}, \\dots, \\alpha_{n - 1} such that b is a linear combination of the vectors in \\hat{\\mathcal{B}}\n\n  b = \\sum_{b_{i} \\in \\hat{\\mathcal{B}}} \\alpha_{i} b_{i}.\n  \nHowever, this means that the vectors in \\mathcal{B} are linear dependent, which violates our assumption.\n\n\n\nIf \\mathcal{B} = \\{ b_{1}, \\dots, b_{n} \\} is a basis of the subspace \\mathcal{W}, \\mathcal{B} is a maximal linearly independent subset of \\mathcal{W}. That is, there is no vector in \\mathcal{W} that can be added to \\mathcal{B} such that \\mathcal{B} is still a linearly independent set.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove by contradiction. Assume \\mathcal{B} is a basis of the subspace \\mathcal{W} but is not a maximal linearly independent set, which means that \\mathcal{B} is a spanning set of \\mathcal{W} and there exists a vector b \\in \\mathcal{W} that can be added to \\mathcal{B} such that \\hat{\\mathcal{B}} = \\mathcal{B} \\cup \\{ b \\} is still a linearly independent set.\nHowever, since \\mathcal{B} is a spanning set of \\mathcal{W}, there exists a set of coefficients \\alpha_{1}, \\dots, \\alpha_{n} such that b is a linear combination of the vectors in \\mathcal{B}\n\n  b = \\sum_{b_{i} \\in \\mathcal{B}} \\alpha_{i} b_{i}.\n  \nwhich means that the vectors in \\hat{\\mathcal{B}} are linear dependent, which violates our assumption.\n\n\n\n\n(existence-of-basis)=\n\nExistence of basis\nA subspace may NOT have a basis e.g. \\mathcal{W} = \\{ 0 \\} has no linearly independent vector, but a subspace must have a basis if it has a finite spanning set.\nThat is, every finite spanning set of a subspace contains a basis.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose the subspace \\mathcal{W} has a finite spanning set of\n\n  \\mathcal{A}_{0} = \\{ v_{i}, \\dots, v_{n} \\}.\n  \nIf the spanning set is linearly independent, the set \\{ v_{i}, \\dots, v_{n} \\} is a basis of \\mathcal{W}.\nIf the spanning set is not linearly independent, then there exists j (1 \\leq j \\leq n) such that\n\n  v_{j} = \\sum_{i=1, i \\neq j}^{n} \\alpha_{i} v_{i},\n  \nand the set\n\n  \\mathcal{A}_{1} = \\{ v_{1}, \\dots, v_{n} \\} \\setminus \\{ v_{j} \\}\n  \nis still a spanning set.\nWe can continue removing such v_{j} if the resulting set \\mathcal{A}_{i} is not linearly independent.\nSince the resulting set \\mathcal{A}_{i} is always a spanning set of \\mathcal{W}, we will eventually get to the step i where the \\mathcal{A}_{i} is linearly dependent and \\mathcal{A}_{i} will be the basis for the subspace \\mathcal{W}.\n\n\n\n\n(cardinality-of-basis)=\n\nCardinality of basis\nThe numbers of elements of all bases of a given subspace are the same.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\mathcal{A} is a basis of a subspace \\mathcal{S} and \\mathcal{B} is another basis of \\mathcal{S}. Thus, \\mathcal{A} and \\mathcal{B} are both linearly independent and spanning sets.\nSince \\mathcal{A} is a spanning set and \\mathcal{B} is linearly independent,\n\n  \\lvert \\mathcal{A} \\rvert \\geq \\lvert \\mathcal{B} \\rvert.\n  \nSince \\mathcal{B} is a spanning set and \\mathcal{A} is linearly independent,\n\n  \\lvert \\mathcal{A} \\rvert \\leq \\lvert \\mathcal{B} \\rvert.\n  \nThus,\n\n  \\lvert \\mathcal{A} \\rvert = \\lvert \\mathcal{B} \\rvert.\n  \n\n\n\n\n(extension-of-a-basis)=\n\nExtension of a basis\nIf the subspace \\mathcal{U} is a subset of a finite dimensional subspace \\mathcal{V} and \\{ b_{1}, \\dots, b_{k} \\} is a basis of \\mathcal{U}, then there exists an extension of \\{ b_{1}, \\dots, b_{k} \\}\n\n  \\{ b_{1}, \\dots, b_{k}, b_{k + 1}, \\dots, b_{n} \\}\n  \nthat is a basis for \\mathcal{V}\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\{ b_{1}, \\dots, b_{k}, b_{k + 1}, \\dots, b_{n} \\} is a basis for \\mathcal{V} if \\mathcal{U} \\subseteq \\mathcal{V} and \\{ b_{1}, \\dots, b_{k} \\} is a basis of \\mathcal{U}."
  },
  {
    "objectID": "Linear Algebra/04_Basis_and_Dimension.html#properties-of-dimension",
    "href": "Linear Algebra/04_Basis_and_Dimension.html#properties-of-dimension",
    "title": "4  Basis and Dimension",
    "section": "6.1 Properties of dimension",
    "text": "6.1 Properties of dimension\n(dimension-property-1)=\n\nIf a subspace \\mathcal{U} is a subset of a finite dimensional subspace \\mathcal{V}, then\n\n  \\text{dim} (\\mathcal{U}) \\leq \\text{dim} (\\mathcal{V}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose there is a basis\n\n  \\mathcal{A} = \\{ b_{1}, \\dots, b_{n} \\}\n  \nfor \\mathcal{U}.\nSince \\mathcal{U} \\subseteq \\mathcal{V}, there are 2 cases.\nIn the case where \\mathcal{U} = \\mathcal{V}, \\mathcal{A} is also a basis of \\mathcal{V} and thus,\n\n  \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}).\n  \nIn the case where \\mathcal{U} \\subset \\mathcal{V}, there is at least 1 vector x \\in \\mathcal{V} such that\n\n  x \\notin \\text{span} (\\mathcal{A}).\n  \n\n\n\n\n(dimension-property-2)=\n\nIf a subspace \\mathcal{U} is a subset of a finite dimensional subspace \\mathcal{V} and \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}), then\n\n  \\mathcal{U} = \\mathcal{V}.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathcal{U} \\subset \\mathcal{V} and according to the property of extension of the basis, there exists a basis b_{\\mathcal{U}} for \\mathcal{U} and b_{\\mathcal{V}} for \\mathcal{V} such that\n\n  \\mathcal{B}_{\\mathcal{U}} \\subset \\mathcal{B}_{\\mathcal{V}}.\n  \nHowever, since \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}), it must follows that\n\n  \\mathcal{B}_{\\mathcal{U}} = \\mathcal{B}_{\\mathcal{V}}.\n  \nSince \\mathcal{U} and \\mathcal{V} shares the same basis,\n\n  \\mathcal{U} = \\mathcal{V}.\n  \n\n\n\n\n(dimension-property-3)=\n\nGiven \\mathcal{U} and \\mathcal{V} are subspaces, then\n\n  \\text{dim} (\\mathcal{U} + \\mathcal{V}) = \\text{dim} (\\mathcal{U}) + \\text{dim} (\\mathcal{V}) - \\text{dim} (\\mathcal{U} \\cap \\mathcal{V}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\text{dim} (\\mathcal{U} \\cap \\mathcal{V}) = k, \\text{dim} (\\mathcal{U}) = m, and \\text{dim} (\\mathcal{V}) = n.\nAssume there is a basis\n\n  \\mathcal{B} = \\{ b_{1}, \\dots, b_{k} \\}\n  \nfor \\mathcal{U} \\cap \\mathcal{V}.\nSince \\mathcal{U} \\cap \\mathcal{V} \\subseteq \\mathcal{U} and \\mathcal{U} \\cap \\mathcal{V} \\subseteq \\mathcal{V}, there exists a basis \\mathcal{A} for \\mathcal{U} and \\mathcal{C} for \\mathcal{V}\n\n  \\mathcal{A} = \\{ b_{1}, \\dots, b_{k}, a_{k + 1}, \\dots, a_{m} \\}\n  \n\n  \\mathcal{C} = \\{ b_{1}, \\dots, b_{k}, c_{k + 1}, \\dots, c_{n} \\}\n  \nas extensions of \\mathcal{B}, according to extension of basis property.\nConsider the set\n\n  \\mathcal{A} \\cup \\mathcal{C} = \\{ b_{1}, \\dots, b_{k}, a_{k + 1}, \\dots, a_{m}, c_{k + 1}, \\dots, c_{n} \\}\n  \nwe will prove that it is a basis of \\mathcal{U} + \\mathcal{V}.\nFirst, consider all x \\in \\mathcal{U} + \\mathcal{V}, which can be represented using the basis of \\mathcal{U} and the basis of \\mathcal{V}\n\n  \\begin{aligned}\n  x\n  & = \\sum_{i=1}^{k} \\alpha_{i} b_{i} + \\sum_{i=k+1}^{m} \\alpha_{i} a_{i} + \\sum_{i=1}^{k} \\beta_{i} b_{i} + \\sum_{i=k+1}^{m} \\beta_{i} c_{i}\n  \\\\\n  & = \\sum_{i=1}^{k} (\\alpha_{i} + \\beta_{i}) b_{i} + \\sum_{i=k+1}^{m} \\alpha_{i} a_{i} + \\beta_{i} c_{i}.\n  \\end{aligned}\n  \nThus, any vector x \\in \\mathcal{U} + \\mathcal{V} is a linear combination of vectors in \\mathcal{A} \\cup \\mathcal{C} and we have\n\n  \\mathcal{U} + \\mathcal{V} \\subseteq \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right).\n  \nConversely, all x \\in \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right) can be written as the linear combinations of vectors in \\mathcal{A} and \\mathcal{C}, and thus\n\n  \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right) \\subseteq \\mathcal{U} + \\mathcal{V}.\n  \nThus,\n\n  \\mathcal{U} + \\mathcal{V} = \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right).\n  \nNext, we will prove the vectors in \\mathcal{A} \\cup \\mathcal{C} are linearly independent by contradiction.\nConsider x \\in \\mathcal{U} \\cap \\mathcal{V}, which can be represented by the basis of \\mathcal{U} \\cap \\mathcal{V}, \\mathcal{U} and, \\mathcal{V} at the same time\n\n  \\begin{aligned}\n  x\n  & = \\sum_{i=1}^{k} \\alpha_{i} b_{i} + \\sum_{i=k+1}^{m} \\alpha_{i} a_{i}\n  \\\\\n  & = \\sum_{i=1}^{k} \\beta_{i} b_{i} + \\sum_{i=k+1}^{n} \\beta_{i} c_{i}\n  \\\\\n  & = \\sum_{i=1}^{k} \\epsilon_{i} b_{i}.\n  \\end{aligned}\n  \nSuppose x = 0, then because"
  },
  {
    "objectID": "Linear Algebra/04_Basis_and_Dimension.html#types-of-the-subspaces",
    "href": "Linear Algebra/04_Basis_and_Dimension.html#types-of-the-subspaces",
    "title": "4  Basis and Dimension",
    "section": "6.2 Types of the subspaces",
    "text": "6.2 Types of the subspaces\nWe can classify the types of the subspaces based on their dimensions. For example, in \\mathbb{R}^{n} there are n types of subspaces\n\n0 dimension subspace: \\{ 0 \\}.\n1 dimension subspaces.\n…\nn dimension subspace: \\mathbb{R}^{n} itself."
  },
  {
    "objectID": "Linear Algebra/05_Linear_Map_and_Rank.html",
    "href": "Linear Algebra/05_Linear_Map_and_Rank.html",
    "title": "5  Linear Map and Rank",
    "section": "",
    "text": "6 Linear Map\nLet \\mathcal{U} and \\mathcal{V} be the vector spaces over the same filed \\mathbb{F}. A map T: \\mathcal{U} \\to \\mathcal{V} is linear if\nConsider matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}, the rank of matrix \\mathbf{A} is\n\\text{rank} (\\mathbf{A}) = \\text{dim} (R (\\mathbf{A}))."
  },
  {
    "objectID": "Linear Algebra/05_Linear_Map_and_Rank.html#properties-of-linear-map",
    "href": "Linear Algebra/05_Linear_Map_and_Rank.html#properties-of-linear-map",
    "title": "5  Linear Map and Rank",
    "section": "6.1 Properties of linear map",
    "text": "6.1 Properties of linear map\nLet \\mathcal{U} and \\mathcal{V} be two vector spaces over the same filed \\mathbb{F}, and suppose T is a linear map T: \\mathcal{U} \\to \\mathcal{V}.\n\nA linear map must satisfy T (0) = 0.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction. Suppose there exists a linear map such that\n\n  T (0) \\neq 0.\n  \nSuppose v = 0 and according to the definition of the linear map\n\n  \\begin{aligned}\n  T (\\alpha \\cdot v)\n  & = \\alpha \\cdot T (v), \\forall \\alpha \\in \\mathbb{F}\n  \\\\\n  T (0)\n  & = \\alpha \\cdot T (0), \\forall \\alpha \\in \\mathbb{F}.\n  \\end{aligned}\n  \nSince T (0) \\neq 0, we can divide both sides by T (0)\n\n  \\alpha = 1, \\forall \\alpha \\in \\mathbb{F},\n  \nwhich raises a contradiction.\n\n\n\nThere exists a matrix representation of T.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\text{dim} (\\mathcal{U}) = n and \\text{dim} (\\mathcal{V}) = m.\nLet \\{ u_{1}, \\dots, u_{n} \\} be a basis of \\mathcal{U}, and \\{ v_{1}, \\dots, v_{m} \\} be a basis of \\mathcal{V}.\nSuppose any vector x \\in \\mathcal{U} satisfies that\n\n  x = \\sum_{i=1}^{n} \\alpha_{i} \\cdot u_{i},\n  \nand the mapped vector T (x) \\in \\mathcal{V} satisfies that\n\n  T (x) = \\sum_{j=1}^{m} \\beta_{j} \\cdot v_{j}.\n  \nSince T (u_{i}) \\in \\mathcal{V}, we have\n\n  T (u_{i}) = \\sum_{j=1}^{m} c_{i, j} \\cdot v_{j}, \\forall i = 1, \\dots, n.\n  \nBy the definition of the linear map,\n\n  \\begin{aligned}\n  T (x)\n  & = T \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} \\cdot u_{i}\n  \\right)\n  \\\\\n  & = \\sum_{i=1}^{n} \\alpha_{i} \\cdot T (u_{i}).\n  \\\\\n  & = \\sum_{i=1}^{n} \\alpha_{i} \\sum_{j=1}^{m} c_{i, j} \\cdot v_{j}\n  \\\\\n  & = \\sum_{j=1}^{m} \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} c_{i, j}\n  \\right) \\cdot v_{j}.\n  \\\\\n  \\end{aligned}\n  \nBecause of the unique representation property,\n\n  \\begin{aligned}\n  T(x) = \\sum_{j=1}^{m} \\beta_{j} \\cdot v_{j}\n  & = \\sum_{j=1}^{m} \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} c_{i, j}\n  \\right) \\cdot v_{j}\n  \\\\\n  \\beta_{j}\n  & = \\sum_{i=1}^{n} \\alpha_{i} c_{i, j} \\quad \\forall j = 1, \\dots, m,\n  \\end{aligned}\n  \nwhich can be represented in the matrix form\n\n  \\begin{bmatrix}\n  \\beta_{1} \\\\\n  \\vdots \\\\\n  \\beta_{m} \\\\\n  \\end{bmatrix}\n  =\n  \\begin{bmatrix}\n  c_{1, 1} & \\dots & c_{n, 1}  \\\\\n  \\vdots & \\dots & \\vdots \\\\\n  c_{1, m} & \\dots & c_{n, m} \\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n  \\alpha_{1} \\\\\n  \\vdots \\\\\n  \\alpha_{n} \\\\\n  \\end{bmatrix}.\n  \nHence, given any vector x \\in \\mathcal{U} that has basis coefficients in a given basis \\{ u_{1}, \\dots, u_{n} \\}\n\n  \\mathbf{a} = \\begin{bmatrix}\n  \\alpha_{1} \\\\\n  \\vdots \\\\\n  \\alpha_{n} \\\\\n  \\end{bmatrix},\n  \nthere is a mapped vector T (x) \\in \\mathcal{V} with basis coefficients in a given basis \\{ v_{1}, \\dots, v_{n} \\}\n\n  \\mathbf{b} = \\begin{bmatrix}\n  \\beta_{1} \\\\\n  \\vdots \\\\\n  \\beta_{m} \\\\\n  \\end{bmatrix},\n  \nwhere\n\n  \\mathbf{b} = \\mathbf{C} \\mathbf{a}."
  },
  {
    "objectID": "Linear Algebra/05_Linear_Map_and_Rank.html#generalization-of-null-space-and-range-space",
    "href": "Linear Algebra/05_Linear_Map_and_Rank.html#generalization-of-null-space-and-range-space",
    "title": "5  Linear Map and Rank",
    "section": "6.2 Generalization of null space and range space",
    "text": "6.2 Generalization of null space and range space\nSince every linear map has a matrix representation, the null space and range space defined using matrix can be redefined using linear map.\nGiven a map T: \\mathcal{U} \\to \\mathcal{V}, the null space (kernel) is\n\nN (T) = \\left\\{\n    x \\in \\mathcal{U} \\mid T (x) = 0\n\\right\\},\n\nand the range space (column space) is\n\nR (T) = \\left\\{\n    y \\in \\mathcal{V} \\mid y \\in T (x), \\forall x \\in \\mathcal{U}\n\\right\\}."
  },
  {
    "objectID": "Linear Algebra/05_Linear_Map_and_Rank.html#properties-of-rank",
    "href": "Linear Algebra/05_Linear_Map_and_Rank.html#properties-of-rank",
    "title": "5  Linear Map and Rank",
    "section": "7.1 Properties of rank",
    "text": "7.1 Properties of rank\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n},\n(rank-property-1)=\n\nrank-nullity theorem: \\text{rank} (\\mathbf{A}) + \\text{dim} (N (\\mathbf{A})) = n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathbf{A} represents a linear transform T: \\mathcal{U} \\to \\mathcal{V} with \\mathrm{dim} (\\mathcal{U}) = m, \\mathrm{dim} (\\mathcal{V}) = n the rank-nullity theorem can also be stated as\n\n  \\mathrm{dim} (R (T)) + \\mathrm{dim} (N (T)) = \\mathrm{dim} (\\mathcal{U}).\n  \nBy assuming that \\text{dim} (N (T)) = k, there exists a basis \\{ b_{1}, \\dots, b_{k} \\} of N (T).\nSince by definition of the null space, N (T) is a subspace of \\mathcal{U}, then according to the property of the basis, there exists a set of vectors \\{ b_{k + 1}, \\dots, b_{n} \\} in \\mathcal{U} such that\n\n  \\{ b_{1}, \\dots, b_{k}, b_{k + 1}, \\dots, b_{n} \\}\n  \nis a basis of \\mathcal{U}.\nConsider any x \\in \\mathcal{U},\n\n  \\begin{aligned}\n  T (x)\n  & = T \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} b_{i}\n  \\right)\n  & [\\{ b_{1}, \\dots, b_{i} \\} \\text{ is a basis of } \\mathcal{U}]\n  \\\\\n  & = \\sum_{i=1}^{n} \\alpha_{i} T (b_{i})\n  & [\\text{T is linear}]\n  \\\\\n  \\end{aligned}\n  \nSince \\{ b_{1}, \\dots, b_{k} \\} is a basis of N (T), they are also in N (T). According to the definition of the null space\n\n  T (b_{i}) = 0 \\quad i = 1, \\dots, k.\n  \nThen we can show that every T (x) is a linear combination of T (b_{k+1}) \\dots, T (b_{n}):\n\n  \\begin{aligned}\n  T (x)\n  & = \\sum_{i=1}^{n} \\alpha_{i} T (b_{i})\n  \\\\\n  & = \\sum_{i=1}^{k} \\alpha_{i} T (b_{i}) + \\sum_{i=k+1}^{n} \\alpha_{i} T (b_{i})\n  \\\\\n  & = 0 + \\sum_{i=k+1}^{n} \\alpha_{i} T (b_{i})\n  \\\\\n  & = \\sum_{i=k+1}^{n} \\alpha_{i} T (b_{i})\n  \\end{aligned}.\n  \nSince the definition of the range space is\n\n  R (T) = \\{ y \\mid y = T(x), \\forall x \\in \\mathcal{U} \\},\n  \nall vectors in R (T) are linear combinations of T (b_{k+1}) \\dots, T (b_{n}).\nAlso, since b_{k+1}, \\dots, b_{n} \\in \\mathcal{U}, we have\n\n  T (b_{k+1}), \\dots, T (b_{n}) \\in R (T).\n  \nby the definition of the range space.\nThus, by the property of span\n\n  R (T) = \\text{span} (T (b_{k+1}), \\dots, T (b_{n})).\n  \nThen we will show that T (b_{k+1}), \\dots, T (b_{n}) are linearly independent by supposing a set of \\beta_{k+1}, \\dots, \\beta_{n} such that\n\n  \\begin{aligned}\n  \\sum_{i=k+1}^{n} \\beta_{i} T (b_{i})\n  & = 0\n  \\\\\n  T \\left(\n      \\sum_{i=k+1}^{n} \\beta_{i} b_{i}\n  \\right)\n  & = 0\n  & [\\text{T is linear}]\n  \\\\\n  \\sum_{i=k+1}^{n} \\beta_{i} b_{i}\n  & \\in N (T)\n  & [\\text{def of null space}]\n  \\\\\n  \\end{aligned}\n  \nSince \\{ b_{1}, \\dots, b_{k} \\} is also a basis of the N (T), there exists a set of \\gamma_{1}, \\dots, \\gamma_{k} such that\n\n  \\begin{aligned}\n  \\sum_{i=k+1}^{n} \\beta_{i} b_{i}\n  & = \\sum_{i=j}^{k} \\gamma_{j} b_{j}\n  \\\\\n  \\sum_{i=k+1}^{n} \\beta_{i} b_{i} + \\sum_{j=1}^{k} - \\gamma_{j} b_{j}\n  & = 0.\n  \\end{aligned}\n  \nSince \\{ b_{1}, \\dots, b_{n} \\} are linearly independent, it must hold that\n\n  \\beta_{i} = \\gamma_{j} = 0 \\quad i = k + 1, \\dots, n, j = 1, \\dots, k.\n  \nThus, we have shown that\n\n  \\sum_{i=k+1}^{n} \\beta_{i} T (b_{i}) = 0 \\iff \\beta_{i} = 0 \\quad i = k + 1, \\dots, n,\n  \nwhich means the vectors T (b_{k + 1}), \\dots, T (b_{n}) are linearly independent and therefore is a basis of R (T).\nFinally, since\n\n\\{ b_{1}, \\dots, b_{k} \\} is a basis of N (T),\n\\{ b_{k + 1}, \\dots, b_{n} \\} is a basis of R (T), and\n\\{ b_{1}, \\dots, b_{n} \\} is a basis of \\mathcal{U},\n\nwe have the rank-nullity theorem\n\n  \\text{dim} (N (T)) + \\text{dim} (R (T)) = \\text{dim} (\\mathcal{U}).\n  \n\n\n\n\n(rank-property-2)=\n\n\\text{rank} (\\mathbf{A}) \\leq \\min (m, n)\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the rank property\n\n  \\text{dim} (N (\\mathbf{\\mathbf{A}})) = n - \\text{rank} (\\mathbf{A}).\n  \nSince by definition, the number of vectors in a basis is non-negative\n\n  \\text{dim} (N (\\mathbf{\\mathbf{A}})) \\geq 0,\n  \nwe have\n\n  \\text{rank} (\\mathbf{A}) \\leq n.\n  \nAlso, since R (\\mathbf{A}) is a subspace of \\mathbb{F}^{m} and the property\n\n  \\text{rank} (\\mathbf{A}) = \\text{dim} (R (\\mathbf{A})) \\leq m.\n  \nThus,\n\n  \\text{rank} (\\mathbf{A}) \\leq \\min(m, n).\n  \n\n\n\n\nIf \\text{rank} (\\mathbf{A}) = \\min (m, n), matrix is called a full (row or column) rank matrix.\nIf \\text{rank} (\\mathbf{A}) = n, \\mathbf{a}_{*, 1}, \\dots, \\mathbf{a}_{*, n} are linearly independent.\n\n\n(rank-property-3)=\n\n\\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T})\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\mathbf{A} \\in \\mathbb{F}^{m \\times n}. Let \\{ \\mathbf{a}_{1}, \\dots, \\mathbf{a}_{n} \\in \\mathbb{F}^{m} \\} be the columns of \\mathbf{A}\n\n  \\mathbf{A} =\n  \\begin{bmatrix}\n  \\mathbf{a}_{1} & \\dots & \\mathbf{a}_{n}\n  \\end{bmatrix}.\n  \nLet \\text{rank} (\\mathbf{A}) = r.\nBy the definition of the range space, the columns of \\mathbf{A} are in its range space\n\n  \\mathbf{a}_{1}, \\dots, \\mathbf{a}_{n} \\in R (\\mathbf{A})\n  \nand thus the columns of \\mathbf{A} are linear combinations of any basis of R (\\mathbf{A}).\nSuppose there exists a basis of R (\\mathbf{A}) as columns of \\mathbf{B}\n\n  \\mathbf{B} =\n  \\begin{bmatrix}\n  \\mathbf{b}_{1} & \\dots & \\mathbf{b}_{r}\n  \\end{bmatrix}.\n  \nand a matrix \\mathbf{C} \\in \\mathbb{F}^{r \\times n}\n\n  \\mathbf{C} =\n  \\begin{bmatrix}\n  c_{1, 1} & \\dots & c_{n, 1} \\\\\n  \\vdots & \\dots & \\vdots \\\\\n  c_{1, r} & \\dots & c_{n, r} \\\\\n  \\end{bmatrix}\n  \nsuch that the columns of \\mathbf{A} are linear combinations of the basis \\mathbf{B} using the columns of \\mathbf{C} as cofficients\n\n  \\mathbf{A} = \\mathbf{B} \\mathbf{C}.\n  \nBy the defintion of matrix multiplication, the rows of \\mathbf{A} are also linear combinations of rows of \\mathbf{C} using the rows of \\mathbf{B} as coefficients. Thus, by the spanning set property,\n\n  R (\\mathbf{A}^{T}) \\subseteq R (\\mathbf{C}^{T}).\n  \nThen, by the dimension property,\n\n  \\begin{aligned}\n  \\text{dim} (R (\\mathbf{A}^{T}))\n  & \\leq \\text{dim} (R (\\mathbf{C}^{T}))\n  \\\\\n  \\text{rank} (\\mathbf{A}^{T})\n  & \\leq \\text{rank} (\\mathbf{C}^{T}).\n  \\end{aligned}\n  \nSince \\mathbf{C}^{T} \\in \\mathbb{F}^{n \\times n}, according to the rank property\n\n  \\text{rank} (\\mathbf{C}^{T}) \\leq r = \\text{rank} (\\mathbf{A}).\n  \nThus,\n\n  \\text{rank} (\\mathbf{A}^{T}) \\leq \\text{rank} (\\mathbf{A}).\n  \nFinally, we can use the same argument to prove\n\n  \\text{rank} (\\mathbf{A}) \\leq \\text{rank} (\\mathbf{A}^{T}).\n  \nby substituting \\mathbf{A} as \\mathbf{A}^{T}.\nSince \\text{rank} (\\mathbf{A}^{T}) \\leq \\text{rank} (\\mathbf{A}) and \\text{rank} (\\mathbf{A}) \\leq \\text{rank} (\\mathbf{A}^{T}) at the same time, we can prove\n\n  \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T}).\n  \n\n\n\n\n(rank-property-6)=\n\nIf \\mathbf{A} \\in \\mathbb{R}^{m \\times n}, then\n\n  \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\n(rank-property-4)=\n\nIf \\mathbf{A} \\in \\mathbb{R}^{m \\times n} and \\mathbf{B} \\in \\mathbb{R}^{m \\times m} and \\mathbf{C} \\in \\mathbb{R}^{n \\times n} are full rank matrices, then\n\n  \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{B} \\mathbf{A}) = \\text{rank} (\\mathbf{A} \\mathbf{C}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/05_Linear_Map_and_Rank.html#rank-and-matrix-inverse",
    "href": "Linear Algebra/05_Linear_Map_and_Rank.html#rank-and-matrix-inverse",
    "title": "5  Linear Map and Rank",
    "section": "7.2 Rank and matrix inverse",
    "text": "7.2 Rank and matrix inverse\n(rank-property-4)=\n\nSuppose \\mathbf{A} \\in \\mathbb{F}^{m \\times n}.\nThere exists a left inverse matrix \\mathbf{B} \\in \\mathbb{F}^{n \\times m} of \\mathbf{A} such that\n\n  \\mathbf{B} \\mathbf{A} = \\mathbf{I}_{n \\times n},\n  \nif and only if \\text{rank} (\\mathbf{A}) = n.\nThere exists a right inverse matrix \\mathbf{C} \\in \\mathbb{F}^{n \\times m} of \\mathbf{A} such that\n\n  \\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m}.\n  \nif and only if \\text{rank} (\\mathbf{A}) = m.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTo prove this, we first prove that there exists a matrix \\mathbf{C} such that\n\n  \\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m} \\iff \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (A).\n  \nFirst notice that\n\n  \\mathbf{I}_{m \\times m} =\n  \\begin{bmatrix}\n  \\mathbf{e}_{1} & \\dots & \\mathbf{e}_{m}\n  \\end{bmatrix}\n  \nwhere \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} \\in \\mathbb{F}^{m}.\nThus, \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} is a linear combination of the columns of \\mathbf{A}. By the definition of the range space, we can see\n\n  \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} \\in R (\\mathbf{A}).\n  \nConversely, since \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} \\in R (\\mathbf{A}), each \\mathbf{e}_{i}, i = 1, \\dots, m is a linear combination of the columns in \\mathbf{A}.\nThus, there exists a set of coefficients c_{j, i}, j = 1, \\dots, n for the linear combination of \\mathbf{e}_{i}, and the matrix of these coefficients is\n\n  \\mathbf{C} =\n  \\begin{bmatrix}\n  c_{1, 1} & \\dots & c_{1, m} \\\\\n  \\vdots & c_{j, i} & \\vdots \\\\\n  c_{n, 1} & \\dots & c_{n, m} \\\\\n  \\end{bmatrix}.\n  \nThen we prove that\n\n  \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (\\mathbf{A}) \\iff \\text{rank} (\\mathbf{A}) = m.\n  \nAgain notice that \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} is the standard basis of \\mathbb{F}^{m}.\nSince R (\\mathbf{A}) \\subseteq \\mathbb{F}^{m}, all vectors in R (\\mathbf{A}) are linear combinations of \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m}.\nSince we know that \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (\\mathbf{A}), according to the spanning set property,\n\n  R (\\mathbf{A}) = \\text{span} (\\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m}).\n  \nSince by definition \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} are linearly independent, the set \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} is a basis for R (\\mathbf{A}).\nAccording to the cardinality of basis,\n\n  \\text{rank} (\\mathbf{A}) = m.\n  \nConversely, since we know \\text{rank} (\\mathbf{A}) = m = \\text{dim} (\\mathbb{F}^{m} and R (\\mathbf{A}) \\subseteq \\mathbb{F}^{m},\n\n  R (\\mathbf{A}) = \\mathbb{F}^{m}.\n  \nSince by definition \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in \\mathbb{F}^{m},\n\n  \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (\\mathbf{A}).\n  \nIn conclusion, we have proven that there exists a matrix \\mathbf{C} such that\n\n  \\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m} \\iff \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (A) \\iff \\text{rank} (\\mathbf{A}) = m.\n  \n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first notice that\n\n  \\begin{aligned}\n  (\\mathbf{B} \\mathbf{A})^{T}\n  & = \\mathbf{I}_{n \\times n}^{T}\n  \\\\\n  \\mathbf{A}^{T} \\mathbf{B}^{T}\n  & = \\mathbf{I}_{n \\times n}.\n  \\end{aligned}\n  \nBy applying the proof above, we can prove that\n\n  \\text{rank} (\\mathbf{A}^{T}) = n.\n  \nAccording to the rank property,\n\n  \\text{rank} (\\mathbf{A}^{T}) = \\text{rank} (\\mathbf{A}) = n.\n  \n\n\n\n\n(rank-property-5)=\n\nIf \\mathbf{A} \\in \\mathbb{F}^{n \\times n} is a square matrix and has full rank (\\text{rank}(\\mathbf{A}) = n), then \\mathbf{A} has a unique inverse matrix \\mathbf{A}^{-1} such that\n\n  \\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/06_Inner_Product_and_Norm.html",
    "href": "Linear Algebra/06_Inner_Product_and_Norm.html",
    "title": "6  Inner Product and Norm",
    "section": "",
    "text": "7 Inner Product\nA vector space \\mathcal{V} over field \\mathbb{R} or \\mathbb{C} is a inner product space if there exists a function called inner product, denoted\n\\langle \\cdot, \\cdot \\rangle: \\mathcal{V} \\times \\mathcal{V} \\to \\mathbb{R}\nwith the following properties:\nA norm of a vector in a inner product vector space \\mathcal{V} is a function\n\\lVert \\cdot \\rVert \\to \\mathbb{R}^{+}\nwith the following properties:\nA vector space \\mathcal{V} equipped with a norm is called a normed vector space."
  },
  {
    "objectID": "Linear Algebra/06_Inner_Product_and_Norm.html#properties-of-inner-product",
    "href": "Linear Algebra/06_Inner_Product_and_Norm.html#properties-of-inner-product",
    "title": "6  Inner Product and Norm",
    "section": "7.1 Properties of inner product",
    "text": "7.1 Properties of inner product\n(inner-product-property-1)=\n\n\\langle x, \\alpha y + \\alpha z \\rangle = \\overline{\\alpha} \\langle x, y \\rangle + \\overline{\\alpha} \\langle x, z \\rangle\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n  \\begin{aligned}\n  \\langle x, \\alpha y + \\alpha z \\rangle\n  & = \\overline{\\langle \\alpha y + \\alpha z, x \\rangle}\n  & [\\text{Conjugate sym}]\n  \\\\\n  & = \\overline{\\alpha \\langle y, x \\rangle + \\alpha \\langle z, x \\rangle}\n  & [\\text{Linearity of 1st}]\n  \\\\\n  & = \\overline{\\alpha} \\overline{\\langle y, x \\rangle} + \\overline{\\alpha} \\overline{\\langle z, x \\rangle}\n  \\\\\n  & = \\overline{\\alpha} \\langle x, y \\rangle + \\overline{\\alpha} \\langle x, z \\rangle\n  \\end{aligned}\n  \n\n\n\n\n(inner-product-property-2)=\n\nIf \\langle x, y \\rangle = 0 for all x \\in \\mathcal{V}, then y = 0.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/06_Inner_Product_and_Norm.html#lp-norm",
    "href": "Linear Algebra/06_Inner_Product_and_Norm.html#lp-norm",
    "title": "6  Inner Product and Norm",
    "section": "8.1 Lp-norm",
    "text": "8.1 Lp-norm\nGiven \\mathcal{V} = \\mathbb{R}^{n}, define \\lVert \\cdot \\rVert_{p}: \\mathcal{V} \\to \\mathbb{R}^{+} as\n\n\\lVert \\mathbf{v} \\rVert_{p} = \\left(\n    \\sum_{i=1}^{n} \\lvert v_{i} \\rvert^{p}\n\\right)^{\\frac{1}{p}}, \\quad \\mathbf{v} \\in \\mathcal{V},\n\nfor p \\geq 1.\n\nL_{1} norm:\n\n  \\lVert \\mathbf{v} \\rVert_{1} = \\sum_{i = 1}^{n} \\lvert v_{i} \\rvert\n  \nL_{2} norm:\n\n  \\lVert \\mathbf{v} \\rVert_{2} = \\left( \\sum_{i = 1}^{n} \\lvert v_{i} \\rvert^{2} \\right)^{\\frac{1}{2}}"
  },
  {
    "objectID": "Linear Algebra/06_Inner_Product_and_Norm.html#norm-induced-by-inner-product",
    "href": "Linear Algebra/06_Inner_Product_and_Norm.html#norm-induced-by-inner-product",
    "title": "6  Inner Product and Norm",
    "section": "8.2 Norm induced by inner product",
    "text": "8.2 Norm induced by inner product\nGiven an inner product vector space \\mathcal{V}, a norm \\lVert \\cdot \\rVert_{ip}: \\mathcal{V} \\to R^{+} induced by its inner product is\n\n\\lVert v \\rVert_{ip} = \\sqrt{\\langle v, v \\rangle}, \\quad v \\in \\mathcal{V}.\n\n\nFor the vector space \\mathbb{C}^{n}, the norm induced by inner product is the same as L_{2} norm\n\n  \\lVert \\mathbf{v} \\rVert_{ip} = \\sqrt{ \\sum_{i=1}^{n} v_{i} v_{i} } = \\sqrt{ \\sum_{i=1}^{n} \\lvert v_{i} \\rvert^{2} } = \\lVert \\mathbf{v} \\lVert_{2}\n  \nCauchy-Schwarz Inequality: let \\mathcal{V} be an inner product space. Then, for any u, v \\in \\mathcal{V}, we have\n\n  \\lvert \\langle u, v \\rangle \\rvert \\leq \\lVert u \\rVert_{ip} \\lVert v \\rVert_{ip}."
  },
  {
    "objectID": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html",
    "href": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html",
    "title": "7  Orthogonality and Unitary Matrix",
    "section": "",
    "text": "8 Orthogonality\nA set of non-zero vectors v_{1}, \\dots, v_{k} are orthogonal if\n\\langle v_{i}, v_{j} \\rangle = 0, \\quad \\forall i \\neq j.\nA square complex (real) matrix \\mathbf{U} is unitary (orthogonal) if and only if \\mathbf{U} has orthonormal columns."
  },
  {
    "objectID": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#properties-of-orthogonality",
    "href": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#properties-of-orthogonality",
    "title": "7  Orthogonality and Unitary Matrix",
    "section": "8.1 Properties of orthogonality",
    "text": "8.1 Properties of orthogonality\nIf v_{1}, \\dots, v_{k} are orthogonal vectors,\n(orthogonality-property-1)=\n\nv_{1}, \\dots, v_{k} are also linearly independent.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\n(orthogonality-property-2)=\n\nSuppose \\mathcal{S} is a subspace with \\text{dim} (S) = n and v_{1}, \\dots, v_{k} \\in \\mathcal{S}, then the set \\{ v_{1}, \\dots, v_{k} \\} forms a basis of \\mathcal{S}. Then, \\{ v_{1}, \\dots, v_{k} \\} is an orthogonal basis of \\mathcal{S}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#representing-vectors-using-orthogonal-basis",
    "href": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#representing-vectors-using-orthogonal-basis",
    "title": "7  Orthogonality and Unitary Matrix",
    "section": "8.2 Representing vectors using orthogonal basis",
    "text": "8.2 Representing vectors using orthogonal basis\nSuppose \\mathcal{S} is a subspace and \\{ v_{1}, \\dots, v_{n} \\} is an orthogonal basis of \\mathcal{S}, any vector v \\in \\mathcal{S} can be represented using \\{ v_{1}, \\dots, v_{n} \\}:\n\nv = \\sum_{i=1}^{n} \\alpha_{i} v_{i},\n\nwhere\n\n\\alpha_{i} =\n\\frac{\n    \\langle v, v_{i} \\rangle\n}{\n    \\lVert v_{i} \\rVert_{ip}^{2}\n}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#orthonormal-vectors",
    "href": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#orthonormal-vectors",
    "title": "7  Orthogonality and Unitary Matrix",
    "section": "8.3 Orthonormal vectors",
    "text": "8.3 Orthonormal vectors\nA set of vectors v_{1}, \\dots, v_{k} are orthonormal if all vectors in the set are orthogonal to each other, and each vector has the inner product norm of 1.\nIf \\mathbf{a}_{1}, \\dots, \\mathbf{a}_{k} \\in \\mathbb{C}^{n} and are the columns of the matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times k}, then \\mathbf{A} has orthonormal columns and thus\n\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{I}."
  },
  {
    "objectID": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#properties-of-unitary-matrix",
    "href": "Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#properties-of-unitary-matrix",
    "title": "7  Orthogonality and Unitary Matrix",
    "section": "9.1 Properties of unitary matrix",
    "text": "9.1 Properties of unitary matrix\n(unitary-matrix-property-1)=\n\nThe matrix \\mathbf{U} is unitary if and only if \\mathbf{U}^{H} = \\mathbf{U}^{-1}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition, \\mathbf{U} has orthogonal columns and thus linearly independent columns. By the rank property, \\mathbf{U} has a unique inverse matrix \\mathbf{U}^{-1} such that\n\n  \\mathbf{U}^{-1} \\mathbf{U} = \\mathbf{I}_{n \\times n}.\n  \nSince by definition we know\n\n  \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{I}_{n \\times n},\n  \nthen it must follow that\n\n  \\mathbf{U}^{H} = \\mathbf{U}^{-1}.\n  \nThe reverse can be proved backward following the procedure above.\n\n\n\n\n(unitary-matrix-property-2)=\n\nThe matrix \\mathbf{U} is unitary if and only if \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{H} = \\mathbf{I}_{n \\times n}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nFollowing the unitary matrix property, the inverse \\mathbf{U}^{-1} can be both left and right inverse\n\n  \\mathbf{U}^{-1} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{-1} = \\mathbf{I}.\n  \nReplacing \\mathbf{U}^{-1} with \\mathbf{U}^{H}, we have the results:\n\n  \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{H} = \\mathbf{I}.\n  \n\n\n\n\n(unitary-matrix-property-3)=\n\nThe matrix \\mathbf{U} is unitary if and only if \\mathbf{U} \\mathbf{x} doesn’t change the length of \\mathbf{x}:\n\n  \\lVert \\mathbf{U} \\mathbf{x} \\rVert = \\lVert \\mathbf{x} \\rVert.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\n\n  \\begin{aligned}\n  \\lVert \\mathbf{U} \\mathbf{x} \\rVert\n  & = \\sqrt{\\lVert \\mathbf{U} \\mathbf{x} \\rVert^{2}}\n  \\\\\n  & = \\sqrt{\\mathbf{x}^{H} \\mathbf{U}^{H} \\mathbf{U} \\mathbf{x}}\n  \\\\\n  & = \\sqrt{\\mathbf{x}^{H} \\mathbf{I} \\mathbf{x}}\n  & [\\mathbf{U}^{H} \\mathbf{U} = \\mathbf{I}]\n  \\\\\n  & = \\sqrt{\\mathbf{x}^{H} \\mathbf{x}}\n  \\\\\n  & = \\sqrt{\\lVert \\mathbf{x} \\rVert^{2}}\n  \\\\\n  & = \\lVert \\mathbf{x} \\rVert\n  \\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html",
    "href": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html",
    "title": "8  Complementary Subspaces and Projection",
    "section": "",
    "text": "9 Complementary Subspaces\nSubspaces \\mathcal{X}, \\mathcal{Y} of a vector space \\mathcal{V} are complementary if\n\\mathcal{V} = \\mathcal{X} + \\mathcal{Y},\nand\n\\mathcal{X} \\cap \\mathcal{Y} = 0,\nin which case \\mathcal{V} is the direct sum of \\mathcal{X} and \\mathcal{Y} and is denoted as\n\\mathcal{V} = \\mathcal{X} \\oplus \\mathcal{Y}.\nSuppose that \\mathcal{X} and \\mathcal{Y} are complementary subspaces in \\mathcal{V}. Thus, there exists unique x \\in \\mathcal{X} and y \\in \\mathcal{Y} such that v = x + y for every vector v \\in \\mathcal{V}\nThen the unique linear operator \\mathbf{P} \\in \\mathbb{C}^{n \\times n} defined by\n\\mathbf{P} v = x\nis the projection matrix of \\mathcal{V} onto \\mathcal{X} along \\mathcal{Y} and x \\in \\mathcal{X} is the projection of v \\in \\mathcal{V} onto \\mathcal{X} along \\mathcal{Y}."
  },
  {
    "objectID": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html#properties-of-complementary-subspaces",
    "href": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html#properties-of-complementary-subspaces",
    "title": "8  Complementary Subspaces and Projection",
    "section": "9.1 Properties of complementary subspaces",
    "text": "9.1 Properties of complementary subspaces\n(complementary-subspaces-property-1)=\n\n\\mathcal{X} and \\mathcal{Y} are complementary if and only if there exist unique vectors x \\in \\mathcal{X} and y \\in \\mathcal{Y} such that\n\n  v = x + y,\n  \nfor each v \\in \\mathcal{V}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose there are two pairs of x_{1}, x_{2} \\in \\mathcal{X} and y_{1}, y_{2} \\in \\mathcal{Y} such that\n\n  v = x_{1} + y_{1} = x_{2} + y_{2}.\n  \nThen\n\n  \\begin{aligned}\n  x_{1} + y_{1}\n  & = x_{2} + y_{2}\n  \\\\\n  x_{1} - x_{2}\n  & = y_{2} - y_{1}\n  \\\\\n  \\end{aligned}\n  \nAccording to the definition of the subspace\n\n  \\begin{aligned}\n  x_{1}, x_{2} \\in \\mathcal{X}\n  & \\Rightarrow x_{1} - x_{2} \\in \\mathcal{X}\n  \\\\\n  y_{1}, y_{2} \\in \\mathcal{X}\n  & \\Rightarrow y_{1} - y_{2} \\in \\mathcal{Y}.\n  \\end{aligned}\n  \nwhich means\n\n  x_{1} - x_{2} = y_{2} - y_{1} \\Rightarrow x_{1} - x_{2} \\in \\mathcal{Y}.\n  \nThus,\n\n  x_{1} - x_{2} \\in \\mathcal{X} \\cap \\mathcal{Y}.\n  \nHowever, since by definition \\mathcal{X} \\cap \\mathcal{Y} = 0,\n\n  \\begin{aligned}\n  x_{1} - x_{2}\n  & = 0.\n  \\\\\n  x_{1}\n  & =  x_{2}.\n  \\end{aligned}\n  \nSimilar argument can be made for y_{1} and y_{2}.\n\n\n\n\n(complementary-subspaces-property-2)=\n\nSuppose \\mathcal{X} has a basis \\mathcal{B}_{\\mathcal{X}} and \\mathcal{Y} has a basis \\mathcal{B}_{\\mathcal{Y}}. Then \\mathcal{X} and \\mathcal{Y} are complementary if and only if\n\n  \\mathcal{B}_{\\mathcal{X}} \\cap \\mathcal{B}_{\\mathcal{Y}} = \\emptyset\n  \nand\n\n  \\mathcal{B}_{\\mathcal{X}} \\cup \\mathcal{B}_{\\mathcal{Y}}\n  \nis a basis for \\mathcal{V}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html#properties-of-projection-matrix",
    "href": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html#properties-of-projection-matrix",
    "title": "8  Complementary Subspaces and Projection",
    "section": "10.1 Properties of projection matrix",
    "text": "10.1 Properties of projection matrix\n(projection-matrix-property-1)=\n\n\\mathbf{I} - \\mathbf{P} is the complementary projection matrix of \\mathbf{v} onto \\mathcal{Y} along \\mathcal{X}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of projection matrix,\n\n  \\begin{aligned}\n  v\n  & = x + y\n  \\\\\n  & = \\mathbf{P} v + y.\n  \\end{aligned}\n  \nThus,\n\n  \\begin{aligned}\n  y\n  & = v - \\mathbf{P} v\n  \\\\\n  & = (\\mathbf{I} - \\mathbf{P})v.\n  \\end{aligned}\n  \n\n\n\n\n(projection-matrix-property-2)=\n\nR (\\mathbf{P}) = N (\\mathbf{I} - \\mathbf{P}) = \\mathcal{X} and N (\\mathbf{P}) = R (\\mathbf{I} - \\mathbf{P}) = \\mathcal{Y}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince v \\in \\mathcal{V} and \\mathcal{X},\n\n  \\mathbf{P} v = x \\Rightarrow R (\\mathbf{P}) = \\mathcal{X}.\n  \nAlso, for all v \\in N (\\mathbf{I} - \\mathbf{P}),\n\n  \\begin{aligned}\n  (\\mathbf{I} - \\mathbf{P}) v\n  & = 0\n  \\\\\n  v - \\mathbf{P} v\n  & = 0\n  \\\\\n  v - x\n  & = 0\n  \\\\\n  v\n  & = x\n  \\end{aligned}\n  \nwhich means v \\in \\mathcal{X} \\Rightarrow \\mathcal{V} \\subseteq N (\\mathbf{I} - \\mathbf{P}).\nSince N (\\mathbf{I} - \\mathbf{P}) \\subseteq \\mathcal{V},\n\n  N (\\mathbf{I} - \\mathbf{P}) = \\mathcal{V}.\n  \nThe proof for N (\\mathbf{P}) = R (\\mathbf{I} - \\mathbf{P}) = \\mathcal{Y} is the same.\n\n\n\n\n(projection-matrix-property-3)=\n\nFor x \\in \\mathcal{X}, y \\in \\mathcal{Y},\n\n  \\mathbf{P} x = x,\n  \nand\n\n  \\mathbf{P} y = 0.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\n(projection-matrix-property-4)=\n\nA linear operator \\mathbf{P} on \\mathcal{V} is a projection matrix if and only if \\mathbf{P} is idempotent (\\mathbf{P} = \\mathbf{P}^{2}).\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of the projector \n  \\mathbf{P}^{2} v = \\mathbf{P} \\mathbf{P} v = \\mathbf{P} x.\n  \nAccording to the property of projection matrix\n\n  \\mathbf{P} x = x = \\mathbf{P} v.\n  \nThus, \n  \\mathbf{P} v = \\mathbf{P}^{2} v \\Rightarrow \\mathbf{P} = \\mathbf{P}^{2}.\n  \n\n\n\n\n(projection-matrix-property-5)=\n\nIf \\mathcal{V} = \\mathbb{R}^{n} or \\mathbb{C}^{n}, then \\mathbf{P} is given by\n\n  \\mathbf{P} =\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{0}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{Y}\n  \\end{bmatrix}^{-1}\n  =\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{Y}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\mathbf{I} & \\mathbf{0} \\\\\n      \\mathbf{0} & \\mathbf{0}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{Y}\n  \\end{bmatrix}^{-1}\n  \nwhere the columns of \\mathbf{X} and \\mathbf{Y} are respective bases for \\mathcal{X} and \\mathcal{Y}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\n(projection-matrix-property-6)=\n\n\\mathbf{P} is unique for a given \\mathcal{X} and \\mathcal{Y}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html",
    "href": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html",
    "title": "9  Orthogonal Complement and Decomposition",
    "section": "",
    "text": "10 Orthogonal Complement\nFor a subset \\mathcal{M} of an inner-product space \\mathcal{V}, the orthogonal complement \\mathcal{M}^{\\perp} of \\mathcal{M} is defined to be the set of all vectors in \\mathcal{V} that are orthogonal to every vector in \\mathcal{M}\n\\mathcal{M}^{\\perp} = \\left\\{\n    x \\in \\mathcal{V} \\mid \\langle x, m \\rangle = 0, \\forall m \\in \\mathcal{M}\n\\right\\}.\nThe set \\mathcal{M}^{\\perp} is a subspace even if \\mathcal{M} is not.\nFor every \\mathbf{A} \\in \\mathbb{C}^{m \\times n},\nR (\\mathbf{A}) \\perp N (\\mathbf{A}^{H}),\nN (\\mathbf{A}) \\perp R (\\mathbf{A}^{H}),\nwhich means that every matrix \\mathbf{A} \\in \\mathbb{C}^{m \\times n} produces an orthogonal decomposition of \\mathbb{C}^{m} and \\mathbb{C}^{n} in the sense that\n\\mathbb{C}^{m} = R (\\mathbf{A}) \\oplus R (\\mathbf{A})^{\\perp} = R (\\mathbf{A}) \\oplus N (\\mathbf{A}^{H}),\n\\mathbb{C}^{n} = N (\\mathbf{A}) \\oplus N (\\mathbf{A})^{\\perp} = N (\\mathbf{A}) \\oplus R (\\mathbf{A}^{H})."
  },
  {
    "objectID": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-complementary-subspaces",
    "href": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-complementary-subspaces",
    "title": "9  Orthogonal Complement and Decomposition",
    "section": "10.1 Orthogonal Complementary Subspaces",
    "text": "10.1 Orthogonal Complementary Subspaces\nWhen \\mathcal{M} is a subspace of a finite-dimensional inner-product space \\mathcal{V},\n\n\\mathcal{V} = \\mathcal{M} \\oplus \\mathcal{M}^{\\perp}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#properties-of-orthogonal-complementary-subspaces",
    "href": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#properties-of-orthogonal-complementary-subspaces",
    "title": "9  Orthogonal Complement and Decomposition",
    "section": "10.2 Properties of orthogonal complementary subspaces",
    "text": "10.2 Properties of orthogonal complementary subspaces\nSuppose \\mathcal{M} is a subspace of an n-dimensional inner-product space \\mathcal{V}.\n(orthogonal-complementary-subspaces-property-1)=\n\n\\text{dim} (\\mathcal{M}) + \\text{dim} (\\mathcal{M}^{\\perp}) = n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathcal{M}^{T} and \\mathcal{M} are complementary subspaces, the property of complementary subspace shows that\n\n  \\mathcal{B}_{\\mathcal{M}} \\cup \\mathcal{B}_{\\mathcal{M}^{\\perp}} = \\mathcal{B}_{\\mathcal{V}}\n  \nand\n\n  \\mathcal{B}_{\\mathcal{M}} \\cap \\mathcal{B}_{\\mathcal{M}^{\\perp}} = \\emptyset.\n  \nThus,\n\n  \\begin{aligned}\n  \\lvert \\mathcal{B}_{\\mathcal{M}} \\rvert + \\lvert \\mathcal{B}_{\\mathcal{M}^{\\perp}} \\rvert\n  & = \\lvert \\mathcal{B}_{\\mathcal{V}} \\rvert\n  \\\\\n  \\text{dim} (\\mathcal{M}) + \\text{dim} (\\mathcal{M}^{\\perp})\n  & = n\n  \\end{aligned}\n  \n\n\n\n\n(orthogonal-complementary-subspaces-property-2)=\n\n\\mathcal{M}^{\\perp^{\\perp}} = \\mathcal{M}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathcal{M}^{\\perp^{\\perp}} \\subseteq \\mathcal{V},\n\n  x \\in \\mathcal{M}^{\\perp^{\\perp}} \\Rightarrow x \\in \\mathcal{V}.\n  \nSince \\mathcal{M} and \\mathcal{M}^{\\perp} are complementary subspaces, every x \\in \\mathcal{V} can be uniquely represented by m \\in \\mathcal{M} and n \\in \\mathcal{M}^{\\perp}\n\n  x = m + n.\n  \nSince \\mathcal{M}^{\\perp^{\\perp}} \\perp \\mathcal{M}^{\\perp}, by definition\n\n  \\begin{aligned}\n  0\n  & = \\langle x, n \\rangle\n  \\\\\n  & = \\langle m + n, n \\rangle\n  \\\\\n  & = \\langle m, n \\rangle + \\langle n, n \\rangle\n  \\\\\n  & = \\langle n, n \\rangle\n  & [\\mathcal{M} \\perp \\mathcal{M}^{\\perp} \\Rightarrow \\langle m, n \\rangle = 0].\n  \\end{aligned}\n  \nBy the definition of the inner product,\n\n  \\langle n, n \\rangle = 0 \\Rightarrow n = 0.\n  \nThus, for each x \\in \\mathcal{M}^{\\perp^{\\perp}}\n\n  x = m \\in \\mathcal{M} \\Rightarrow \\mathcal{M}^{\\perp^{\\perp}} \\subseteq \\mathcal{M}.\n  \nBy the property\n\n  \\text{dim} (\\mathcal{M}) = n - \\text{dim} (\\mathcal{M}^{\\perp}) = \\text{dim} (\\mathcal{M}^{\\perp^{\\perp}}).\n  \nThen by the property,\n\n  \\mathcal{M} = \\mathcal{M}^{\\perp^{\\perp}}."
  },
  {
    "objectID": "Linear Algebra/10_URV_Factorization_and_SVD.html",
    "href": "Linear Algebra/10_URV_Factorization_and_SVD.html",
    "title": "10  URV Factorization and SVD",
    "section": "",
    "text": "11 URV Factorization\nFor each A \\in \\mathbb{R}^{m \\times n} of rank r, there are orthogonal matrices \\mathbf{U}_{m \\times m} and \\mathbf{V}_{n \\times n} and a nonsingular matrix \\mathbf{C}_{r \\times r} such that\n\n\\mathbf{A} = \\mathbf{U} \\mathbf{R} \\mathbf{V}^{T} = \\mathbf{U}\n\\begin{bmatrix}\n\\mathbf{C}_{r \\times r} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}_{m \\times n}\n\\mathbf{V}^{T}.\n\n\nThe first r columns in \\mathbf{U} are an orthonormal basis for R (\\mathbf{A}).\nThe last m - r columns of \\mathbf{U} are an orthonormal basis for N (\\mathbf{A}^{T}).\nThe first r columns in \\mathbf{V} are an orthonormal basis for R (\\mathbf{A}^{T}).\nThe last m - r columns of \\mathbf{V} are an orthonormal basis for N (\\mathbf{A}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose vector spaces \\mathbb{R}^{m} and \\mathbb{R}^{n} have orthonormal bases \\{ \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{m} \\} and \\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\}, respectively.\nGiven any matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n}, the orthogonal decomposition theorem shows that the following subspaces are complementary in \\mathbb{R}^{m} and \\mathbb{R}^{n}.\n\nR (\\mathbf{A}) \\oplus N (\\mathbf{A}^{T}) = \\mathbb{R}^{m},\n\n\nR (\\mathbf{A}^{T}) \\oplus N (\\mathbf{A}) = \\mathbb{R}^{n}.\n\nAccording to the property of the complementary subspaces, we can separate the basis for \\mathbb{R}^{m} into the basis for R (\\mathbf{A}) and N (\\mathbf{A}^{T}), and the basis for \\mathbb{R}^{n} into the basis for N (\\mathbf{A}) and R (\\mathbf{A}^{T}).\nSince \\mathrm{rank} (\\mathbf{A}) = r, suppose the bases are separated in the following way:\n\n\\mathcal{B}_{R (\\mathbf{A})} = \\{ \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{r} \\},\n\\mathcal{B}_{N (\\mathbf{A}^{T})} = \\{ \\mathbf{u}_{r + 1}, \\dots , \\mathbf{u}_{m} \\},\n\\mathcal{B}_{R (\\mathbf{A}^{T})} = \\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{r} \\},\n\\mathcal{B}_{N (\\mathbf{A})} = \\{ \\mathbf{v}_{r + 1}, \\dots , \\mathbf{v}_{n} \\}.\n\nNow we treat the bases for \\mathbb{R}^{m} and \\mathbb{R}^{n} as the columns of the matrices \\mathbf{U} and \\mathbf{V}:\n\n\\mathbf{U} =\n\\begin{bmatrix}\n\\mathbf{u}_{1} & \\dots & \\mathbf{u}_{m}\n\\end{bmatrix},\n\n\n\\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{v}_{1} & \\dots & \\mathbf{v}_{n}\n\\end{bmatrix},\n\nand define\n\n\\mathbf{R} = \\mathbf{U}^{T} \\mathbf{A} \\mathbf{V}.\n\nNote that\n\n\\mathbf{A} \\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{A} \\mathbf{v}_{n}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{A} \\mathbf{v}_{r} & 0 & \\dots & 0\n\\end{bmatrix}\n\n\n\\mathbf{U}^{T} \\mathbf{A} = (\\mathbf{A}^{T} \\mathbf{U})^{T} =\n\\begin{bmatrix}\n\\mathbf{A}^{T} \\mathbf{u}_{1} & \\dots & \\mathbf{A}^{T} \\mathbf{u}_{n}\n\\end{bmatrix}^{T} =\n\\begin{bmatrix}\n\\mathbf{A}^{T} \\mathbf{u}_{1} & \\dots & \\mathbf{A}^{T} \\mathbf{u}_{r} & 0 & \\dots & 0\n\\end{bmatrix}^{T}\n\nsince \\mathbf{v}_{r + 1}, \\dots, \\mathbf{v}_{n} \\in N (\\mathbf{A}), and \\mathbf{u}_{r + 1}, \\dots, \\mathbf{u}_{m} \\in N (\\mathbf{A}^{T}).\nThus, \\mathbf{R} has mostly 0 except the top left corner:\n\n\\mathbf{R} = \\mathbf{U}^{T} \\mathbf{A} \\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{u}_{1}^{T} \\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{u}_{1}^{T} \\mathbf{A} \\mathbf{v}_{r} & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\mathbf{u}_{r}^{T} \\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{u}_{r}^{T} \\mathbf{A} \\mathbf{v}_{r} & 0 & \\dots & 0 \\\\\n0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{C} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}.\n\nThen we can see \\mathbf{A} can always be decomposed in terms of \\mathbf{U}, \\mathbf{R}, \\mathbf{V} because of the property of orthogonal matrix\n\n\\begin{aligned}\n\\mathbf{U} \\mathbf{R} \\mathbf{V}^{T}\n& = \\mathbf{U} \\mathbf{U}^{T} \\mathbf{A} \\mathbf{V} \\mathbf{V}^{T}\n\\\\\n& = \\mathbf{U} \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{V} \\mathbf{V}^{-1}\n\\\\\n& = \\mathbf{A}.\n\\end{aligned}\n\nTo see why \\mathbf{C} is always non-singular, we first note that\n\n\\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{U}^{T} \\mathbf{A} \\mathbf{V}) = r\n\nbecause the multiplication by a full-rank square matrix preserves rank.\nThus,\n\n\\text{rank} (\\mathbf{C}) = \\text{rank} \\left(\n    \\begin{bmatrix}\n    \\mathbf{C} & 0 \\\\\n    0 & 0 \\\\\n    \\end{bmatrix}\n\\right) = \\text{rank} (\\mathbf{U}^{T} \\mathbf{A} \\mathbf{V}) = r.\n\n\n\n\n\n\n12 Singular Value Decomposition (SVD)\nSingular value decomposition is a special case of the URV factorization where the \\mathbf{C} matrix is a diagonal matrix with increasing values in the diagonal.\nFor each \\mathbf{A} \\in \\mathbb{C}^{m \\times n} of rank r, there are orthogonal matrices \\mathbf{U} \\in \\mathbb{R}^{m \\times m} and \\mathbf{V} \\in \\mathbb{R}^{n \\times n}, and a diagonal matrix \\mathbf{D} \\in \\mathbb{C}^{r \\times r} = \\text{diag} (\\sigma_{1}, \\dots, \\sigma_{r}) such that\n\n\\mathbf{A} = \\mathbf{U}\n\\begin{bmatrix}\n\\mathbf{D} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\mathbf{V}^{H}\n\nwith\n\n\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots, \\geq \\sigma_{r},\n\nwhere\n\nthe columns in \\mathbf{U} and \\mathbf{V} are singular vectors and\nthe diagonal values of \\mathbf{D} (\\sigma_{i}) are singular values."
  },
  {
    "objectID": "Linear Algebra/11_Pseudoinverse.html",
    "href": "Linear Algebra/11_Pseudoinverse.html",
    "title": "11  Pseudoinverse",
    "section": "",
    "text": "12 Pseudoinverse\nA generalized inverse for any matrix can be defined using URV factorization or SVD.\nGiven a URV factorization of matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n}\n\\mathbf{A} = \\mathbf{U}\n\\begin{bmatrix}\n\\mathbf{C} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\mathbf{V}^{T}\nthe pseudoinverse of \\mathbf{A} is defined as\n\\mathbf{A}^{\\dagger} = \\mathbf{V}\n\\begin{bmatrix}\n\\mathbf{C}^{-1} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\mathbf{U}^{T}.\nThe pseudoinverse of matrix \\mathbf{A} can also be stated as a matrix \\mathbf{A}^{\\dagger} that satisfies the following:\n\\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A},\n\\mathbf{A}^{\\dagger} \\mathbf{A} \\mathbf{A}^{\\dagger} = \\mathbf{A}^{\\dagger},\nwhere \\mathbf{A} \\mathbf{A}^{\\dagger} and \\mathbf{A}^{\\dagger} \\mathbf{A} are symmetric matrix:\n(\\mathbf{A} \\mathbf{A}^{\\dagger})^{T} = \\mathbf{A} \\mathbf{A}^{\\dagger},\n(\\mathbf{A}^{\\dagger} \\mathbf{A})^{T} = \\mathbf{A}^{\\dagger} \\mathbf{A}."
  },
  {
    "objectID": "Linear Algebra/11_Pseudoinverse.html#properties-of-pseudoinverse",
    "href": "Linear Algebra/11_Pseudoinverse.html#properties-of-pseudoinverse",
    "title": "11  Pseudoinverse",
    "section": "12.1 Properties of pseudoinverse",
    "text": "12.1 Properties of pseudoinverse\n\nThe pseudoinverse \\mathbf{A}^{\\dagger} of \\mathbf{A} is unique.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction. Suppose that there are 2 pseudoinverse \\mathbf{B}, \\mathbf{C} for \\mathbf{A}.\n\n  \\begin{aligned}\n  \\mathbf{A} \\mathbf{B}\n  & = (\\mathbf{A} \\mathbf{C} \\mathbf{A}) \\mathbf{B}\n  & [\\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A}]\n  \\\\\n  & = (\\mathbf{C}^{T} \\mathbf{A}^{T}) (\\mathbf{B}^{T} \\mathbf{A}^{T})\n  & [(\\mathbf{A} \\mathbf{A}^{\\dagger})^{T} = \\mathbf{A} \\mathbf{A}^{\\dagger}]\n  \\\\\n  & = \\mathbf{C}^{T} (\\mathbf{A}^{T} \\mathbf{B}^{T} \\mathbf{A}^{T})\n  \\\\\n  & = \\mathbf{C}^{T} (\\mathbf{A} \\mathbf{B} \\mathbf{A})^{T}\n  \\\\\n  & = \\mathbf{C}^{T} \\mathbf{A}^{T}\n  \\\\\n  & = \\mathbf{A} \\mathbf{C}\n  \\\\\n  \\end{aligned}\n  \n\n  \\begin{aligned}\n  \\mathbf{B} \\mathbf{A}\n  & = \\mathbf{B} (\\mathbf{A} \\mathbf{C} \\mathbf{A})\n  & [\\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A}]\n  \\\\\n  & = (\\mathbf{A}^{T} \\mathbf{B}^{T}) (\\mathbf{A}^{T} \\mathbf{C}^{T})\n  & [(\\mathbf{A} \\mathbf{A}^{\\dagger})^{T} = \\mathbf{A} \\mathbf{A}^{\\dagger}]\n  \\\\\n  & = (\\mathbf{A}^{T} \\mathbf{B}^{T} \\mathbf{A}^{T}) \\mathbf{C}^{T}\n  \\\\\n  & = (\\mathbf{A} \\mathbf{B} \\mathbf{A})^{T} \\mathbf{C}^{T}\n  \\\\\n  & = \\mathbf{A}^{T} \\mathbf{C}^{T}\n  \\\\\n  & = \\mathbf{C} \\mathbf{A}\n  \\\\\n  \\end{aligned}\n  \nThus,\n\n  \\begin{aligned}\n  \\mathbf{B}\n  & = \\mathbf{B} \\mathbf{A} \\mathbf{B}\n  & [\\mathbf{A}^{\\dagger} \\mathbf{A} \\mathbf{A}^{\\dagger} = \\mathbf{A}^{\\dagger}]\n  \\\\\n  & = \\mathbf{C} \\mathbf{A} \\mathbf{B}\n  & [\\mathbf{C} \\mathbf{A} = \\mathbf{B} \\mathbf{A}]\n  \\\\\n  & = \\mathbf{C} \\mathbf{A} \\mathbf{C}\n  & [\\mathbf{A} \\mathbf{B} = \\mathbf{A} \\mathbf{C}]\n  \\\\\n  & = \\mathbf{C}\n  \\end{aligned}\n  \n\n\n\nGiven a full rank matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n}.\nIf m &gt; n, the left inverse of \\mathbf{A} is the pseudoinverse of \\mathbf{A} and is written as\n\n  (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T}.\n  \nIf m &lt; n, the right inverse of \\mathbf{A} is the pseudoinverse of \\mathbf{A} and is written as\n\n  \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} .\n  \nIf m = n, the inverse of \\mathbf{A} is the pseudoinverse of \\mathbf{A}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first prove that (\\mathbf{A}^{T} \\mathbf{A})^{-1} (when m &gt; n) and (\\mathbf{A} \\mathbf{A}^{T})^{-1} (when m &lt; n) exist.\nSince \\mathbf{A} is a full rank matrix,\n\n  \\text{rank} (\\mathbf{A}) = \\min (m, n).\n  \nAccording to the property of rank,\n\n  \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}) = \\text{rank} (\\mathbf{A} \\mathbf{A}^{T}) = \\text{rank} (\\mathbf{A}) = \\text{min} (m, n).\n  \nThus, \\mathbf{A}^{T} \\mathbf{A} (m &gt; n) and \\mathbf{A} \\mathbf{A}^{T} (m &lt; n) are non-singular.\n\nif m &gt; n, \\mathbf{A}^{T} \\mathbf{A} \\in \\mathbb{R}^{n \\times n} is full rank because \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}) = \\min (m, n) = n,\nif m &lt; n, \\mathbf{A} \\mathbf{A}^{T} \\in \\mathbb{R}^{m \\times m} is full rank because \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}) = \\min (m, n) = m.\n\nwe prove that (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} when (m &gt; n) is the pseudoinverse of \\mathbf{A}.\n\n  \\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} = \\mathbf{A} \\mathbf{I}_{n \\times n} = \\mathbf{A}\n  \n\n  \\mathbf{A}^{\\dagger} \\mathbf{A} \\mathbf{A}^{\\dagger} = (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} = \\mathbf{I}_{n \\times n} \\mathbf{A}^{\\dagger} = \\mathbf{A}^{\\dagger}\n  \n\n  \\mathbf{A}^{\\dagger} \\mathbf{A} = (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} = \\mathbf{I}_{n \\times n} = \\mathbf{A}^{T} \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} = \\mathbf{A}^{T} (\\mathbf{A}^{\\dagger})^{\\mathbf{T}}= (\\mathbf{A}^{\\dagger} \\mathbf{A})^{T}\n  \n\n  \\mathbf{A} \\mathbf{A}^{\\dagger} = \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} = (\\mathbf{A}^{\\dagger})^{T} \\mathbf{A}^{T} = (\\mathbf{A} \\mathbf{A}^{\\dagger})^{T}\n  \nThe case that \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} when (m &lt; n) can be proved similarly.\nThen we prove that (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} is the left inverse of \\mathbf{A} when m &gt; n:\n\n  \\mathbf{A}^{\\dagger} \\mathbf{A} = (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} = \\mathbf{I}_{n \\times n}.\n  \nThe case that \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} is the right inverse of \\mathbf{A} when m &lt; n can be proved similarly.\nThus we have proved that (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} (m &gt; n) and \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} (m &lt; n) are the left and right inverse of \\mathbf{A}.\nWhen m = n, we have both\n\n  \\mathbf{A} \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{A},\n  \n\n  \\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A},\n  \nbut since \\mathbf{A}^{-1} and \\mathbf{A}^{\\dagger} are both unique, it must be that\n\n  \\mathbf{A}^{-1} = \\mathbf{A}^{\\dagger}."
  },
  {
    "objectID": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html",
    "href": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html",
    "title": "12  Orthogonal and Affine Projection",
    "section": "",
    "text": "13 Orthogonal projection\nSuppose \\mathcal{M} is a subspace in the vector space \\mathcal{V}. Since \\mathcal{M}^{\\perp} is a orthogonal complementary subspace of \\mathcal{M}, we have \\mathcal{M} \\oplus \\mathcal{M}^{\\perp} = \\mathcal{V}.\nFor v \\in \\mathcal{V}, let v = m + n, where m \\in \\mathcal{M} and n \\in \\mathcal{M}^{\\perp}. We define a linear operator \\mathbf{P}_{\\mathcal{M}} \\in \\mathbb{C}^{n \\times n} such that\n\\mathbf{P}_{\\mathcal{M}} v = m.\nwhere"
  },
  {
    "objectID": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#properties-of-orthogonal-projection",
    "href": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#properties-of-orthogonal-projection",
    "title": "12  Orthogonal and Affine Projection",
    "section": "13.1 Properties of orthogonal projection",
    "text": "13.1 Properties of orthogonal projection\nSuppose m is the orthogonal projection of v on the subspace \\mathcal{M}\n\nm = \\mathbf{P}_{\\mathcal{M}} v.\n\n(orthogonal-projection-property-1)=\n\nm always exists and is unique.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\n(orthogonal-projection-property-2)=\n\nOrthogonality of the difference.\n\n  \\langle v - m, x \\rangle = 0, \\quad \\forall x \\in \\mathcal{M}\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nSince n \\in \\mathcal{M}^{\\perp}, by definition of the orthogonal complement of subspaces,\n\n  \\langle n, x \\rangle = 0, \\quad \\forall x \\in \\mathcal{M}.\n  \nSince by definition v = m + n,\n\n  \\langle v - m, x \\rangle = 0, \\quad \\forall x \\in \\mathcal{M}.\n  \n\n\n\n\n(orthogonal-projection-property-3)=\n\nClosest point theorem: m is the closest point in \\mathcal{M} to v in terms of the ip norm:\n\n  m = \\min_{x \\in \\mathcal{M}} \\lVert v - x \\rVert.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the property of orthogonal projection, the projection m always exists.\nThus, we can add and subtract m,\n\n  \\begin{aligned}\n  \\lVert v - x \\rVert^{2}\n  & = \\lVert v - m + m - x \\rVert^{2} \\quad \\forall x \\in \\mathcal{M}\n  \\\\\n  & = \\langle v - m + m - x, v - m + m - x \\rangle\n  \\\\\n  & = \\langle v - m, v - m + m - x \\rangle + \\langle m - x, v - m + m - x \\rangle\n  \\\\\n  & = \\lVert v - m \\rVert^{2} + \\langle v - m, m - x \\rangle + \\langle m - x, v - m \\rangle + \\lVert m - x \\rVert^{2}\n  \\\\\n  \\end{aligned}\n  \nSince m - x \\in \\mathcal{M}, according to the property of orthogonal projection,\n\n  \\langle v - m, m - x \\rangle = \\langle m - x, v - m \\rangle = 0.\n  \nThus,\n\n  \\begin{aligned}\n  \\lVert v - x \\rVert^{2}\n  & = \\lVert v - m \\rVert^{2} + \\lVert m - x \\rVert^{2}\n  \\\\\n  \\lVert v - x \\rVert^{2}\n  & \\geq \\lVert v - m \\rVert^{2}\n  & [\\lVert m - x \\rVert^{2} \\geq 0]\n  \\end{aligned}\n  \nwhich shows that\n\n  \\lVert v - m \\rVert^{2}\n  \nminimizes the distances between v to all vectors x in the subspace \\mathcal{M}."
  },
  {
    "objectID": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#orthogonal-projection-matrix",
    "href": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#orthogonal-projection-matrix",
    "title": "12  Orthogonal and Affine Projection",
    "section": "13.2 Orthogonal projection matrix",
    "text": "13.2 Orthogonal projection matrix\nIf \\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of v onto the subspace \\mathcal{M}, and the columns of \\mathbf{M} \\in \\mathbb{C}^{n \\times r} are r bases for \\mathcal{M}, then\n\n\\mathbf{P}_{\\mathcal{M}} = \\mathbf{M} (\\mathbf{M}^{H} \\mathbf{M})^{-1} \\mathbf{M}^{H}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\text{rank} (\\mathbf{M}^{H} \\mathbf{H}) = \\text{rank} (\\mathcal{M}) = r and \\mathbf{M}^{H} \\mathbf{H} \\in \\mathbb{C}^{r \\times r}, according to the property of rank, (\\mathbf{M}^{H} \\mathbf{H})^{-1} exists.\nSuppose the columns of \\mathbf{N} \\in \\mathbb{C}^{n \\times s} are orthonormal bases for \\mathcal{M}^{\\perp}, where s = n - r according to the property of complementary subspaces.\nThus, we can construct the below matrix\n\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}.\n\nAccording to the definition of complementary subspaces, \\mathbf{N}^{T} \\mathbf{M} = 0 and \\mathbf{M}^{T} \\mathbf{N} = 0.\nThus,\n\n\\begin{aligned}\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{N} \\\\\n\\end{bmatrix}\n& =\n\\begin{bmatrix}\n    \\mathbf{I}_{r \\times r} & \\mathbf{0} \\\\\n    \\mathbf{0} & \\mathbf{I}_{s \\times s} \\\\\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{N} \\\\\n\\end{bmatrix}^{-1}\n& =\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}.\n\\end{aligned}\n\nSince \\mathcal{M} and \\mathcal{M}^{\\perp} are complementary subspaces, by the definition of projector,\n\n\\begin{aligned}\n\\mathbf{P}_{\\mathcal{M}}\n& =\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{N}\n\\end{bmatrix}^{-1} \\\\\n& =\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}\n\\\\\n& = \\mathbf{M} (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H}.\n\\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#properties-of-orthogonal-projection-matrix",
    "href": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#properties-of-orthogonal-projection-matrix",
    "title": "12  Orthogonal and Affine Projection",
    "section": "13.3 Properties of orthogonal projection matrix",
    "text": "13.3 Properties of orthogonal projection matrix\n(orthogonal-projection-matrix-property-1)=\n\n\\mathbf{P} is a orthogonal projection matrix if and only if\n\n  R (\\mathbf{P}) = N (\\mathbf{P})^{\\perp}.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\n(orthogonal-projection-matrix-property-2)=\n\n\\mathbf{P} is a orthogonal projection matrix if and only if\n\n  \\mathbf{P}^{H} = \\mathbf{P}.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the orthogonal decomposition theorem,\n\n  \\begin{aligned}\n  R (\\mathbf{P})^{\\perp}\n  & = N (\\mathbf{P}^{H})\n  \\\\\n  R (\\mathbf{P})\n  & = N (\\mathbf{P}^{H})^{\\perp}.\n  \\\\\n  \\end{aligned}\n  \nAccording to the property of the orthogonal projection matrix,\n\n  \\begin{aligned}\n  N (\\mathbf{P})^{\\perp}\n  & = N (\\mathbf{P}^{H})^{\\perp}\n  \\\\\n  \\mathbf{P}\n  & = \\mathbf{P}^{H}.\n  \\\\\n  \\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#application-least-square-problem",
    "href": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#application-least-square-problem",
    "title": "12  Orthogonal and Affine Projection",
    "section": "13.4 Application: least square problem",
    "text": "13.4 Application: least square problem\nConsider the problem of solving a system of linear equation for \\mathbf{x} \\in \\mathbb{C}^{n} given \\mathbf{y} \\in \\mathbb{C}^{m} and \\mathbf{A} \\in \\mathbb{C}^{m \\times n},\n\n\\mathbf{y} = \\mathbf{A} \\mathbf{x}.\n\nThis problem has a solution only when \\mathbf{y} \\in R (\\mathbf{A}). When it has no solution, the objective is changed to solve the least square problem\n\n\\mathbf{x}^{*} = \\min_{\\mathbf{x} \\in \\mathbb{C}^{n}} \\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2}^{2}\n\nso that \\mathbf{A} \\mathbf{x} can be as close to \\mathbf{y} as possible.\nSolving the least square problem is the same as solving an orthogonal projection problem,\n\n\\begin{aligned}\n\\mathbf{x}^{*}\n& = \\min_{\\mathbf{x} \\in \\mathbb{C}^{n}} \\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2}^{2}\n\\\\\n& = \\min_{\\mathbf{x} \\in \\mathbb{C}^{n}} \\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2}\n& [\\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2} \\geq 0]\n\\\\\n\\mathbf{z}^{*}\n& = \\min_{\\mathbf{z} \\in R (\\mathbf{A})} \\lVert \\mathbf{y} - \\mathbf{z} \\rVert_{2}\n& [\\mathbf{z} = \\mathbf{A} \\mathbf{x}, \\mathbf{z}^{*} = \\mathbf{A} \\mathbf{x}^{*}].\n\\end{aligned}\n\nwhich is the problem of finding the closest point of \\mathbf{y} on R (\\mathbf{A}),\n\n\\begin{aligned}\n\\mathbf{z}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}\n\\\\\n\\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}.\n\\end{aligned}\n\nWe can then deduce the system of normal equations:\n\n\\mathbf{A} \\mathbf{x}^{*} = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y} \\iff \\mathbf{A}^{H} \\mathbf{A} \\mathbf{x}^{*} = \\mathbf{A}^{H} \\mathbf{y}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst multiplying both ends by \\mathbf{P}_{R (\\mathbf{A})} and use the projector property,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}\n\\\\\n\\mathbf{P}_{R (\\mathbf{A})} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})}^{2} \\mathbf{y}\n\\\\\n\\mathbf{P}_{R (\\mathbf{A})} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}.\n& [\\mathbf{P}_{R (\\mathbf{A})}^{2} = \\mathbf{P}_{R (\\mathbf{A})}]\n\\\\\n\\end{aligned}\n\nThen,\n\n\\begin{aligned}\n\\mathbf{P}_{R (\\mathbf{A})} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}.\n\\\\\n\\mathbf{P}_{R (\\mathbf{A})} (\\mathbf{A} \\mathbf{x}^{*} - \\mathbf{y})\n& = 0\n\\\\\n\\end{aligned}\n\nwhich shows that\n\n\\mathbf{A} \\mathbf{x}^{*} - \\mathbf{y} \\in N (\\mathbf{P}_{R (\\mathbf{A})}).\n\nAccording to the property of the orthogonal projection matrix,\n\nN (\\mathbf{P}_{R (\\mathbf{A})}) = R (\\mathbf{A})^{\\perp} = N (\\mathbf{A}^{H}),\n\nwhich means\n\n\\begin{aligned}\n\\mathbf{A}^{H} (\\mathbf{A} \\mathbf{x}^{*} - \\mathbf{y})\n& = 0\n\\\\\n\\mathbf{A}^{H} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{A}^{H} \\mathbf{y}.\n\\end{aligned}\n\n\n\n\n(affine-projection)="
  },
  {
    "objectID": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#affine-space",
    "href": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#affine-space",
    "title": "12  Orthogonal and Affine Projection",
    "section": "14.1 Affine space",
    "text": "14.1 Affine space\nA set of vectors in the vector space \\mathcal{V} forms an affine space \\mathcal{A} if they are the sums of the vectors in a subspace \\mathcal{M} \\subset \\mathcal{V} and a non-zero vector v \\in \\mathcal{V},\n\n\\mathcal{A} = v + \\mathcal{M}.\n\nThat is, all vectors in \\mathcal{A} are sums of vectors in \\mathcal{M} and v.\n\n\\mathcal{A} is a subspace as it does NOT necessarily contain 0 vector.\n\\mathcal{A} can visualized as the subspace \\mathcal{M} translated away from origin through v."
  },
  {
    "objectID": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#affine-projection-1",
    "href": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#affine-projection-1",
    "title": "12  Orthogonal and Affine Projection",
    "section": "14.2 Affine projection",
    "text": "14.2 Affine projection\nAlthough affine spaces are not a subspaces, the concept of orthogonal projection can also be applied to affine spaces.\nGiven a vector b \\in \\mathcal{V} and an affine space \\mathcal{A}, the affine projection a \\in \\mathcal{A} is the orthogonal projection of b onto the affine space \\mathcal{A} and can be expressed as\n\na = v + \\mathbf{P}_{\\mathcal{M}} (b - v),\n\nwhere \\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of \\mathcal{M}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the affine space \\mathcal{A} is the set of vectors translated from \\mathcal{M} through the vector v, the affine projection a of b onto \\mathcal{A} is the translated version of orthogonal projection of the translated version of b onto the translated version of \\mathcal{A}.\nThus, the affine projection onto \\mathcal{A} is a orthogonal projection onto \\mathcal{M} with a translated input and output.\nBy the definition of the orthogonal projection,\n\na - v = \\mathbf{P}_{\\mathcal{M}} (b - v)\n\nwhere a - v is the translated version of a, b - v the translated version of b, and \\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of subspace \\mathcal{M}.\nThus, the affine projection a of b onto \\mathcal{A} can be expressed as\n\na = v + \\mathbf{P}_{\\mathcal{M}} (b - v)."
  },
  {
    "objectID": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#hyperplanes",
    "href": "Linear Algebra/12_Orthogonal_and_Affine_Projection.html#hyperplanes",
    "title": "12  Orthogonal and Affine Projection",
    "section": "14.3 Hyperplanes",
    "text": "14.3 Hyperplanes\nAn affine space \\mathcal{H} = \\mathbf{v} + \\mathcal{M} \\subseteq \\mathbb{R}^{n} for which \\text{dim} (\\mathcal{M}) = n - 1 is called a hyperplane, and is usually expressed as the set\n\n\\mathcal{H} = \\{ \\mathbf{x} | \\mathbf{w}^{T} \\mathbf{x} = \\beta \\}\n\nwhere \\beta is a scalar and \\mathbf{w} is a non-zero vector.\nIn this case, the hyperplane can be viewed as the subspace\n\n\\mathcal{M} = \\mathbf{w}^{\\perp}\n\ntranslated by the vector\n\n\\mathbf{v} = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe set \\mathcal{H} contains the general solutions of the linear system \\mathbf{w}^{T} \\mathbf{x} = \\beta.\nA particular solution to the linear system is\n\n\\begin{aligned}\n\\mathbf{x}\n& = (\\mathbf{w}^{T})^{-1} \\beta\n\\\\\n& = (\\mathbf{w}^{T})^{T} (\\mathbf{w}^{T} (\\mathbf{w}^{T})^{T})^{-1} \\beta\n\\\\\n& = \\mathbf{w} (\\mathbf{w}^{T} \\mathbf{w})^{-1} \\beta\n\\\\\n& = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n& [\\beta, \\mathbf{w}^{T} \\mathbf{w} \\text{ are scalars}].\n\\end{aligned}\n\nAccording to the orthogonal decomposition theorem, the general solution to the linear system can be expressed as\n\n\\begin{aligned}\nN (\\mathbf{w}^{T})\n& = R (\\mathbf{w})^{\\perp}\n\\\\\n& = \\{ \\mathbf{x} \\mid \\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0, \\forall \\mathbf{y} \\in R (\\mathbf{w}) \\}\n& [\\text{def of perp}]\n\\\\\n& = \\{ \\mathbf{x} \\mid \\langle \\mathbf{x}, \\alpha \\mathbf{w} \\rangle = 0, \\forall \\alpha \\in \\mathbb{R} \\}\n& [\\text{def of } R (\\mathbf{w})]\n\\\\\n& = \\{ \\mathbf{x} \\mid \\alpha \\langle \\mathbf{x}, \\mathbf{w} \\rangle = 0, \\forall \\alpha \\in \\mathbb{R} \\}\n& [\\text{def of } \\langle \\cdot, \\cdot \\rangle]\n\\\\\n& = \\{ \\mathbf{x} \\mid \\langle \\mathbf{x}, \\mathbf{w} \\rangle = 0 \\}\n\\\\\n& = \\mathbf{w}^{\\perp}.\n\\end{aligned}\n\nSince the general solution of any linear system is the sum of a particular solution and the general solution of the associated homogeneous equation, the set of general solutions is expressed as\n\n\\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w} + \\mathbf{w}^{\\perp}.\n\nThus,\n\n\\mathcal{H} = \\mathbf{v} + \\mathcal{M},\n\nwhere\n\n\\mathbf{v} = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w},\n\n\n\\mathcal{M} = \\mathbf{w}^{\\perp}.\n\nTODO: explain the general solution of any linear system.\n\n\n\nThe orthogonal projection \\mathbf{a} of a point \\mathbf{b} \\in \\mathbb{R}^{n} onto the hyperplane \\mathcal{H} is given by\n\n\\mathbf{a} = \\mathbf{b} - \\left(\n    \\frac{\n        \\mathbf{w}^{T} \\mathbf{b} - \\beta\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\right) \\mathbf{w}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince we know that \\mathcal{H} is an affine space with \\mathcal{W} = \\mathbf{w}^{\\perp} and \\mathbf{v} = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}, the affine projection can be expressed as\n\n\\begin{aligned}\n\\mathbf{a}\n& = \\mathbf{v} + \\mathbf{P}_{\\mathcal{M}} (\\mathbf{b} - \\mathbf{v}),\n\\\\\n& = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w} + \\mathbf{P}_{\\mathbf{w}^{\\perp}} \\left(\n    \\mathbf{b} - \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\right),\n\\end{aligned}\n\nAccording to the property of projection matrix,\n\n\\mathbf{P}_{\\mathbf{w}^{\\perp}} = \\mathbf{I} - \\mathbf{P}_{\\mathbf{w}}.\n\nDenoted the basis of subspace \\mathcal{M} as the columns of matrix \\mathbf{M}, the orthogonal projection matrix onto \\mathcal{M} is\n\n\\begin{aligned}\n\\mathbf{P}_{\\mathcal{M}}\n& = \\mathbf{M} (\\mathbf{M}^{T} \\mathbf{M})^{-1} \\mathbf{M}^{T}\n\\\\\n\\mathbf{P}_{\\mathbf{w}}\n& = \\mathbf{w} (\\mathbf{w}^{T} \\mathbf{w})^{-1} \\mathbf{w}^{T}\n\\\\\n\\mathbf{P}_{\\mathbf{w}}\n& = \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}}.\n\\end{aligned}\n\nThus,\n\n\\mathbf{P}_{\\mathbf{w}^{\\perp}} = \\mathbf{I} - \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}}.\n\nPlugging it back into the top equation\n\n\\begin{aligned}\n\\mathbf{a}\n& = \\frac{\n    \\beta\n}{\n    \\mathbf{w}^{T} \\mathbf{w}\n} \\mathbf{w} + \\mathbf{P}_{\\mathbf{w}^{\\perp}} \\left(\n    \\mathbf{b} - \\frac{\n        \\beta\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    } \\mathbf{w}\n\\right),\n\\\\\n& = \\frac{\n    \\beta\n}{\n    \\mathbf{w}^{T} \\mathbf{w}\n} \\mathbf{w} + \\left(\n    \\mathbf{I} - \\frac{\n        \\mathbf{w} \\mathbf{w}^{T}\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\right) \\left(\n    \\mathbf{b} - \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\right),\n\\\\\n& = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w} + \\left(\n    \\mathbf{b}\n    - \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n    - \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{b}\n    + \\frac{\\mathbf{w}^{T} \\mathbf{w} \\beta}{\\mathbf{w}^{T} \\mathbf{w} \\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\right)\n\\\\\n& = \\mathbf{b}\n- \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{b}\n+ \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/13_Determinants_and_Eigensystems.html",
    "href": "Linear Algebra/13_Determinants_and_Eigensystems.html",
    "title": "13  Determinants and Eigensystems",
    "section": "",
    "text": "14 Determinants\nFor an matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n}, the determinant of A is defined to be the scalar\n\\text{det} (\\mathbf{A}) = \\lvert \\mathbf{A} \\rvert = \\sum_{p \\in \\mathcal{P}} \\sigma (p) \\prod_{i=1}^{n} a_{i, p_{i}}\nwhere\nNote that each term a1p1a2p2 · · · anpn in (6.1.1) contains exactly one entry from each row and each column of A.\nFor a matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n}, (\\lambda, \\mathbf{x}) is an eigenpair for \\mathbf{A} if\n\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}\nwhere\nThe set of all distinct eigenvalues, denoted by \\sigma (A), is the spectrum of A"
  },
  {
    "objectID": "Linear Algebra/13_Determinants_and_Eigensystems.html#eigenspace",
    "href": "Linear Algebra/13_Determinants_and_Eigensystems.html#eigenspace",
    "title": "13  Determinants and Eigensystems",
    "section": "15.1 Eigenspace",
    "text": "15.1 Eigenspace\nGiven a eigenvalue \\lambda,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x}\n& = \\lambda \\mathbf{x}\n\\\\\n(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{x}\n& = 0\n\\\\\n\\mathbf{x}\n& \\in N (\\mathbf{A} - \\lambda \\mathbf{I}),\n\\\\\n\\end{aligned}\n\nwhich shows that\n\n\\{ \\mathbf{x} \\neq 0 \\mid \\mathbf{x} \\in N (\\mathbf{A} - \\lambda \\mathbf{I}) \\}\n\nis the set of all eigenvectors associated with the \\lambda, and N (\\mathbf{A} - \\lambda \\mathbf{I}) is an eigenspace for \\mathbf{A}.\nBecause of \\mathbf{x} \\neq \\mathbf{0} and rank-nullity theorem,\n\nN (\\mathbf{A} - \\lambda \\mathbf{I}) \\neq \\{ \\mathbf{0} \\}\n\\Rightarrow \\text{rank} (\\mathbf{A} - \\lambda \\mathbf{I}) &lt; n\n\nwhich, according to the property of rank, indicates that \\mathbf{A} - \\lambda \\mathbf{I} is a singular matrix, that is,\n\n\\text{det} (\\mathbf{A} - \\lambda \\mathbf{I}) = 0."
  },
  {
    "objectID": "Linear Algebra/13_Determinants_and_Eigensystems.html#characteristic-polynomial-and-equation",
    "href": "Linear Algebra/13_Determinants_and_Eigensystems.html#characteristic-polynomial-and-equation",
    "title": "13  Determinants and Eigensystems",
    "section": "15.2 Characteristic polynomial and equation",
    "text": "15.2 Characteristic polynomial and equation\nThe characteristic polynomial of \\mathbf{A} \\in \\mathbb{C}^{n \\times n} is a function\n\np (\\lambda) = \\text{det} (\\mathbf{A} - \\lambda \\mathbf{I}).\n\n\nThe degree of p (\\lambda) is n,\nthe leading term in p (\\lambda) is (-1)^{n} \\lambda^{n}.\n\nThe characteristic equation for \\mathbf{A} is\n\np (\\lambda) = 0.\n\n\nThe eigenvalues of \\mathbf{A} are the solutions of the characteristic equation.\nThus, \\mathbf{A} has n eigenvalues, but some may be complex numbers (even if the entries of A are real numbers), and some eigenvalues may be repeated."
  },
  {
    "objectID": "Linear Algebra/13_Determinants_and_Eigensystems.html#multiplicities",
    "href": "Linear Algebra/13_Determinants_and_Eigensystems.html#multiplicities",
    "title": "13  Determinants and Eigensystems",
    "section": "15.3 Multiplicities",
    "text": "15.3 Multiplicities\nLet \\lambda_{1}, \\dots, \\lambda_{n} \\in \\sigma (\\mathbf{A}) be the unique eigenvalues for \\mathbf{A}. Then\n\nthe algebraic multiplicity of an eigenvalue \\lambda_{i} is the number of times it appears as the root of p (\\lambda),\n\n\\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}) = a_{i} \\iff \\dots + (\\lambda - \\lambda_{i})^{a_{i}} + \\dots = 0.\n\nThat is, the algebraic multiplicity of \\lambda_{i} is the number of times \\lambda_{i} has repeated in all eigenvalues.\nthe geometric multiplicity of an eigenvalue \\lambda_{i} is the number of the dimension of eigenspace associated with \\lambda_{i},\n\n  \\text{geo mult}_{\\mathbf{A}} (\\lambda_{i}) = \\text{dim} N (\\mathbf{A} - \\lambda_{i} \\mathbf{I}).\n  \nThat is, the geometric multiplicity of \\lambda_{i} is the number of linearly independent eigenvectors associated with \\lambda_{i}.\n\n\n15.3.1 Special multiplicities\n\nWhen \\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}) = 1, \\lambda_{i} is called a simple eigenvalue, since there can only be one unique eigenvector associated with this eigenvalue.\nEigenvalues such that \\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}) = \\text{geo mult}_{\\mathbf{A}} (\\lambda_{i}) are called semi-simple eigenvalues of A, as all eigenvectors associated with the eigenvalues that have the value of \\lambda_{i} are linearly independent.\n\n\n\n15.3.2 Properties of multiplicities\n(multiplicities-property-1)=\n\nFor every \\mathbf{A} \\in \\mathbb{C}^{n \\times n}, and for each \\lambda_{i} \\in \\sigma(\\mathbf{A}),\n\n  \\text{geo mult}_{\\mathbf{A}} (\\lambda_{i}) \\leq \\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/13_Determinants_and_Eigensystems.html#independent-eigenvectors",
    "href": "Linear Algebra/13_Determinants_and_Eigensystems.html#independent-eigenvectors",
    "title": "13  Determinants and Eigensystems",
    "section": "15.4 Independent eigenvectors",
    "text": "15.4 Independent eigenvectors\nLet \\{ \\lambda_{1}, \\dots, \\lambda_{k} \\} be a set of distinct eigenvalues for \\mathbf{A}.\n\nIf \\{ (\\lambda_{1}, \\mathbf{x}_{1}), \\dots, (\\lambda_{k}, \\mathbf{x}_{k}) \\} is a set of eigenpairs for \\mathbf{A}, then \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k} \\} is a linearly independent set.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction.\nSuppose \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k} \\} is linearly dependent, but has been reordered so that the first r eigenvectors \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k} \\} is linearly independent.\nThus, the r + 1th eigenvector is\n\n  \\mathbf{x}_{r + 1} = \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i}.\n  \nMultiply the both sides by the \\mathbf{A} - \\lambda_{r + 1} \\mathbf{I} to get\n\n  (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\mathbf{x}_{r + 1} = (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i}.\n  \nSince (\\lambda_{r + 1}, x_{r + 1}) is an eigenpair,\n\n  \\begin{aligned}\n  (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\mathbf{x}_{r + 1}\n  & = 0\n  \\\\\n  (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i}\n  & = 0\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\mathbf{x}_{i}\n  & = 0\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\mathbf{A} \\mathbf{x}_{i} - \\lambda_{r + 1} \\mathbf{x}_{i})\n  & = 0\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\lambda_{i} \\mathbf{x}_{i} - \\lambda_{r + 1} \\mathbf{x}_{i})\n  & = 0\n  & [\\mathbf{A} \\mathbf{x}_{i} = \\lambda_{i} \\mathbf{x}_{i}]\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\lambda_{i}  - \\lambda_{r + 1}) \\mathbf{x}_{i}\n  & = 0\n  \\\\\n  \\end{aligned}\n  \nSince \\{ \\mathbf{1}, \\dots, \\mathbf{x}_{r} \\} are linearly independent,\n\n  \\alpha_{i} (\\lambda_{i}  - \\lambda_{r + 1}) = 0, \\forall i = 1, \\dots, r\n  \nbut since we assume \\lambda_{i}, \\dots, \\lambda_{r + 1}, \\dots, \\lambda_{n} are different,\n\n  \\lambda_{i} \\neq \\lambda_{r + 1} \\Rightarrow \\alpha_{i} = 0, \\forall i = 1, \\dots, r,\n  \nwhich means\n\n  \\mathbf{x}_{r + 1} = \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i} = 0.\n  \nHowever, eigenvectors are all non-zero vectors and thus an contradiction occurs.\n\n\n\nIf \\mathcal{B}_{i} is a basis for N (\\mathbf{A} - \\lambda_{i} \\mathbf{I}), then \\mathcal{B} = \\mathcal{B}_{1} \\cup \\dots, \\cup \\mathcal{B}_{k} is a linearly independent set.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/14_Similarity_and_Diagonalization.html",
    "href": "Linear Algebra/14_Similarity_and_Diagonalization.html",
    "title": "14  Similarity and Diagonalization",
    "section": "",
    "text": "15 Similarity\nTwo matrices \\mathbf{A}, \\mathbf{B} \\in \\mathbb{C}^{n \\times n} are similar if there exists a non-singular matrix \\mathbf{P} such that\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} = \\mathbf{B},\n\\mathbf{A} = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}.\nSimilar matrices have the same eigenvalues.\nA square matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n} is diagonalizable if \\mathbf{A} is similar to a diagonal matrix:\n\\mathbf{A} = \\mathbf{P} \\Lambda \\mathbf{P}^{-1}\nwhere \\Lambda is a diagonal matrix."
  },
  {
    "objectID": "Linear Algebra/14_Similarity_and_Diagonalization.html#unitarily-similar",
    "href": "Linear Algebra/14_Similarity_and_Diagonalization.html#unitarily-similar",
    "title": "14  Similarity and Diagonalization",
    "section": "15.1 Unitarily similar",
    "text": "15.1 Unitarily similar\nTwo matrices \\mathbf{A}, \\mathbf{B} \\in \\mathbb{C}^{n \\times n} are unitarily similar if there exists an unitary matrix \\mathbf{U} such that\n\n\\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{B},\n\nwhich, according to the property of unitary matrix, can also be written as\n\n\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U} = \\mathbf{B}."
  },
  {
    "objectID": "Linear Algebra/14_Similarity_and_Diagonalization.html#diagonalization-and-eigensystems",
    "href": "Linear Algebra/14_Similarity_and_Diagonalization.html#diagonalization-and-eigensystems",
    "title": "14  Similarity and Diagonalization",
    "section": "16.1 Diagonalization and eigensystems",
    "text": "16.1 Diagonalization and eigensystems\n\\mathbf{A} is diagonalizable if and only if \\mathbf{A} has n linearly independent eigenvectors. That is,\n\n\\mathbf{A} = \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{-1}\n\nor\n\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} = \\mathbf{\\Lambda},\n\nwhere\n\nthe columns of \\mathbf{P} \\in \\mathbb{C}^{n \\times n} are n linearly independent eigenvectors,\nthe diagonal values of \\mathbf{\\Lambda} are corresponding eigenvalues.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first prove that If \\mathbf{A} is diagonalizable, then \\mathbf{A} has n linearly independent eigenvectors.\nFirst note that\n\n\\begin{aligned}\n\\mathbf{A}\n& = \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{-1}\n\\\\\n\\mathbf{A} \\mathbf{P}\n& = \\mathbf{P} \\mathbf{\\Lambda}\n\\\\\n\\end{aligned}\n\nSince \\mathbf{\\Lambda} is a diagonal matrix with \\lambda_{1}, \\dots, \\lambda_{n} on its diagonal,\n\n\\mathbf{A} \\mathbf{P}_{*, i} = \\mathbf{P}_{*, i} \\lambda_{i} = \\lambda_{i} \\mathbf{P}_{*, i}.\n\nThus, (\\lambda, \\mathbf{P}_{*, i}) is an eigenpair of \\mathbf{A}, and \\mathbf{A} has n such eigenpairs.\nSince \\mathbf{P} is non-singular (full-rank), \\mathbf{P} has independent columns. Thus, \\mathbf{A} has n independent eigenvectors.\nThen we prove that if \\mathbf{A} has n linearly independent eigenvectors, then \\mathbf{A} is diagonalizable.\nAssume the columns of \\mathbf{P} \\in \\mathbf{C}^{n \\times n} are the eigenvectors of \\mathbf{A}, and \\lambda_{1}, \\dots, \\lambda_{n} are corresponding eigenvalues,\n\n\\mathbf{A} \\mathbf{P}_{*, i} = \\lambda_{i} \\mathbf{P}_{*, i} \\quad \\forall i = 1, \\dots, n.\n\nBy rewriting \\lambda_{1}, \\dots, \\lambda_{n} as diagonals for the diagonal matrix \\mathbf{\\Lambda},\n\n\\mathbf{A} \\mathbf{P} = \\mathbf{P} \\mathbf{\\Lambda}.\n\nSince the columns of the \\mathbf{P} are linearly independent, \\mathbf{P} has full rank and thus there exists the inverse of \\mathbf{P}\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{P}\n& = \\mathbf{P} \\mathbf{\\Lambda}\n\\\\\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P}\n& = \\mathbf{\\Lambda}\n\\end{aligned}\n\nwhich shows that \\mathbf{A} is similar to \\mathbf{\\Lambda} and thus is diagonalizable."
  },
  {
    "objectID": "Linear Algebra/14_Similarity_and_Diagonalization.html#diagonalizability-and-multiplicities",
    "href": "Linear Algebra/14_Similarity_and_Diagonalization.html#diagonalizability-and-multiplicities",
    "title": "14  Similarity and Diagonalization",
    "section": "16.2 Diagonalizability and multiplicities",
    "text": "16.2 Diagonalizability and multiplicities\nTODO"
  },
  {
    "objectID": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html",
    "href": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html",
    "title": "15  Normal and Positive Definite Matrices",
    "section": "",
    "text": "16 Normal Matrices\nThe matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n} is a normal matrix if and only if\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H}.\nGiven a Hermitian matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n}, it is positive definite if and only if\n\\mathbf{x}^{H} \\mathbf{A} \\mathbf{x} &gt; 0,\nfor all nonzero vectors \\mathbf{x} \\in \\mathbb{C}^{n}, and it is positive semi-definite if and only if\n\\mathbf{x}^{H} \\mathbf{A} \\mathbf{x} \\geq 0,\nfor all vectors \\mathbf{x}."
  },
  {
    "objectID": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#properties-of-normal-matrices",
    "href": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#properties-of-normal-matrices",
    "title": "15  Normal and Positive Definite Matrices",
    "section": "16.1 Properties of normal matrices",
    "text": "16.1 Properties of normal matrices\n(normal-matrices-property-1)=\n\nUnitary matrices are normal.\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the property of unitary matrices, a matrix \\mathbf{U} is unitary if and only if\n\n  \\mathbf{U}^{-1} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{-1} = \\mathbf{I}.\n  \nThus, unitary matrices are normal.\n\n\n\n\n(normal-matrices-property-2)=\n\nDiagonal matrices are normal.\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\mathbf{D} \\in \\mathbb{C}^{n \\times n} as a diagonal matrix with d_{1}, \\dots, d_{n} in its diagonal.\n\n  \\mathbf{D}^{H} \\mathbf{D} = \\sum_{i=1}^{n} d_{i}^{2} = \\mathbf{D} \\mathbf{D}^{H}.\n  \n\n\n\n\n(normal-matrices-property-3)=\n\nUnitary similarity preserves normality. That is, if \\mathbf{A} is a normal matrix and is unitarily similar to \\mathbf{B}\n\n  \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{B}\n  \nor\n\n  \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U} = \\mathbf{B},\n  \nthen \\mathbf{B} is also a normal matrix.\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe goal is to prove\n\n  \\mathbf{B}^{H} \\mathbf{B} = \\mathbf{B} \\mathbf{B}^{H}.\n  \nFirst we expand \\mathbf{B}^{H} \\mathbf{B} to have\n\n  \\begin{aligned}\n  \\mathbf{B}^{H} \\mathbf{B}\n  & = (\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U})^{H} (\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U})\n  \\\\\n  & = \\mathbf{U} \\mathbf{A}^{H} \\mathbf{U}^{H} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{U} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  & [TODO]\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{I} \\mathbf{A} \\mathbf{U}\n  & [\\mathbf{U} \\mathbf{U}^{H} = \\mathbf{U} \\mathbf{U}^{-1} = \\mathbf{I}]\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{A} \\mathbf{U}.\n  \\end{aligned}\n  \nSince \\mathbf{A} is a normal matrix,\n\n  \\mathbf{A} \\mathbf{A}^{H} = \\mathbf{A}^{H} \\mathbf{A}.\n  \nContinue from the derivation above,\n\n  \\begin{aligned}\n  \\mathbf{B}^{H} \\mathbf{B}\n  & = \\mathbf{U}^{H} \\mathbf{A} \\mathbf{A}^{H} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{I} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{U} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U} \\mathbf{A}^{H} \\mathbf{U}^{H} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  & [TODO]\n  \\\\\n  & = (\\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{U})^{H} (\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U})\n  \\\\\n  & = \\mathbf{B} \\mathbf{B}^{H}.\n  \\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#unitary-diagonalization",
    "href": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#unitary-diagonalization",
    "title": "15  Normal and Positive Definite Matrices",
    "section": "16.2 Unitary diagonalization",
    "text": "16.2 Unitary diagonalization\nA matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n} is normal if and only if \\mathbf{A} is unitarily similar to a diagonal matrix\n\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H} \\iff \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{\\Lambda}.\n\nwhere \\mathbf{U} is a unitary matrix and \\mathbf{\\Lambda} is a diagonal matrix.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first prove that\n\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H} \\Rightarrow \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{\\Lambda}.\n\nTODO\nThen we prove that\n\n\\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{\\Lambda} \\Rightarrow \\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H}.\n\nAccording to the property of unitary matrices, unitary similarity preserves normality.\nAlso according to property of unitary matrices, all diagonal matrices are normal.\nThus, \\mathbf{A} is normal because \\mathbf{A} is unitarily similar to a diagonal matrix, which is always normal.\n\n\n\nSince the columns of \\mathbf{U} are eigenvectors of \\mathbf{A} and are orthonormal to each other the columns of \\mathbf{U} must be a complete orthonormal set of eigenvectors for \\mathbf{A}, and the diagonal entries of \\mathbf{\\Lambda} are the associated eigenvalues."
  },
  {
    "objectID": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#hermitian-symmetric-matrices",
    "href": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#hermitian-symmetric-matrices",
    "title": "15  Normal and Positive Definite Matrices",
    "section": "16.3 Hermitian (symmetric) matrices",
    "text": "16.3 Hermitian (symmetric) matrices\nA square complex (real) matrix \\mathbf{A} is hermitian (symmetric) if and only if\n\n\\mathbf{A}^{H} = \\mathbf{A},\n\nwhich implies a hermitian (symmetric) matrix is a normal matrix.\nAll eigenvalues of Hermitian matrices are real.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose (\\lambda, \\mathbf{v}) is a eigenpair for the Hermitian matrix \\mathbf{A}.\n\n\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}.\n\nMultiplying both sides by \\mathbf{v}^{H} on the left to get,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{v}\n& = \\lambda \\mathbf{v}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n& = \\lambda \\mathbf{v}^{H} \\mathbf{v}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n& = \\lambda \\lVert \\mathbf{v} \\rVert^{2}.\n\\\\\n\\end{aligned}\n\nAlternatively we can take the transpose conjugate of both sides, and then multiply both sides by \\mathbf{v} on the right to get,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{v}\n& = \\lambda \\mathbf{v}\n\\\\\n(\\mathbf{A} \\mathbf{v})^{H}\n& = (\\lambda \\mathbf{v})^{H}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A}^{H}\n& = \\lambda^{*} \\mathbf{v}^{H}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A}^{H} \\mathbf{v}\n& = \\lambda^{*} \\mathbf{v}^{H} \\mathbf{v}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A}^{H} \\mathbf{v}\n& = \\lambda^{*} \\lVert \\mathbf{v} \\rVert^{2}\n\\\\\n\\end{aligned}\n\nSince \\mathbf{A} is a hermitian matrix\n\n\\begin{aligned}\n\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n& = \\mathbf{v}^{H} \\mathbf{A}^{H} \\mathbf{v}\n\\\\\n\\lambda \\lVert \\mathbf{v} \\rVert^{2}\n& = \\lambda^{*} \\lVert \\mathbf{v} \\rVert^{2}\n\\\\\n\\lambda\n& = \\lambda^{*},\n\\end{aligned}\n\nwhich means \\lambda is a real number."
  },
  {
    "objectID": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#rayleigh-quotient",
    "href": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#rayleigh-quotient",
    "title": "15  Normal and Positive Definite Matrices",
    "section": "16.4 Rayleigh quotient",
    "text": "16.4 Rayleigh quotient\nGiven a Hermitian matrix \\mathbf{M} \\in \\mathbb{C}^{n \\times n}, the Rayleigh quotient is a function R_{\\mathbf{M}} (\\mathbf{x}): \\mathbb{C}^{n} \\setminus \\{ 0 \\} \\rightarrow \\mathbb{R}\n\nR_{\\mathbf{M}} (\\mathbf{x}) = \\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n\nthat takes a nonzero vector \\mathbf{x} and returns a real number.\nSince the Hermitian matrix \\mathbf{M} has all real eigenvalues, they can be ordered. Suppose \\lambda_{1}, \\dots, \\lambda_{n} is the eigenvalues in descending orders.\nThen, given a Hermitian matrix, its Rayleigh quotient is upper bounded and lower bounded by maximum and minimum eigenvalues of \\mathbf{M} respectively,\n\n\\lambda_{1} \\geq R_{\\mathbf{M}} (\\mathbf{x}) \\geq \\lambda_{n}.\n\nThat is,\n\n\\lambda_{1} = \\max_{\\mathbf{x} \\neq 0} \\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n},\n\n\n\\lambda_{n} = \\min_{\\mathbf{x} \\neq 0} \\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathbf{M} is a Hermitian matrix, we can expand it using unitary diagonalization:\n\n\\begin{aligned}\n\\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n& = \\frac{\n    \\mathbf{x}^{H} \\mathbf{U}^{H} \\mathbf{\\Lambda} \\mathbf{U} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n\\\\\n& = \\frac{\n    \\mathbf{y}^{H} \\mathbf{\\Lambda} \\mathbf{y}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n&\n[\\mathbf{y} = \\mathbf{U} \\mathbf{x}].\n\\end{aligned}\n\nSince \\mathbf{U} is a unitary matrix, according to the property of the unitary matrix,\n\n\\mathbf{y}^{H} \\mathbf{y} = \\mathbf{x}^{H} \\mathbf{x}.\n\nThus,\n\n\\begin{aligned}\n\\frac{\n    \\mathbf{y}^{H} \\mathbf{\\Lambda} \\mathbf{y}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n& =\n\\frac{\n    \\mathbf{y}^{H} \\mathbf{\\Lambda} \\mathbf{y}\n}{\n    \\mathbf{y}^{H} \\mathbf{y}\n}\n\\\\\n& =\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{i} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}.\n\\end{aligned}\n\nSince \\lambda_{1} \\geq \\lambda_{i} \\geq \\lambda_{n}, \\forall i = 1, \\dots, n,\n\n\\lambda_{1}\n=\n\\lambda_{1}\n\\frac{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n=\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{1} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n\\geq\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{i} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n},\n\n\n\\lambda_{n}\n=\n\\lambda_{n}\n\\frac{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n=\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{n} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n\\leq\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{i} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}."
  },
  {
    "objectID": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#properties-of-definite-matrices",
    "href": "Linear Algebra/15_Normal_and_Positive_Definite_Matrices.html#properties-of-definite-matrices",
    "title": "15  Normal and Positive Definite Matrices",
    "section": "17.1 Properties of definite matrices",
    "text": "17.1 Properties of definite matrices\n(definite-matrices-property-1)=\n\nPositive definite matrix always has full rank.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction.\nSuppose a positive definite matrix \\mathbf{A} does NOT have full rank, which means that there exists at least one non-zero vector \\mathbf{x} \\in N (\\mathbf{A}) such that\n\n  \\mathbf{A} \\mathbf{x} = 0.\n  \nThen, multiplying both sides by \\mathbf{x}^{H},\n\n  \\mathbf{x}^{H} \\mathbf{A} \\mathbf{x} = 0.\n  \nwhich contradicts to the fact that \\mathbf{A} is positive definite.\nThus, positive definite matrix always has full rank.\n\n\n\n\n(definite-matrices-property-2)=\n\nA matrix \\mathbf{A} is positive definite (semi-definite) if and only if its eigenvalues are positive (non-negative).\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first prove that a matrix \\mathbf{A} is positive definite (semi-definite) if its all eigenvalues are positive (non-negative).\nSince \\mathbf{A} is a Hermitian matrix, it can be unitarily diagonalized:\n\n  \\mathbf{A} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{H},\n  \nwhere the columns of \\mathbf{U} contains the orthonormal eigenvectors of \\mathbf{A} and diagonal of \\mathbf{\\Lambda} has the corresponding real eigenvalues.\nSince we are told all eigenvalues are positive or non-negative(TODO),\n\n  \\mathbf{\\Lambda} = \\mathbf{\\Lambda}^{\\frac{1}{2}} \\mathbf{\\Lambda}^{\\frac{1}{2}}\n  = \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{\\Lambda}^{\\frac{1}{2}})^{H}\n  \nThus,\n\n  \\begin{aligned}\n  \\mathbf{A}\n  & = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{H}\n  \\\\\n  & = \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{\\Lambda}^{\\frac{1}{2}})^{H} \\mathbf{U}^{H}\n  \\\\\n  & = \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}})^{H}\n  \\\\\n  \\end{aligned}\n  \nMultiplying \\mathbf{A} with \\mathbf{x} to get\n\n  \\begin{aligned}\n  \\mathbf{x}^{H} \\mathbf{A} \\mathbf{x}\n  & = \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{\\Lambda}^{\\frac{1}{2}} \\mathbf{U})^{H} \\mathbf{x}\n  \\\\\n  & = \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}})^{H}\n  \\\\\n  & = \\lVert \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\rVert^{2}\n  & [\\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\in \\mathbb{C}^{n}].\n  \\\\\n  \\end{aligned}\n  \nSince \\lVert \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\rVert^{2} is non-negative for any vector \\mathbf{x}, the matrix \\mathbf{A} is positive semi-definite.\nIf all eigenvalues are positive, there is no zero in \\mathbf{\\Lambda}. Since \\mathbf{\\Lambda}^{\\frac{1}{2}} is a diagonal matrix, the matrix \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} has scaled columns of \\mathbf{U}. Since \\mathbf{U} is a unitary matrix, its columns are all linearly independent and thus \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} also have linearly independent columns. Thus, \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} has full rank and\n\n  N (\\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}}) = \\{ 0 \\}.\n  \nThus, \\lVert \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\rVert^{2} &gt; 0 for any non-zero vector \\mathbf{x}. Therefore, the matrix \\mathbf{A} is positive definite.\nThen we prove that all eigenvalues of positive definite (semi-definite) matrices are positive (non-negative).\nConsider any eigenpair (\\lambda, \\mathbf{v}) of \\mathbf{A}.\nSince \\mathbf{v} is non-zero by the definition of eigenvector, we have\n\n  \\begin{aligned}\n  \\mathbf{A} \\mathbf{v}\n  & = \\lambda \\mathbf{v}\n  \\\\\n  \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n  & = \\lambda \\mathbf{v}^{H} \\mathbf{v}\n  \\\\\n  \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n  & = \\lambda \\lVert \\mathbf{v} \\rVert^{2}\n  \\\\\n  \\lambda\n  & = \\frac{\n      \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n  }{\n      \\lVert \\mathbf{v} \\rVert^{2}\n  }\n  \\end{aligned}\n  \nSince \\mathbf{A} is positive definite (semi-definite),\n\n  \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v} &gt; 0 \\quad (\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v} \\geq 0),\n  \nwhich means\n\n  \\lambda &gt; 0 \\quad (\\lambda \\geq 0).\n  \n\n\n\nTODO \\mathbf{A} = \\mathbf{B}^{H} \\mathbf{B}"
  },
  {
    "objectID": "Learning Theory/1_Statistical_Learning.html#functions",
    "href": "Learning Theory/1_Statistical_Learning.html#functions",
    "title": "16  Statistical Learning",
    "section": "16.1 Functions",
    "text": "16.1 Functions\n\n16.1.1 Decision function\nA decision function is a function f: \\mathcal{X} \\to \\mathcal{Y} whose domain is \\mathcal{X} and the range is \\mathcal{Y}\n\n\\hat{y} = f (x)\n\nthat maps each input instance x \\in \\mathcal{X} to a label y \\in \\mathcal{Y}.\nHere we have two types of decision functions that have slightly different meanings in the context of machine learning\n\nConcept c and concept class C: a concept from a concept class c \\in \\mathcal{C} is the decision function that the algorithm wants to learn, which assigns all correct labels for given instances.\nHypothesis h and hypothesis class H: a hypothesis from a hypothesis class h \\in \\mathcal{H} is the decision function that the algorithm actually learns from the hypothesis class.\n\n\n\n16.1.2 Loss function\nThe way we evaluate a function f on a labeled instance (x, y) is determined by a loss function L: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}^{+}\n\nL (z) = L (f (x), y)\n\nwhich calculates some notion of discrepancy between the true label y and the predicated label \\hat{y} = f (x).\nAll the loss functions used in this note are 0-1 loss\n\nL (z) = L (f (x), y) = \\mathbb{1} \\left[\n    f (x) \\neq y\n\\right],\n\nwhich incurs a loss of 1 if the predicated label is the same as the true label and 0 if they are the same."
  },
  {
    "objectID": "Learning Theory/1_Statistical_Learning.html#learning-in-a-probability-setting",
    "href": "Learning Theory/1_Statistical_Learning.html#learning-in-a-probability-setting",
    "title": "16  Statistical Learning",
    "section": "16.2 Learning in a probability setting",
    "text": "16.2 Learning in a probability setting\nIn a statistical learning problem, each labeled instance is an independent and identically distributed (i.i.d.) draw from some fixed but unknown joint distribution \\mathbb{P}_{X, Y} over \\mathcal{X} \\times \\mathcal{Y} that describes the probability that both x and y happens in the real world.\nThis means that there is always a probability associated with each term:\n\nthe distribution \\mathbb{P}_{X} for a multivariate random variable X that describes the probability of an instance x\nthe distribution \\mathbb{P}_{Y} for a random variable Y that describes the probability of a label y.\n\nWe can decompose the joint probability according to the chain rule:\n\n\\mathbb{P}_{X, Y}(x, y) = \\mathbb{P}_{X \\mid Y}(x \\mid y) \\mathbb{P}_{Y}(y),\n\nwhere \\mathbb{P}_{X \\mid Y}(x \\mid y) is called class conditional probability, which gives the probability of the instance if we know the label is y.\nFor simplicity, sometimes we will write \\mathbb{P}_{Z} \\coloneqq \\mathbb{P}_{X, Y} to denote the probability of the labeled instance.\n\n16.2.1 True risk\nThe true risk of the hypothesis h is defined as the expectation of the loss function over the joint probability\n\nR (h) =  \\mathbb{E}_{X, Y} [L (h (X), Y)] = \\mathbb{E}_{Z} [L (Z)]\n\nwhich is the probability that h makes a mistake if the loss function is 0-1 loss\n\nR (h) = \\mathbb{P}_{X, Y} \\left[\n    \\mathbb{1} \\left[\n        h (x) \\neq y\n    \\right]\n\\right].\n\n\nLemma 16.1 Apart from the expectation with respect to the join probability of \\mathbb{P}_{X, Y}, the true risk of any hypothesis h can also be written as the expectation of the conditional expectation of the loss function\n\nR(h) = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof is based on the definition of the expectation and the chain rules of the probability\n\n\\begin{aligned}\nR(h)\n& = \\mathbb{E}_{X, Y} [L (h (X), Y)]\n\\\\\n& = \\int \\int \\mathbb{P}_{X, Y} (x, y) L (h (x), y) \\mathop{d x} \\mathop{dy}\n& [\\text{definition of } \\mathbb{E} [\\cdot]]\n\\\\\n& = \\int \\int \\mathbb{P}_{Y \\mid X} (y \\mid x) \\mathbb{P}_{X} (x) L (g(x), y) \\mathop{d x} \\mathop{dy}\n& [\\text{probability chain rule}]\n\\\\\n& = \\int \\mathbb{P}_{X} (x) \\int \\mathbb{P}_{Y \\mid X} (y \\mid x) L (g(x), y) \\mathop{dy} \\mathop{d x}\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\\end{aligned}\n\n\n\n\n\n\n16.2.2 Empirical risk\nThe empirical risk function is used with the past data of n labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} as a surrogate function for the risk function\n\nR_{\\mathcal{S}} (h) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (h (x_{i}), y) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (z_{i}),\n\nwhich is the average number of mistakes h made in \\mathcal{D}^{n} if the loss is 0-1 loss\n\nR_{\\mathcal{S}} (h) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{1} \\left[\n    h (x_{i}) \\neq y_{i}\n\\right].\n\nThe idea is that if the past data we have is representative of the actual distribution, then it will be the case that the empirical risk will be close to the true risk.\n\n\n16.2.3 Empirical risk as a unbiased estimator\nThe empirical risk is an unbiased estimator of the true risk. That is, the expectation of the empirical risk over all samples is the true risk\n\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    R_{\\mathcal{S}} (h)\n\\right] = R (h).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    R_{\\mathcal{S}} (h)\n\\right]\n& = \\mathbb{E}_{Z} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (z_{i})\n\\right]\n\\\\\n& = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{E}_{Z} [L (z_{i})]\n\\\\\n& = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} R (h)\n\\\\\n& = R (h).\n\\end{aligned}\n\n\n\n\nAlso, by the law of large numbers, we have R_{n} (h) \\to R(h) as n \\to \\infty, almost surely, which means the empirical risk is closed to the true risk if the sample size is large enough."
  },
  {
    "objectID": "Learning Theory/1_Statistical_Learning.html#some-probability-facts",
    "href": "Learning Theory/1_Statistical_Learning.html#some-probability-facts",
    "title": "16  Statistical Learning",
    "section": "16.3 Some probability facts",
    "text": "16.3 Some probability facts\n\n16.3.1 Basics\n\nComplement rule\n\n\n\\mathbb{P} (A &gt; t) &lt; \\delta \\implies \\mathbb{P} (A \\leq t) \\geq 1 - \\delta"
  },
  {
    "objectID": "Learning Theory/2_Bayesian_Classifier.html#map-rule",
    "href": "Learning Theory/2_Bayesian_Classifier.html#map-rule",
    "title": "17  Bayesian Classifier",
    "section": "17.1 MAP rule",
    "text": "17.1 MAP rule\nSince the true risk R (h) does not depend on X and Lemma 16.1 shows that R(h) = \\mathbb{E}_{X} \\left[ \\mathbb{E}_{Y \\mid X} \\left[ L (h (X), Y) \\right] \\right], the Bayes classifier can also be written as the hypothesis that minimizes the conditional expectation\n\nh^{*} = \\argmin_{h} \\mathbb{E}_{Y \\mid X} \\left[\n    L (h (X), Y)\n\\right],\n\nwhich can be further simplied to maximum a-posteriori probability (MAP) rule if the loss function is 0-1 loss and there are m labels y \\in [1, m]\n\nh^{*} (x) = \\argmax_{y \\in [1, m]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of Bayes classifier,\n\n\\begin{aligned}\nh^{*}\n& = \\argmin_{h} \\mathbb{E}_{Y \\mid X} \\left[\n    L (h (X), Y)\n\\right]\n\\\\\n& = \\argmin_{h} \\sum_{y=1}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x) L (h, y)\n& [\\text{def of } \\mathbb{E}_{Y \\mid X}]\n\\\\\n& = \\argmin_{h} \\sum_{y = h (x)}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x) \\times 0\n+\n\\sum_{y \\neq h (x)}^{m} \\mathbb{P}_{Y \\mid X}(y \\mid x) \\times 1\n& [\\text{def of 0-1 loss}]\n\\\\\n& = \\argmin_{h} \\sum_{y \\neq h (x)}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\argmin_{h} \\left[\n    1 - \\mathbb{P}_{Y \\mid X} (h (x) \\mid x)\n\\right]\n& [\\sum_{x \\neq \\alpha} \\mathbb{P}_{X} (x) = 1 - \\mathbb{P}_{X} (\\alpha) ]\n\\\\\n& = \\argmax_{h} \\mathbb{P}_{Y \\mid X} (h (x) \\mid x)\n& [\\argmin_{x} (1 - f(x)) = \\arg\\max_{x} (f(x))].\n\\end{aligned}\n\nwhere the last line can be simplied to\n\nh^{*} (x) = \\argmax_{y \\in [1, m]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\nsince h (x) \\in [1, m].\n\n\n\nAccording to Bayes Theorem,\n\n\\begin{aligned}\n\\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\frac{\\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)}{\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x})}\n\\\\\n& = \\arg\\max_{y} \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y) & [\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}) \\text{ doesn't depend on } y],\n\\\\\n\\end{aligned}\n\nMAP rule can thus be computed using the class conditional probability (likelihood) and the class probability (prior), which is more practical since the class conditional probability and class probability can be more easily obtained from the data than the posterior probability.\nUsing the log trick, the BDR for 0-1 loss is often calculated using:\n\n\\begin{aligned}\n\\arg\\max_{y} \\ln \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)\n\\\\\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) + \\ln \\mathbb{P}_{Y}(y).\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Learning Theory/2_Bayesian_Classifier.html#map-rule-for-binary-classification",
    "href": "Learning Theory/2_Bayesian_Classifier.html#map-rule-for-binary-classification",
    "title": "17  Bayesian Classifier",
    "section": "17.2 MAP rule for binary classification",
    "text": "17.2 MAP rule for binary classification\nSince there are only 2 labels in the binary classification problem, the MAP rule for binary classification is simplied to\n\n\\begin{aligned}\nh^{*} (x)\n& = \\argmax_{y \\in [0, 1]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\mathbb{1} \\left[\n    \\mathbb{P}_{Y \\mid X} (1 \\mid x) &gt; \\mathbb{P}_{Y \\mid X} (0 \\mid x)\n\\right]\n\\\\\n& = \\begin{cases}\n    1, \\quad \\mathbb{P}_{Y \\mid X} (1 \\mid x) &gt; \\frac{ 1 }{ 2 }  \\\\\n    0, \\quad \\mathbb{P}_{Y \\mid X} (1 \\mid x) &lt; \\frac{ 1 }{ 2 }.\n\\end{cases}\n\\end{aligned}\n\n\n17.2.1 Regression function\nThe conditional distribution \\mathbb{P}_{Y \\mid X} can be modeled with a Bernoulli distribution \\mathbb{P}_{Y \\mid X} (y \\mid x) = \\mathrm{Ber} (\\eta (x)), where \\eta (x) is the regression function\n\n\\eta (x) = \\mathbb{P}_{Y \\mid X} (1 \\mid x) = \\mathbb{E}_{Y \\mid X} (Y).\n\n\nLemma 17.1 For any hypothesis h, we can write its risk function with 0-1 loss for binary classification as\n\nR (h) = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h (X))\n    +\n    (1 - \\eta (X)) h (X)\n\\right].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of the risk function\n\nR (h) = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\nSince the 0-1 loss for binary classification problem can be written as\n\nL (h (x), y) = \\mathbb{1} \\left[\n    h (x) \\neq y\n\\right] = y (1 - h (x)) + (1 - y) h (x)\n\nwe have\n\n\\begin{aligned}\nR (h)\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        y (1 - h (x)) + (1 - y) h (x)\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} [y](1 - h (x))\n    +\n    \\mathbb{E}_{Y \\mid X} [1 - y] h (x)\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h (X))\n    +\n    (1 - \\eta (X)) h (X)\n\\right].\n\\end{aligned}\n\n\n\n\n\nTheorem 17.1 The risk of the Bayes classifier for binary classification with 0-1 loss is the expectation of the minimum of \\eta (X) and 1 - \\eta (X)\n\nR (h^{*}) = \\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right]\n\nand is less than \\frac{ 1 }{ 2 }\n\n\\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right] \\leq \\frac{ 1 }{ 2 }.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Lemma 17.1 and replacing h with the Bayes classifier h^{*}, we have\n\n\\begin{aligned}\nR (h^{*})\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h^{*} (X))\n    +\n    (1 - \\eta (X)) h^{*} (X)\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) \\mathbb{1} \\left[\n        \\eta (X) &lt; \\frac{ 1 }{ 2 }\n    \\right]\n    +\n    (1 - \\eta (X)) \\mathbb{1} \\left[\n        \\eta (X) &gt; \\frac{ 1 }{ 2 }\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right]\n\\end{aligned}\n\nwhere the last inequality follows because\n\nR (h^{*}) = \\mathbb{E}_{X} [\\eta (X)] = \\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]], \\quad \\text{ if } \\eta (X) &lt; \\frac{ 1 }{ 2 } \\\\\nR (h^{*}) = \\mathbb{E}_{X} [1 - \\eta (X)] = \\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]], \\quad \\text{ if } \\eta (X) &gt; \\frac{ 1 }{ 2 }.\n\nSince \\min [\\eta (X), 1 - \\eta (X)] &lt; \\frac{ 1 }{ 2 }, its expectation is also less than \\frac{ 1 }{ 2 }\n\n\\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]] &lt; \\frac{ 1 }{ 2 }.\n\n\n\n\n\n\n17.2.2 Excess risk\nFor any hypothesis h, we are interested in the difference between its risk R (h) and Bayes risk R (h^{*}), which is called excess risk of h\n\n\\mathcal{E} (h) = R (h) - R (h^{*}).\n\n\nTheorem 17.2 For any hypothesis h, the excess risk satisfies\n\n\\mathcal{E} (h) = \\mathbb{E}_{X} \\left[\n    \\lvert 2 \\eta (X) - 1 \\rvert \\times \\mathbb{1} \\left[\n        h (X) \\neq h^{*} (X)\n    \\right]\n\\right]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Lemma 17.1 for R (h) and R (h^{*}) and linearity of expectation, we have\n\n\\begin{aligned}\nR (h) - R(h^{*})\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (h^{*} (X) - h (X))\n    +\n    (1 - \\eta (X)) (h (X) - h^{*} (X))\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    (2 \\eta (X) - 1) (h^{*} (X) - h (X))\n\\right].\n\\end{aligned}\n\nNote that\n\nh^{*} (X) - h (X) =  \\mathrm{sgn} [2 \\eta (X) - 1] \\times \\mathbb{1} [h^{*} \\neq h (X)],\n\nbecause it combines all 3 cases for the results of h^{*} (X) - h (X).\n\nIf h^{*} (X) = h (X),\n\nh^{*} (X) - h (X) = 0.\n\nSince \\eta (X) &gt; \\frac{ 1 }{ 2 } \\implies h^{*} (X) = 1, if h^{*} (X) = 1, h (X) = 0,\n\nh^{*} (X) - h (X) = 1 = \\mathrm{sgn} [2 \\eta (X) - 1].\n\nSince \\eta (X) &lt; \\frac{ 1 }{ 2 } \\implies h^{*} (X) = 0, if h^{*} (X) = 0, h (X) = 1,\n\nh^{*} (X) - h (X) = -1 = \\mathrm{sgn} [2 \\eta (X) - 1].\n\n\nTherefore,\n\n\\begin{aligned}\nR (h) - R(h^{*})\n& = \\mathbb{E}_{X} \\left[\n    (2 \\eta (X) - 1) \\mathrm{sgn} [2 \\eta (X) - 1] \\times \\mathbb{1} [h^{*} \\neq h (X)],\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\lvert 2 \\eta (X) - 1 \\rvert \\times \\mathbb{1} [h^{*} \\neq h (X)]\n\\right].\n\\end{aligned}\n\nwhere the last equality holds since x \\times \\mathrm{sgn} [x] = \\lvert x \\rvert."
  },
  {
    "objectID": "Learning Theory/3_Effective_Class_Size.html#growth-function-shattering-coefficient",
    "href": "Learning Theory/3_Effective_Class_Size.html#growth-function-shattering-coefficient",
    "title": "18  Effective Class Size",
    "section": "18.1 Growth function (shattering coefficient)",
    "text": "18.1 Growth function (shattering coefficient)\nGiven a set of instances \\mathcal{S} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}, the projection of a hypothesis class \\mathcal{H} onto \\mathcal{S} is the set of all distinct labels that \\mathcal{H} can produce onto \\mathcal{S}\n\n\\mathcal{H} (\\mathcal{S}) = \\{ \\{ h (x_{1}), \\dots, h (x_{n}) \\}, \\forall h \\in \\mathcal{H} \\}.\n\nThe concept of the growth function (shattering coefficient) is to measure the richness of the decisions that a hypothesis class can make with respect to a dataset of size n.\n\nDefinition 18.1 The growth function of a hypothesis class \\mathcal{H} is defined as the maximum number of unique ways that the hypotheses in \\mathcal{H} can label any set of n instances\n\n\\Pi_{\\mathcal{H}} (n) = \\sup_{\\mathcal{S}: \\lvert \\mathcal{S} \\rvert = n} \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert.\n\n\nNote that \\Pi_{\\mathcal{H}} (n) \\leq 2^{n} for any \\mathcal{H} with binary labels."
  },
  {
    "objectID": "Learning Theory/3_Effective_Class_Size.html#vapnik-chervonenkis-vc-dimension",
    "href": "Learning Theory/3_Effective_Class_Size.html#vapnik-chervonenkis-vc-dimension",
    "title": "18  Effective Class Size",
    "section": "18.2 Vapnik-Chervonenkis (VC) dimension",
    "text": "18.2 Vapnik-Chervonenkis (VC) dimension\nWe say that a sample \\mathcal{S} is shattered by the hypothesis class \\mathcal{H} if a \\mathcal{H} can label \\mathcal{S} in every possible way. That is, \\mathcal{S} is shattered by \\mathcal{H} if\n\n\\mathcal{H} (\\mathcal{S}) = 2^{\\lvert \\mathcal{S} \\rvert}.\n\n\nThe Vapnik-Chervonenkis (VC) dimension of \\mathcal{H} is the size of the largest set that is shattered by \\mathcal{H}\n\n\\mathrm{VC} (\\mathcal{H}) = \\max_{n: \\mathcal{H} (\\mathcal{S}) = 2^{n}} n."
  },
  {
    "objectID": "Learning Theory/3_Effective_Class_Size.html#sauers-lemma",
    "href": "Learning Theory/3_Effective_Class_Size.html#sauers-lemma",
    "title": "18  Effective Class Size",
    "section": "18.3 Sauer’s lemma",
    "text": "18.3 Sauer’s lemma\nSauer’s lemma shows that the growth function of any hypothesis class \\mathcal{H} is upper-bounded by a function of its VC dimension.\n\nTheorem 18.1 For any hypothesis class \\mathcal{H} and any dataset size n, we have\n\n\\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove the lemma for any set of n instances and any hypothesis class \\mathcal{H} with \\mathrm{VC} (\\mathcal{H}) = d using induction on n and d.\nThat is, we will prove\n\nthe lemma is correct under the base cases where (n, d) = (0, d) and (n, d) = (n, 0),\nthe lemma is correct under general case for (n ,d) by assuming the lemma is true under the previous cases (m, c) where m &lt; n and c &lt; d.\n\nFirst the base case (n, d) = (0, d) implies that\n\n\\Pi_{\\mathcal{H}} (0) = 1 = {0 \\choose 0} = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {0 \\choose i}\n\nbecause any hypothesis class can have at most 1 distinct label on an empty set (n = 0).\nThen the base case (n, d) = (n, 0) means that \\mathcal{H} can only shatter the empty set \\emptyset and therefore according to the definition of shattering\n\n\\Pi_{\\mathcal{H}} (n) = \\mathcal{H} (\\emptyset) = 2^{0} = 1 = {n \\choose 0} = \\sum_{i = 0}^{0} {n \\choose i}.\n\nTherefore we have proven the lemma is true in the base case, that is,\n\n\\Pi_{\\mathcal{H}} (n) = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}\n\nis true when (n, d) = (0, d) and (n, d) = (n, 0).\nTo prove the general case of the lemma, take any set \\mathcal{S} = \\{\\mathbf{x}_{1} \\dots, \\mathbf{x}_{n} \\} with n instances and any hypothesis class \\mathcal{H}, we can classify each unique way of the label assignment \\pi \\in \\mathcal{H} (\\mathcal{S}) into 2 groups \\mathcal{G}_{1} and \\mathcal{G}_{2} based on whether \\pi can form a pair in \\mathcal{H} (\\mathcal{S}).\n\n(\\pi, \\pi') form a pair if \\pi (x_{i}) = \\pi' (x_{i}), \\forall i \\in [1, n - 1] and \\pi (x_{n}) = \\pi' (x_{n}) (\\pi agree with \\pi' for all x_{1}, \\dots, x_{n - 1} but not for x_{n}). We add the both \\pi and \\pi' to \\mathcal{G}_{1}.\n\\pi belongs to \\mathcal{G}_{2} if doesn’t belong to \\mathcal{G}_{1}.\n\nSince the pairs (\\pi, \\pi') \\in \\mathcal{G}_{1} have the same labels for \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n - 1} if we create \\mathcal{G}'_{1} by removing the labels for \\mathbf{x}_{n} in each \\pi \\in \\mathcal{G}_{1}\n\n\\mathcal{G}'_{1} = \\{ (\\pi (\\mathbf{x}_{1}), \\dots, \\pi (\\mathbf{x}_{n - 1})), \\forall \\pi \\in \\mathcal{G}_{1} \\} \\\\\n\nthen all of the pairs in \\mathcal{G}_{1} become the same label assignment, but if we create \\mathcal{G}_{2}' using the same procedure above, the resulting label assignments in \\mathcal{G}_{2}' are still unique.\nAlso, by the definition of \\mathcal{G}_{1}, \\mathcal{G}_{2}, \\mathcal{G}_{1}', and \\mathcal{G}_{2}', the label assignments in \\mathcal{G}_{1}' and \\mathcal{G}_{2}' do not overlap, and therefore\n\n\\mathcal{H} (\\mathcal{S}) = \\lvert \\mathcal{G}_{1} \\rvert + \\lvert \\mathcal{G}_{2} \\rvert = 2 \\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert.\n\nThen we will define 2 new hypotheses classes \\mathcal{H}_{1}, \\mathcal{H}_{2} whose domain is a set \\mathcal{S}' that is constructed by removing x_{n} from \\mathcal{S}\n\n\\mathcal{S}' = \\{\\mathbf{x}_{1} \\dots, \\mathbf{x}_{n - 1} \\}.\n\nand whose projections on \\mathcal{S}' are defined as\n\n\\mathcal{H}_{1} (\\mathcal{S}') = \\mathcal{G}_{1}' \\cup \\mathcal{G}_{2}' \\\\\n\\mathcal{H}_{2} (\\mathcal{S}') = \\mathcal{G}_{1}',\n\nand therefore,\n\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert = 2 \\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert = (\\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert) + \\lvert \\mathcal{G}_{1}' \\rvert = \\lvert \\mathcal{H}_{1} (\\mathcal{S}') \\rvert + \\lvert \\mathcal{H}_{2} (\\mathcal{S}') \\rvert.\n\nAlthough we never exactly defined what \\mathcal{H}_{1} and \\mathcal{H}_{2} are, we have completely specified their projections on the entire domain \\mathcal{S}', using which we can derive\n\n\\mathrm{VC} (\\mathcal{H}_{1}) \\leq \\mathrm{VC} (\\mathcal{H}),\n\nsince the \\mathcal{H} has all the same label assignments for \\mathcal{S}' = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n - 1} \\} that \\mathcal{H}_{1} has and any subset of \\mathcal{S}' that is shattered by \\mathcal{H}_{1} is shattered by \\mathcal{H}.\nFurthermore, since \\mathcal{H}_{2} (\\mathcal{S}') = \\mathcal{G}_{1}, if a subset \\mathcal{T} \\subseteq \\mathcal{S}' is shattered by \\mathcal{H}_{2}, then the set \\mathcal{T} \\cup \\{ x_{n} \\} must be shattered by \\mathcal{H}, which means\n\n\\mathrm{VC} (\\mathcal{H}_{2}) + 1 \\leq \\mathrm{VC} (\\mathcal{H}).\n\nNow we are ready to prove the general case of the lemma using the all results we proved above with \\mathcal{H}_{1} and \\mathcal{H}_{2}.\nAccording to the definition of growth function, for any hypothesis class \\mathcal{H} and any set \\mathcal{S} of size n\n\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert \\leq \\Pi_{\\mathcal{H}} (n)\n\nand since we have assumed that the lemma is correct for the case (m, c) where m &lt; n, c &lt; d,\n\n\\Pi_{\\mathcal{H}} (m) \\leq \\sum_{i = 0}^{c} {m \\choose i},\n\nwe have\n\n\\begin{aligned}\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert\n& = \\lvert \\mathcal{H}_{1} (\\mathcal{S}') \\rvert + \\lvert \\mathcal{H}_{2} (\\mathcal{S}') \\rvert\n\\\\\n& \\leq \\Pi_{\\mathcal{H}_{1}} (n - 1) + \\Pi_{\\mathcal{H}_{2}} (n - 1).\n\\\\\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{1})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{2})} {n - 1 \\choose i}\n\\\\\n\\end{aligned}\n\nSince we have proved that the relations between \\mathrm{VC} (\\mathcal{H}_{1}), \\mathrm{VC} (\\mathcal{H}_{2}) and \\mathrm{VC} (\\mathcal{H}),\n\n\\begin{aligned}\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{1})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{2})} {n - 1 \\choose i}\n\\\\\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}) - 1} {n - 1 \\choose i}\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 1}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i - 1} + {n - 1 \\choose -1}\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i - 1}\n& [{n - 1 \\choose - 1} = 0]\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} \\left[\n    {n - 1 \\choose i} + {n - 1 \\choose i - 1}\n\\right]\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}.\n\\end{aligned}\n\nSince all of the above proof is for any \\mathcal{S}, it also works for the largest \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert,\n\n\\sup_{\\mathcal{S}} \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert = \\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i},\n\nwhich proves the lemma under the general case.\n\n\n\nThe following theorem uses Sauer’s lemma to provide a closed form upper-bound of the growth function of any hypothesis class with its VC dimension.\n\nTheorem 18.2 For any 1 &lt; d = \\mathrm{VC} (\\mathcal{H}) &lt; n, we have\n\n\\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{d} {n \\choose i} \\leq \\left(\n    \\frac{ e }{ d } n\n\\right)^{d} = O (n^d).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that (\\frac{ d }{ n })^{d} &lt; (\\frac{ d }{ n })^{i}, i &lt; d since d &lt; n, and therefore\n\n\\begin{aligned}\n\\sum_{i = 0}^{d} {n \\choose i} \\left(\n    \\frac{ d }{ n }\n\\right)^{d}\n& \\leq \\sum_{i = 0}^{d} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i}\n\\right]\n\\\\\n& = \\sum_{i = 0}^{d} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\\\\n& \\leq \\sum_{i = 0}^{n} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\end{aligned}\n\nBy applying Binomial theorem (x + y)^{n} = \\sum_{i = 0}^{n} {n \\choose i} x^{i} y^{n - i}\n\n\\begin{aligned}\n\\sum_{i = 0}^{d} {n \\choose i} \\left(\n    \\frac{ d }{ n }\n\\right)^{d}\n& \\leq \\sum_{i = 0}^{n} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\\\\n& = \\left(\n    \\frac{ d }{ n } + 1\n\\right)^{n}\n\\\\\n& \\leq e^{d}.\n\\end{aligned}\n\n\n\n\nThe theorem above shows that the VC dimension marks the threshold between the exponential growth and polynomial growth of the growth function.\n\nWhen n &lt; d, by the definition of the VC dimension, we can always find a set of instances of size n \\mathcal{H} can shatter, so the growth function \\Pi_{\\mathcal{H}} (n) = 2^{n}, which means it grows exponentially with a factor of 2 as n increases,\nWhen n &gt; d, by the theorem above, the growth function is upper bounded by n^{d}, so it only grows in polynomials as n increases."
  },
  {
    "objectID": "Learning Theory/4_Empirical_Risk_Minimization.html#no-free-lunch-theorem",
    "href": "Learning Theory/4_Empirical_Risk_Minimization.html#no-free-lunch-theorem",
    "title": "19  Empirical Risk Minimization",
    "section": "19.1 No free lunch theorem",
    "text": "19.1 No free lunch theorem\nThe no-free-lunch (NFL) theorem for machine learning states that there is no algorithm that can generate perfect hypothesis for any distribution using a finite training set.\n\nTheorem 19.1 TODO\n\nAnother interpretation of the NFL theorem is that any two algorithms are equivalent when their performance is averaged across all possible distributions. Therefore, one must make some biased assumptions about the underlying distribution about the targeted problems, so that the algorithm can be designed to have good performance on the interested problem but having bad performance on the problems that we don’t care."
  },
  {
    "objectID": "Learning Theory/4_Empirical_Risk_Minimization.html#erm-with-inductive-bias",
    "href": "Learning Theory/4_Empirical_Risk_Minimization.html#erm-with-inductive-bias",
    "title": "19  Empirical Risk Minimization",
    "section": "19.2 ERM with inductive bias",
    "text": "19.2 ERM with inductive bias\nTo apply the NFL theorem to ERM, we make assumptions about the underlying distribution by adding inductive bias to the ERM algorithm. The inductive bias implies that we have a predetermined preference for some hypotheses over other hypotheses. One reasonable approach to inductive bias is to restrict the hypothesis class that ERM considers. Therefore, instead of looking for the best hypothesis over all possible functions, ERM method minimizes the risk over a selected hypothesis class \\mathcal{H} to derive the empirical risk minimizer h_{n}\n\nh_{n} = \\argmin_{h \\in \\mathcal{H}} R_{n} (h).\n\n\nLemma 19.1 A special property of the ERM over any hypothesis class \\mathcal{H} is that\n\n\\forall h \\in \\mathcal{H}: R (h) - R (h_{n}) \\leq 2 \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition, for all h \\in \\mathcal{H},\n\n\\begin{aligned}\nR (h) - R (h_{n})\n& = (R (h) + R_{n} (h) - R_{n} (h)) - (R (h_{n}) + R_{n} (h_{n}) - R_{n} (h_{n}))\n\\\\\n& = (R_{n} (h) - R_{n} (h_{n})) + (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\\end{aligned}\n\nSince h_{n} is the one that minimizes R_{n},\n\nR_{n} (h) - R_{n} (h_{n}) \\geq 0,\n\nso\n\nR (h) - R (h_{n}) \\leq (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\nSince both h, h_{n} \\in \\mathcal{H},\n\nR (h) - R_{n} (h) \\leq \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert\n\\\\\nR_{n} (h_{n}) - R (h_{n}) \\leq \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert,\n\nso\n\nR (h) - R (h_{n}) \\leq (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\\leq 2 \\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert."
  },
  {
    "objectID": "Learning Theory/5_Uniform_Convergence.html#uniform-convergence-property",
    "href": "Learning Theory/5_Uniform_Convergence.html#uniform-convergence-property",
    "title": "20  Uniform Convergence",
    "section": "20.1 Uniform convergence property",
    "text": "20.1 Uniform convergence property\nThe uniform convergence property of a given hypothesis class \\mathcal{H} states that there exists a large enough sample size such that for all hypotheses in the class \\mathcal{H}, the empirical risk is close to the true risk with high probability, regardless of the underlying distribution \\mathbb{P}_{Z}.\n\nDefinition 20.1 (Uniform convergence property) A hypothesis class \\mathcal{H} has the uniform convergence property if,\n\ngiven a set of labeled instances \\mathcal{S}, where instances and labels are sampled from any joint distribution \\mathbb{P}_{Z} over the instance space and the label space, and there exists a function for some \\epsilon &gt; 0 and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nfor every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P}_{\\mathcal{S}} (\\lvert R (h) - R_{\\mathcal{S}} (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\nNote that this definition is stated using the sample complexity n_{\\mathcal{H}} (\\epsilon, \\delta), which is the sample size that we need to supply, so that with an arbitrarily high probability 1 - \\delta, the considered event has an arbitrarily small error \\epsilon."
  },
  {
    "objectID": "Learning Theory/5_Uniform_Convergence.html#uniform-convergence-results",
    "href": "Learning Theory/5_Uniform_Convergence.html#uniform-convergence-results",
    "title": "20  Uniform Convergence",
    "section": "20.2 Uniform convergence results",
    "text": "20.2 Uniform convergence results\nThe following theorems state that a hypothesis class has the uniform convergence property if either it has a finite number of hypotheses or has a finite VC dimension.\n\nTheorem 20.1 (Uniform convergence theorem) Any finite hypothesis class \\mathcal{H} has the uniform convergence property with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 2 }{ \\delta }\n}{\n    2 \\epsilon^{2}\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the true risk of a hypothesis is the expectation of the empirical risk with respect to the joint distribution \\mathbb{P}_{Z}\n\nR(h) = \\mathbb{E}_{Z} \\left[\n    R_{n} (h)\n\\right] = \\mathbb{E}_{Z} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (h (z_{i}))\n\\right]\n\nand we can view the 0-1 loss on an instance as a bounded random variable\n\nL_{i} = L (h (\\mathbf{X_{i}}), Y_{i}) = \\mathbb{1} \\left[\n    h (\\mathbf{X_{i}}) \\neq Y_{i}\n\\right] \\in [0, 1],\n\nwe can apply Hoeffding’s inequality on L_{i} for a fixed hypothesis h \\in \\mathcal{H},\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L_{i} - \\mathbb{E}_{Z} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L_{i}\n        \\right]\n    \\right\\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} \\epsilon^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    \\lvert R_{n} (h) - \\mathbb{E}_{Z} \\left[\n        R_{n} (h)\n    \\right] \\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} \\epsilon^{2} }{ n }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    \\lvert R_{n} (h) - R (h) \\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right].\n\\end{aligned}\n\nThe above inequality only works for one h \\in \\mathcal{F}. we can apply union bound to extend it for all f \\in \\mathcal{F},\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\exist f \\in \\mathcal{F}, \\lvert R_{n} (f) - R (f) \\rvert \\geq \\epsilon\n\\right)\n& \\leq \\sum_{i = 1}^{\\lvert \\mathcal{F} \\rvert} \\mathbb{P} \\left(\n    \\lvert R_{n} (f_{i}) - R (f_{i}) \\rvert \\geq \\epsilon\n\\right)\n\\\\\n& \\leq 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right].\n\\end{aligned}\n\nSince \\mathbb{P} (X \\geq a) = 1 - \\mathbb{P} (X \\leq a)\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\exist f \\in \\mathcal{F}, \\lvert R_{n} (f) - R (f) \\rvert \\leq \\epsilon\n\\right)\n& \\geq 1 - 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right]\n\\\\\n& \\geq 1 - \\delta\n\\end{aligned}\n\nwhere\n\n\\begin{aligned}\n\\delta\n& = 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right]\n\\\\\nn\n& = \\frac{\n    \\log \\lvert \\mathcal{F} \\rvert + \\log \\frac{ 2 }{ \\delta }\n}{\n    2 \\epsilon^{2}\n}.\n\\end{aligned}\n\n\n\n\n\nTheorem 20.2 Any infinite hypothesis class \\mathcal{H} with a finite VC dimension has the uniform convergence property with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = 8 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 4 }{ \\delta }\n}{\n    \\epsilon^{2}\n}\n\nwhere n is the number of samples in the training set.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet’s first define 3 “bad” events that are useful in the following proof.\nGiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, let B (\\mathcal{S}) denote the event that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its true risk and empirical risk on \\mathcal{S} is larger than \\epsilon,\n\nB (\\mathcal{S}) \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}} (h) -  R (h) \\rvert \\geq \\epsilon.\n\nand therefore we want to prove\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq \\delta.\n\nNow let’s draw the “ghost samples”, which is another set of i.i.d labeled instances \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} from the distribution \\mathbb{P}_{Z}, and define another event B' as a function of \\mathcal{S} and \\mathcal{S}', which states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S} and empirical risk on \\mathcal{S}' is larger than \\frac{ \\epsilon }{ 2 },\n\nB' (\\mathcal{S}, \\mathcal{S}') \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}} (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }.\n\nFinally, let’s define an event B (\\mathcal{S}, \\mathcal{S}', \\sigma) as a function of \\mathcal{S}, \\mathcal{S}', and a set of independent Rademacher random variables \\sigma_{1}, \\dots, \\sigma_{n} that takes values -1 or 1 with equal probabilities\n\n\\begin{aligned}\nB'' (\\mathcal{S}, \\mathcal{S}', \\sigma)\n& \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}_{\\sigma}} (h) - R_{\\mathcal{S}_{\\sigma}'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\\\\n& \\coloneqq \\exist h \\in \\mathcal{H}: \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n        \\mathbb{1} \\left[\n            h (\\mathbf{x}_{i}) \\neq y_{i}\n        \\right] - \\mathbb{1} \\left[\n            h (\\mathbf{x}_{i}') \\neq y_{i}'\n        \\right]\n    \\right)\n\\right\\rvert \\geq \\frac{ \\epsilon }{ 2 },\n\\\\\n\\end{aligned}\n\nwhere the samples \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' are created by swapping the labeled instances in \\mathcal{S}, \\mathcal{S}' based on the values of \\sigma\n\nz_{i} and z_{i}' are swapped if the corresponding \\sigma_{i} = 1,\nand z_{i} and z_{i}' are not swapped if the corresponding \\sigma_{i} = -1.\n\nThe event B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S}_{\\sigma} and empirical risk on \\mathcal{S}_{\\sigma}' is larger than \\frac{ \\epsilon }{ 2 },\nClaim 1: \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) is upper-bounded by 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')),\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\nSince the probability of an event cannot be larger than its conjunction with another event,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n& \\geq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\cap B (\\mathcal{S}))\n\\\\\n& = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n\\end{aligned}\n\nNow consider the probability of the event\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})),\n\nwhich can be written as\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\leq \\frac{ \\epsilon }{ 2 }\n\\right)\n\nbecause it is the same as the \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left( \\lvert R_{\\mathcal{S}} (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 } \\right) if the event B (\\mathcal{S}) \\coloneqq \\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\geq \\epsilon is given.\nSince R (h) is the mean of R_{\\mathcal{S}'} (h), the probability of the difference between R (h) and R_{\\mathcal{S}'} (h) can be upper bounded by applying Chebyshev’s inequality with X = R_{\\mathcal{S}'} (h), \\mu = R (h), t = \\frac{ \\epsilon }{ 2 }, \\sigma^{2} = \\mathrm{Var} [R_{\\mathcal{S}'} (h)]\n\n\\begin{aligned}\n\\mathbb{P}_{X} \\left(\n    \\lvert x - \\mu \\rvert \\geq t\n\\right)\n& \\leq \\frac{ \\sigma^{2} }{ t^{2} }\n\\\\\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R_{S'} (h) - R (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\leq \\frac{ 4 \\mathrm{Var} [R_{\\mathcal{S}'} (h)] }{\\epsilon^{2}}.\n\\end{aligned}\n\nNote that h (\\mathbf{x}_{i}) \\neq y_{i} is a Bernoulli random variable whose variance is less than \\frac{ 1 }{ 4 }\n\n\\mathrm{Var} [R_{\\mathcal{S}'} (h)] = \\mathrm{Var} \\left[\n    \\frac{ 1 }{ n } \\sum_{\\mathbf{x} \\in \\mathcal{S}'} h (\\mathbf{x}_{i}) \\neq y_{i}\n\\right] = \\frac{ 1 }{ n^{2} } \\sum_{\\mathbf{x_{i} \\in \\mathcal{S}'}} \\mathrm{Var} [h (\\mathbf{x}) \\neq y_{i}] \\leq \\frac{ 1 }{ 4 n },\n\nand therefore,\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right) \\leq \\frac{ 1 }{n \\epsilon^{2}}.\n\nAssume that n \\epsilon^{2} \\geq 2\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\leq \\frac{ 1 }{ 2 }\n\\\\\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\leq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\geq \\frac{ 1 }{ 2 }.\n\\\\\n\\end{aligned}\n\nThen we have proved the claim\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\frac{ 1 }{ 2 } \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\\end{aligned}\n\nClaim 2: the probability of event B' (\\mathcal{S}, \\mathcal{S}') is the same as the expectation of the probability that B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) happens given \\mathcal{S}, \\mathcal{S}'\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n= \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right].\n\nSince the event B' (\\mathcal{S}, \\mathcal{S}') and B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) only differ on the set of instances \\mathcal{S}, \\mathcal{S}' and \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' and they can both be seen as the set of instances i.i.d sampled from the \\mathbb{P}_{Z}, their probability should be the same\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')) = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)).\n\nThen, we can prove the claim by using marginalization of the probability\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)) = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')].\n\nClaim 3: \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')] is upper-bounded by 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right]\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\nConsider the following probability for a fixed h \\in \\mathcal{H},\n\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n            \\mathbb{1} \\left[\n                h (\\mathbf{x}_{i}) \\neq y_{i}\n            \\right] - \\mathbb{1} \\left[\n                h (\\mathbf{x}_{i}') \\neq y_{i}'\n            \\right]\n        \\right)\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right).\n\nSince \\mathcal{S}, \\mathcal{S}' are given, the value \\alpha_{i} = \\mathbb{1} \\left[ h (\\mathbf{x}_{i}) \\neq y_{i} \\right] - \\mathbb{1} \\left[ h (\\mathbf{x}_{i}') \\neq y_{i}' \\right] is a fixed value and therefore\n\n\\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n    \\mathbb{1} \\left[\n        h (\\mathbf{x}_{i}) \\neq y_{i}\n    \\right] - \\mathbb{1} \\left[\n        h (\\mathbf{x}_{i}') \\neq y_{i}'\n    \\right]\n\\right) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n\nis a random variable with\n\n\\mathbb{E}_{\\sigma} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n\\right] = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\mathbb{E}_{\\sigma} [\\sigma_{i}] = 0.\n\nApplying Hoeffding’s inequality with X_{i} = \\alpha_{i} \\sigma_{i}, \\mu = 0, t = \\frac{ \\epsilon }{ 2 },\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i} - \\mathbb{E} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i}\n        \\right]\n    \\right\\rvert \\geq t\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} t^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i} - 0\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n& \\leq 2 \\exp \\left[\n    - \\frac{ 2 n^{2} \\frac{ \\epsilon^{2} }{ 4 } }{ 4 n }\n\\right]\n\\\\\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n& \\leq 2 \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nTo get the probability for any h \\in \\mathcal{H}, we apply union bound on all possible label assignments that \\mathcal{H} can make over the set \\mathcal{S} \\cup \\mathcal{S}',\n\n\\begin{aligned}\n\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n& =\n\\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H}: \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}'): \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\sum_{h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}')} \\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\\\\n\\end{aligned}\n\nNote that the term 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right] doesn’t depend on \\mathcal{S}, \\mathcal{S}'. Since the expectation of a constant is that constant, we have proved the claim\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n        - \\frac{ n \\epsilon^{2} }{ 8 }\n    \\right]\n\\right] \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\nFinally we can prove the theorem by using all of the claims above\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n& = 2 \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right]\n\\\\\n& \\leq 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nBy setting \\delta = 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right],\n\n\\begin{aligned}\n\\delta\n& = 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right]\n\\\\\nn\n& = 8 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 4 }{ \\delta }\n}{\n    \\epsilon^{2}   \n}.\n\\end{aligned}"
  },
  {
    "objectID": "Learning Theory/6_PAC_Learning.html#realizable-case",
    "href": "Learning Theory/6_PAC_Learning.html#realizable-case",
    "title": "21  PAC Learning",
    "section": "21.1 Realizable case",
    "text": "21.1 Realizable case\nUnder the realizable assumption, it is assumed that there exists a perfect concept from a concept class c \\in \\mathcal{C} such that all labels of the instances are labeled according to c and the hypothesis class that our algorithm ERM considers is the concept class \\mathcal{H} = \\mathcal{C}.\n\nDefinition 21.1 (Consistent) We say that a hypothesis h is consistent with a set of labeled instances \\mathcal{S} = \\{ (\\mathbf{x}_{1}, y_{1}), \\dots, (\\mathbf{x}_{n}, y_{n}) \\} if h (\\mathbf{x}_{i}) = y_{i} for all i.\n\nTherefore, under the realizable assumption, ERM can always find a hypothesis that is consistent with any given training set, and therefore we say that ERM learns in the consistency model.\n\n21.1.1 Consistency model\nLearning in the consistency model requires the algorithm to always predict correctly on the training set, but doesn’t care much about the generalization of the performance on the test set.\n\nDefinition 21.2 (Consistency model) An algorithm A learns the hypothesis class \\mathcal{H} = \\mathcal{C} in the consistency model if\n\ngiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, where instances are sampled from any distribution \\mathbb{P}_{\\mathbf{X}} over the instance space and are labeled by any concept c \\in \\mathcal{C},\nA can find a concept h \\in \\mathcal{H} that is consistent with \\mathcal{S} if h exists, or A outputs False if no such concept exists.\n\n\n\n\n21.1.2 Probably Approximately Correct (PAC) model\nLearning in the PAC model is more applicable in real world, as it emphasizes more on the generalization ability of the learned function from the algorithm.\n\nDefinition 21.3 (PAC model) An algorithm A learns the concept class \\mathcal{C} in the PAC model by the hypothesis class \\mathcal{H} = \\mathcal{C} if,\n\ngiven a set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, where instances are sampled from any distribution \\mathbb{P}_{\\mathbf{X}} over the instance space and are labeled by any concept c \\in \\mathcal{C}, and there exists a function for some $&gt; 0 $ and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nA returns a hypothesis h \\in \\mathcal{H}, where its true risk is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P} (R (h) \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\n\n\n21.1.3 ERM as a PAC learner\nHere we present some results about the generalization error of the algorithms using the definitions of consistency model and PAC model. Since ERM learns the hypothesis class in the consistency model, the following theorems naturally apply to it.\nThe following theorem states that a finite concept class \\mathcal{C} is PAC learnable by the same hypothesis class \\mathcal{H} = \\mathcal{C} if \\mathcal{C} is learnable in the consistency model, and proves its sample complexity as a function of the size of the hypothesis class.\n\nTheorem 21.1 If an algorithm A learns a finite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 1 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAnother way to state the PAC learnability with the consistency model is\n\n\\mathbb{P}_{\\mathcal{S}} (\\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon) \\leq \\delta\n\nwhen n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta).\nGiven h \\in \\mathcal{H}, by definition of the empirical risk we can write the probability that h is consistent with \\mathcal{S} as\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (R_{\\mathcal{S}} (h) = 0)\n& = \\mathbb{P}_{\\mathcal{S}} (h (\\mathbf{x_{i}}) = y_{i}, \\forall (\\mathbf{x}_{i}, y_{i}) \\in \\mathcal{S})\n\\\\\n& \\stackrel{(1)}{=} \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}} (h (\\mathbf{x}_{i}) = y_{i})\n\\\\\n& = \\prod_{i = 1}^{n} 1 - \\mathbb{P}_{\\mathbf{X}} (h (\\mathbf{x}_{i}) \\neq y_{i})\n\\\\\n& \\stackrel{(2)}{=} \\prod_{i = 1}^{n} 1 - R (h)\n\\\\\n& = (1 - R (h))^{n}.\n\\end{aligned}\n\n\n\nfollows because the labeled instances in \\mathcal{S} are independent.\n\n\nfollows because the true risk of h is the probability of h makes a mistake on a given labeled instance when the loss function is the 0-1 loss.\n\n\nIf we add the fact that R (h) \\geq \\epsilon,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon)\n& \\leq (1 - \\epsilon)^{n}\n\\\\\n& \\leq e^{- n \\epsilon}\n\\end{aligned}\n\nwhere the last inequality uses the fact that\n\n1 - x &lt; e^{-x}, \\forall x \\in [0, 1].\n\nWe can add the part \\exists h \\in \\mathcal{H} by applying the union bound\n\n\\mathbb{P}_{\\mathcal{S}} (\\exists h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon)\n\\leq \\lvert \\mathcal{H} \\rvert e^{- n \\epsilon},\n\nand make \\delta = \\lvert \\mathcal{H} \\rvert e^{- n \\epsilon}, we can derive\n\nn \\geq \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 1 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\nThe following theorem states a similar results as above: an infinite concept class \\mathcal{C} it is PAC learnable by the same hypothesis class \\mathcal{H} = \\mathcal{C} if \\mathcal{H} is learnable in the consistency model, and proves the sample complexity as a function of the growth function of \\mathcal{H}.\n\nTheorem 21.2 If an algorithm A learns an infinite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = 2 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet’s first define 3 “bad” events that are useful in the following proof.\nGiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, the difference between its true risk and empirical risk on \\mathcal{S} is larger than \\epsilon, let B (\\mathcal{S}) denote the event that there exists a hypothesis h \\in \\mathcal{H} that is consistent with \\mathcal{S} but has the true risk larger than \\epsilon\n\nB (\\mathcal{S}) \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0,  R (h) \\geq \\epsilon.\n\nand therefore we want to prove\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq \\delta.\n\nNow let’s draw the “ghost samples”, which is another set of i.i.d labeled instances \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} from the distribution \\mathbb{P}_{Z}, and define another event B' as a function of \\mathcal{S} and \\mathcal{S}', which states that there exists a hypothesis h \\in \\mathcal{H} that is consistent with \\mathcal{S} but has empirical risk on \\mathcal{S}' larger than \\frac{ \\epsilon }{ 2 }\n\nB' (\\mathcal{S}, \\mathcal{S}') \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0,  R_{S'} (h) \\geq \\frac{ \\epsilon }{ 2 }.\n\nFinally, let’s define an event B (\\mathcal{S}, \\mathcal{S}', \\sigma) as a function of \\mathcal{S}, \\mathcal{S}', and a set of independent Rademacher random variables \\sigma_{1}, \\dots, \\sigma_{n} that takes values -1 or 1 with equal probabilities\n\nB'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 }.\n\nwhere the samples \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' are created by swapping the labeled instances in \\mathcal{S}, \\mathcal{S}' based on the values of \\sigma\n\nz_{i} and z_{i}' are swapped if the corresponding \\sigma_{i} = 1,\nand z_{i} and z_{i}' are not swapped if the corresponding \\sigma_{i} = -1.\n\nThe event B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S}_{\\sigma} and empirical risk on \\mathcal{S}_{\\sigma}' is larger than \\frac{ \\epsilon }{ 2 }.\nClaim 1: \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) is upper-bounded by 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')),\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\nSince the probability of an event cannot be larger than its conjunction with another event,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n& \\geq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\cap B (\\mathcal{S}))\n\\\\\n& = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n\\end{aligned}\n\nNow consider the probability of the event\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})),\n\nwhich can be written as\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n\nbecause the event B' (\\mathcal{S}, \\mathcal{S}') is the event R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 } if the event B (\\mathcal{S}) is given.\nSince R (h) is the mean of R_{\\mathcal{S}'} (h), we can apply the lower tail case of the Chernoff bound for the average of Bernoulli variables and set X = R_{\\mathcal{S}'} (h), \\mu = R (h), \\delta = \\frac{ 1 }{ 2 }\n\n\\begin{aligned}\n\\mathbb{P}_{X} (X \\leq (1 - \\delta) \\mu)\n& \\leq \\exp \\left[\n    -\\frac{ n \\delta^{2} \\mu }{ 2 }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ R (h) }{ 2 }\n\\right)\n& \\leq \\exp \\left[\n    -\\frac{ n R (h) }{ 8 }\n\\right].\n\\end{aligned}\n\nSince R (h) \\geq \\epsilon and the assumption states that n &gt; \\frac{ 8 }{ \\epsilon }\n\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ \\epsilon }{ 2 }\n\\right) \\leq \\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ R (h) }{ 2 }\n\\right) \\leq \\exp \\left[\n    \\frac{ - n R(h) }{ 8 }\n\\right] \\leq \\exp \\left[\n    \\frac{ - R(h) }{ \\epsilon }\n\\right] \\leq \\frac{ 1 }{ e } \\leq \\frac{ 1 }{ 2 }\n\\\\\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 }\n\\right) \\geq \\frac{ 1 }{ 2 }\n\nThen we have proved the claim\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\frac{ 1 }{ 2 } \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\\end{aligned}\n\nClaim 2: the probability of event B' (\\mathcal{S}, \\mathcal{S}') is the same as the expectation of the probability that B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) happens given \\mathcal{S}, \\mathcal{S}'\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n= \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right].\n\nSince the event B' (\\mathcal{S}, \\mathcal{S}') and B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) only differ on the set of instances \\mathcal{S}, \\mathcal{S}' and \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' and they can both be seen as the set of instances i.i.d sampled from the \\mathbb{P}_{Z}, their probability should be the same\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')) = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)).\n\nThen, we can prove the claim by using marginalization of the probability\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)) = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')].\n\nClaim 3: \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')] is upper-bounded by \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon }{ 2 } \\right]\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon }{ 2 }\n\\right].\n\nRemember that \\mathcal{S}, \\mathcal{S}' all have n instances and therefore there are n pairs of instances (\\mathbf{x}_{1}, \\mathbf{x}_{1}'), \\dots, (\\mathbf{x}_{n}, \\mathbf{x}_{n}'). There are 3 cases for the corrections of the predictions made by h for each pair (h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}')).\n\nBoth h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}') are incorrect.\nEither h (\\mathbf{x}_{i}) or h (\\mathbf{x}_{i}') is incorrect (correct).\nBoth h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}') are correct.\n\nFirst if there is a pair in \\mathcal{S}, \\mathcal{S}' with case 1, then\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = 0\n\nbecause R_{\\mathcal{S}_{\\sigma}} (h) &gt; 0 no matter how to generate \\mathcal{S}_{\\sigma} by swapping instances in \\mathcal{S}, \\mathcal{S}'.\nThen denoted by r the number of pairs in \\mathcal{S}, \\mathcal{S}' that case 2 is true, if r &lt; \\frac{ \\epsilon n }{ 2 },\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = 0\n\nbecause R_{\\mathcal{S}_{\\sigma}'} (h) &lt; \\frac{ \\epsilon }{2} no matter how to generate \\mathcal{S}_{\\sigma}' by swapping instances in \\mathcal{S}, \\mathcal{S}'.\nWhen r \\geq \\frac{ \\epsilon n }{ 2 }, the event R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } is possible and its possibility is\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = \\left(\n    \\frac{ 1 }{ 2 }\n\\right)^{r} \\leq 2^{- \\frac{ \\epsilon n }{ 2 }}\n\nbecause the independent Rademacher random variables in \\sigma must take 1 with probability \\frac{ 1 }{ 2 } for all r' mistakes that were in \\mathcal{S} and swapped to be in \\mathcal{S}_{\\sigma}', and take -1 with probability \\frac{ 1 }{ 2 } for the r - r' mistakes that were in \\mathcal{S}' and are stayed in \\mathcal{S}_{\\sigma}'.\nSince the probability of the case 3 is already included in the calculation of the above probabilities, we can prove the claim by adding probabilities for all cases\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) \\leq 2^{- \\frac{ \\epsilon n }{ 2 }}.\n\nTo get the probability for any h \\in \\mathcal{H}, we apply union bound on all possible label assignments that \\mathcal{H} can make over the set \\mathcal{S} \\cup \\mathcal{S}',\n\n\\begin{aligned}\n\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}'): R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\sum_{h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}')} \\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ \\epsilon n }{ 2 }},\n\\\\\n\\end{aligned}\n\nwhere the last inequality is because of the definition of the growth function states that\n\n\\lvert \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}') \\rvert \\leq \\Pi_{H} (\\lvert \\mathcal{S} \\rvert + \\lvert \\mathcal{S}' \\rvert) = \\Pi_{\\mathcal{H}} (2 n).\n\nNote that the term \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }} doesn’t depend on \\mathcal{S}, \\mathcal{S}'. Since the expectation of a constant is that constant, we have proved the claim\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n        - \\frac{ n \\epsilon }{ 2 }\n    \\right]\n\\right] \\leq \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon }{ 2 }\n\\right].\n\nFinally we can prove the theorem by using all of the claims above\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n& = 2 \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right]\n\\\\\n& \\leq 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }}.\n\\end{aligned}\n\nBy setting \\delta = 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }},\n\n\\begin{aligned}\n\\delta\n& = 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }}\n\\\\\nn\n& = 2 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\\end{aligned}\n\n\n\n\nNow we can use the Sauer’s lemma to get a nice closed form expression on sample complexity result for the infinite class.\n\nTheorem 21.3 If an algorithm A learns an infinite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    8 d \\log \\frac{ 16 }{ \\epsilon} + 4 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n},\n\nwhere d = \\mathrm{VC} (\\mathcal{H}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Sauer’s lemma to the sample complexity results for the infinite classes\n\n\\begin{aligned}\n\\frac{\n    4 \\log \\Pi_{\\mathcal{H}} (2 n) + 2 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}\n& \\leq \\frac{\n    4 \\log \\left(\n        \\frac{ 2 e n }{ d }\n    \\right)^{d} + 2 \\log \\frac { 2 }{ \\delta }\n}{\n    \\epsilon\n}\n\\\\\n& = \\frac{ 4 d }{ \\epsilon } \\log n\n+ \\frac{ 4 d  }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\end{aligned}\n\nSince \\log x \\leq a x - \\log a - 1 for a, x &gt; 0, we can show that\n\n\\begin{aligned}\n\\log n\n& \\leq \\frac{ \\epsilon n }{ 8 d } - \\log \\frac{ \\epsilon }{ 8 d  } - 1\n\\\\\n\\frac{ 4 d }{ \\epsilon } \\log n\n& \\leq \\frac{ 4 d }{ \\epsilon } \\left(\n    \\frac{ \\epsilon n }{ 8 d } + \\log \\frac{ 8 d }{ \\epsilon } - 1\n\\right)\n\\\\\n& = \\frac{ n }{ 2 } + \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 8 d }{\\epsilon e }.\n\\end{aligned}\n\nBy combining the results above,\n\n\\begin{aligned}\n\\frac{\n    4 \\log \\Pi_{\\mathcal{H}} (2 n) + 2 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}\n& \\leq \\frac{ 4 d }{ \\epsilon } \\log n\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n& \\leq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 8 d }{\\epsilon e }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n& \\leq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }.\n\\end{aligned}\n\nTherefore, if we have a training set that has a number of instances\n\n\\begin{aligned}\nn\n& \\geq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n\\frac{ n }{ 2 }\n& \\geq \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\nn\n& \\geq \\frac{\n    8 d \\log \\frac{ 16 }{ \\epsilon} + 4 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\\end{aligned}"
  },
  {
    "objectID": "Learning Theory/6_PAC_Learning.html#unrealizable-case",
    "href": "Learning Theory/6_PAC_Learning.html#unrealizable-case",
    "title": "21  PAC Learning",
    "section": "21.2 Unrealizable case",
    "text": "21.2 Unrealizable case\nThe PAC learning in the unrealizable setting is also called agnostic PAC learning where the perfect concept cannot be realized because either one of the following events happens\n\nthe concept that the algorithm A learns is not in the hypothesis class that A considers,\nany instance can have contradictory labels, amd therefore there doesn’t exist a concept that can perfectly label all instances in the input space.\n\n\n21.2.1 Agnostic PAC model\n\nDefinition 21.4 An algorithm A learns the concept class \\mathcal{C} in the agnostic PAC model by the hypothesis class \\mathcal{H} if,\n\ngiven a set of labeled instances \\mathcal{S}, where instances and labels are sampled from any joint distribution \\mathbb{P}_{Z} over the instance space and the label space, and there exists a function for some $&gt; 0 $ and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nA returns a hypothesis h \\in \\mathcal{H}, where the difference between its true risk and the minimum true risk achieved by any hypothesis in \\mathcal{H} is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P} (\\lvert R (h) - \\min_{h \\in \\mathcal{H}} R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\n\n\n21.2.2 Uniform convergence implies agnostic PAC of ERM\nThe uniform convergence result guarantees the agnostic PAC learnability of ERM.\n\nLemma 21.1 If A is the ERM algorithm that learns the hypothesis class \\mathcal{H}, which satisfies uniform convergence with sample complexity n_{\\mathcal{H}}^{u}, then A learns \\mathcal{H} in the agnostic PAC model with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = n_{\\mathcal{H}}^{u} (\\frac{ \\epsilon }{ 2 }, \\delta).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet h_{n} be the hypothesis learned by ERM. According the property of the ERM, we have\n\nR (h) - R (h_{n}) \\leq 2 \\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert.\n\nSince \\mathcal{H} has @def:uniform-convergence-property, if n \\geq n_{\\mathcal{H}}^{u} (\\hat{\\epsilon}, \\delta), then\n\n\\forall h \\in \\mathcal{H}, \\mathbb{P} (\\lvert R (h) - R_{n} (h) \\rvert \\leq \\hat{\\epsilon}) \\geq 1 - \\delta,\n\nwhich is equivalent of\n\n\\begin{aligned}\n\\mathbb{P} (\\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert \\leq \\hat{\\epsilon})\n& \\geq 1 - \\delta\n\\\\\n\\mathbb{P} (R (h) - R (h_{n}) \\leq 2 \\hat{\\epsilon})\n& \\geq 1 - \\delta.\n\\end{aligned}\n\nBy setting \\epsilon = 2 \\hat{\\epsilon}, we have the conclusion that if n \\geq n_{\\mathcal{H}}^{u} (\\frac{ \\epsilon }{ 2 }, \\delta), then\n\n\\mathbb{P} (R (h) - R (h_{n}) \\leq \\epsilon) \\geq 1 - \\delta,\n\nwhich is the definition of agonistic PAC learning.\n\n\n\nBy the lemma Lemma 21.1, we can easily derive the sample complexity results for agnostic PAC by plugging $ = $ to the sample complexity results of the uniform convergence."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#definitions",
    "href": "Learning Theory/7_Rademacher_Complexity.html#definitions",
    "title": "22  Rademacher Complexity",
    "section": "22.1 Definitions",
    "text": "22.1 Definitions\n\nDefinition 22.1 A Rademacher variable has a discrete probability distribution where X has the equal probability of being +1 and -1.\n\nThe empirical Rademacher complexity measures the ability of the functions in a function class \\mathcal{F} to fit the random noise for a fixed sample \\mathcal{S}, which is described by the maximum correlation over all f \\in \\mathcal{F} between f (z_{i}) and \\sigma_{i}.\n\nDefinition 22.2 Given an i.i.d sample \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} from the distribution \\mathbb{P}_{\\mathcal{Z}^{n}} and n independent Rademacher random variables \\sigma = \\{ \\sigma_{1}, \\dots, \\sigma_{n} \\}, the empirical Rademacher complexity of a class of binary function \\mathcal{F} is defined as\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F}) = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right],\n\nwhich is a function of the random variable \\mathcal{S} and therefore is a random variable.\n\nTherefore, the Rademacher complexity of \\mathcal{F} measures the expected noise-fitting-ability of \\mathcal{F} over all data sets \\mathcal{S} \\in \\mathcal{Z}^{n} that could be drawn according to the distribution \\mathbb{P}_{\\mathcal{Z}^{n}}.\n\nDefinition 22.3 Then the Rademacher complexity is defined as the expectation of the empirical Rademacher complexity over all i.i.d samples of size n\n\n\\mathrm{Rad}_{n} (\\mathcal{F}) = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n\\right] = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right]\n\\right].\n\n\nFor completeness, we include the definition of Rademacher average of a set of vectors.\n\nDefinition 22.4 Given n independent Rademacher random variables \\sigma = \\{ \\sigma_{1}, \\dots, \\sigma_{n} \\}, the Rademacher average of a set of vectors \\mathcal{A} \\subseteq \\mathbb{R}^{n} is\n\n\\mathrm{Rad}_{\\mathcal{A}} = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#rademacher-complexity-properties",
    "href": "Learning Theory/7_Rademacher_Complexity.html#rademacher-complexity-properties",
    "title": "22  Rademacher Complexity",
    "section": "22.2 Rademacher complexity properties",
    "text": "22.2 Rademacher complexity properties\n\n22.2.1 Non-negativity\nThe empirical Rademacher complexity and Rademacher complexity are non-negative.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right]\n\\\\\n& \\stackrel{(1)}{\\geq} \\sup_{f \\in \\mathcal{F}} \\mathbb{E}_{\\sigma} \\left[\n     \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right]\n\\\\\n& = \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{E}_{\\sigma} [ \\sigma_{i} ] f (z_{i})\n\\\\\n& \\stackrel{(2)}{=} 0.\n\\end{aligned}\n\nExplanations in the derivations\n\nSince \\sup is a convex function, (1) follows because of the Jensen’s inequality.\n\nfollows because of the definition of Rademacher variable.\n\n\n\n\n\n\n\n22.2.2 Scaling and translation\nGiven any function class \\mathcal{F} and constants a, b \\in \\mathbb{R}, denote the function class \\mathcal{G} = \\{ g (x) = a f (x) + b \\}.\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{G}) = \\lvert a \\rvert \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n\n\n\\mathrm{Rad}_{n} (\\mathcal{G}) = \\lvert a \\rvert \\mathrm{Rad}_{n} (\\mathcal{F}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition of \\mathcal{G} and the empirical Rademacher complexity,\n\n\\begin{aligned}\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{G})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} (a f (z_{i}) + b)\n    \\right)\n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n        + \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b\n    \\right)\n\\right]\n\\\\\n& \\stackrel{(1)}{=} \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n    \\right)\n\\right]\n+ \\mathbb{E}_{\\sigma} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b\n\\right]\n\\\\\n& \\stackrel{(2)}{=} \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n    \\right)\n\\right]\n\\\\\n& \\stackrel{(3)}{=} \\lvert a \\rvert \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right)\n\\right]\n\\\\\n& = \\lvert a \\rvert \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F}).\n\\end{aligned}\n\nExplanations in the derivations\n\nSince the term \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b does not depend on f, it can be pulled out of \\sup_{f \\in \\mathcal{F}}. Then (1) follows because of the linearity of expectation.\n\nfollows because of the linearity of expectation and \\mathbb{E}_{\\sigma} [\\sigma_{i}] = 0.\n\nWhen a &lt; 0, \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i}) = \\lvert a \\rvert \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} - \\sigma_{i} f (z_{i}). Then (3) follows since \\sigma_{i} and -\\sigma_{i} have the same distribution."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#symmetrization-lemma",
    "href": "Learning Theory/7_Rademacher_Complexity.html#symmetrization-lemma",
    "title": "22  Rademacher Complexity",
    "section": "22.3 Symmetrization lemma",
    "text": "22.3 Symmetrization lemma\nHere we proved an important result with the Rademacher complexity using so called symmetrization technique, which involves creating a “ghost” sample as a hypothetical independent copy of the original sample.\n\nTheorem 22.1 For any class of measurable functions \\mathcal{F}, the expectation of the maximum error in estimating the mean of any function f \\in \\mathcal{F} is bounded by 2 times of the Rademacher complexity\n\n\\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n= \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right] \\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})\n\nwhere E_{\\mathcal{S}} (f) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f (z_{i}) is the estimated expectation of f on the sample \\mathcal{S}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy using the symmetrization technique, we introduce a ghost sample \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} that is also i.i.d drawn from \\mathbb{P}_{\\mathcal{Z}^{n}}, which means\n\n\\mathbb{E}_{\\mathcal{S}'} \\left[\n    E_{\\mathcal{S}'} (f)\n\\right] = \\mathbb{E}_{Z} [f (z_{i})].\n\nTherefore, we can get the following results\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right]\n& = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f (z_{i})\n        - \\mathbb{E}_{\\mathcal{S}'} \\left[\n            \\frac{ 1 }{ n } \\sum_{z_{i}' \\in \\hat{\\mathcal{S}}} f (z_{i}')\n        \\right]\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(1)}{=} \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\mathbb{E}_{\\mathcal{S}'} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n        \\right]\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(2)}{\\leq} \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n\\end{aligned}\n\nExplanations in the derivations\n\n\nuses the linearity of expectation and \\frac{ 1 }{ n } \\sum_{z_{i} \\in \\mathcal{S} f (z_{i})} is a constant.\n\n\nuses Jensen’s inequality since \\sup is a convex operator.\n\n\nSince f (z_{i}) - f (z_{i}') is invariant of sign change, we get\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n& = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}', \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(1)}{\\leq} \\mathbb{E}_{\\mathcal{S}, \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right\\rvert\n\\right]\n+ \\mathbb{E}_{\\hat{\\mathcal{S}}, \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} - \\sigma_{i} f (z_{i}')\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(2)}{=} 2 \\mathrm{Rad}_{n} (\\mathcal{F}).\n\\end{aligned}\n\nExplanations in the derivations\n\n\nfollows because of the \\sup_{f \\in \\mathcal{F}} operator,\n\n\nfollows because \\sigma_{i} = - \\sigma_{i} by the definition of Rademacher variable.\n\n\nTherefore we have reached our conclusion\n\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right]\n\\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#rademacher-based-uniform-convergence",
    "href": "Learning Theory/7_Rademacher_Complexity.html#rademacher-based-uniform-convergence",
    "title": "22  Rademacher Complexity",
    "section": "22.4 Rademacher-based uniform convergence",
    "text": "22.4 Rademacher-based uniform convergence\n\nTheorem 22.2 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}}, if the function class \\mathcal{F} only contains the functions f such that f (x) \\in [a, a + 1], then for every f \\in \\mathcal{F}, the difference between its true expectation and estimated expectation is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall f \\in \\mathcal{F}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{F}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nGiven a function f \\in \\mathcal{F}, the difference between its true expectation and estimated expectation on a sample \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} is less than the maximum difference of the expectations among all functions in \\mathcal{F}, which is denoted by \\phi (\\mathcal{S})\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f) \\rvert  \\leq \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}} (\\hat{f}) \\rvert] = \\phi (\\mathcal{S}).\n\nFirst we will prove the following property so that we can use McDiarmid’s inequality on \\phi (\\mathcal{S})\n\n\\sup_{\\mathcal{S}, \\mathcal{S}'} \\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert \\leq \\frac{ 1 }{ n }\n\nwhere \\mathcal{S}' = \\{ z_{1}, \\dots, z_{j}', \\dots, z_{n} \\} has z_{j}' different from z_{j} in \\mathcal{S}.\nLet f^{*} \\in \\mathcal{F} be the function that maximizes \\phi (\\mathcal{S})\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}} (f^{*}) \\rvert\n= \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}} (\\hat{f}) \\rvert]\n= \\phi (\\mathcal{S})\n\nand by definition\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}'} (f^{*}) \\rvert\n\\leq \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}'} (\\hat{f}) \\rvert]\n= \\phi (\\mathcal{S}').\n\nTherefore,\n\n\\begin{aligned}\n\\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert\n& \\leq \\lvert \\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}} (f^{*}) \\rvert\n- \\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}'} (f^{*}) \\rvert \\rvert\n\\\\\n& = \\lvert E_{\\mathcal{S}'} (f^{*}) - E_{\\mathcal{S}} (f^{*}) \\rvert\n\\\\\n& = \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}) - \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}')\n\\right\\rvert.\n\\end{aligned}\n\nSince \\mathcal{S} and \\mathcal{S}' only differ on two elements z_{j}, z_{j}', this becomes\n\n\\begin{aligned}\n\\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert\n& \\leq \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}) - \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}')\n\\right\\rvert\n\\\\\n& = \\frac{ 1 }{ n } \\left\\lvert \\left(\n        \\sum_{i \\neq j} f^{*} (z_{i}) + f^{*} (z_{j})\n    \\right) - \\left(\n        \\sum_{i \\neq j} f^{*} (z_{i}) + f^{*} (z_{j}')\n    \\right)\n\\right\\rvert\n\\\\\n& = \\frac{ 1 }{ n } \\left\\lvert f^{*} (z_{j}) - f^{*} (z_{j}') \\right\\rvert\n\\\\\n& \\leq \\frac{ 1 }{ n }.\n\\end{aligned}\n\nThis results show that the function \\phi follows the bounded difference property, so we can apply the McDiarmid’s inequality on \\phi,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (\\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\geq t)\n& \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} \\left(\n        \\frac{ 1 }{ n }\n    \\right)^{2} }\n\\right]\n\\\\\n& \\leq e^{- 2 m t^{2}}.\n\\end{aligned}\n\nBy setting \\delta = e^{-2 n t^{2}}, we can derive that t = \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}, so\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} \\left(\n    \\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\geq\n    \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\right)\n& \\leq \\delta\n\\\\\n\\mathbb{P}_{\\mathcal{S}} \\left(\n    \\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\leq\n    \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\right)\n& \\geq 1 - \\delta.\n\\end{aligned}\n\nwhich means we have the following fact with the probability larger than 1 - \\delta,\n\n\\phi (\\mathcal{S}) \\leq \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}.\n\nBy plugging back the result to the equation that we want to prove, we have the final results\n\n\\begin{aligned}\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f)] \\rvert\n& \\leq \\phi (\\mathcal{S})\n\\\\\n& \\leq \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\\\\n& \\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\end{aligned}\n\nwith the probability larger than 1 - \\delta.\n\n\n\n\n22.4.1 Results for risks\nGiven a hypothesis class \\mathcal{H}, a corresponding loss class with the 0-1 loss can be defined as\n\n\\mathcal{L} = \\{ l_{h} \\mid l_{h} (z) = L (h (\\mathbf{x}), y), h \\in \\mathcal{H}, z \\sim \\mathcal{Z} \\}\n\nand therefore the empirical risk and true risk can be defined as\n\nR_{\\mathcal{S}} (h) = E_{\\mathcal{S}} (l_{h}), R (h) = \\mathbb{E}_{\\mathcal{Z}} [l_{h} (z)].\n\nSince all the loss functions l_{h} \\in \\mathcal{L} have output range [0, 1], we can apply the uniform theorem above to the loss class \\mathcal{L} to derive the uniform convergence results for risks.\n\nCorollary 22.1 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}}, a hypothesis class \\mathcal{H}, and the corresponding 0-1 loss class \\mathcal{L}, for every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall h \\in \\mathcal{H}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{L}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}.\n\n\nBy using the following lemma, we can write the results in terms of the Rademacher complexity the hypothesis class \\mathcal{H} instead of the loss class \\mathcal{L}.\n\nLemma 22.1 Given a hypothesis class \\mathcal{H} and the corresponding loss class \\mathcal{L}, we have\n\n\\mathrm{Rad}_{n} (\\mathcal{H}) = 2 \\mathrm{Rad}_{n} (\\mathcal{L}).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy the definition of Rademacher complexity and 0-1 loss, we have\n$$\n\\begin{aligned}\n\\mathrm{Rad}_{S} (\\mathcal{H})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} \\mathbb{1} (h(x_{i}) \\neq y_{i})\n    \n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} \\left(\n        \\frac{ 1 }{ 2 } - y_{i} h (x_{i})\n    \\right)\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\left[\n        \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i}\n        + \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} (-y_{i} h (x_{i}))\n    \\right]\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i}\n    + \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} (- y_{i} h (x_{i}))\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} h (x_{i})\n\\right]\n& \\quad [\\mathbb{E}\\left[\\sum_{i=1}^{m}\\sigma_{i}\\right] = 0]\n\\\\\n& = \\frac{ 1 }{ 2 }\\text{Rad}_{S}(\\mathcal{H}).\n\\end{aligned}\n$$\n\n\n\n\nCorollary 22.2 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}} and a hypothesis class \\mathcal{H}, for every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall h \\in \\mathcal{H}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{H}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#bounding-rademacher-complexity",
    "href": "Learning Theory/7_Rademacher_Complexity.html#bounding-rademacher-complexity",
    "title": "22  Rademacher Complexity",
    "section": "22.5 Bounding Rademacher complexity",
    "text": "22.5 Bounding Rademacher complexity\nThe Rademacher complexity can be upper bounded for any function class with a finite VC dimension.\n\n22.5.1 Massart’s lemma\n\nLemma 22.2 (Massart’s lemma) Given any set of vectors \\mathcal{A} \\subseteq \\mathbb{R}^{n} the empirical Rademacher average is upper-bounded\n\n\\mathrm{Rad} (\\mathcal{A}) \\leq \\frac{ R \\sqrt{2 \\log \\lvert \\mathcal{A} \\rvert} }{ n }\n\nwhere R = \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\lVert \\mathbf{a} \\rVert_{2}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy Jensen’s inequality, the following quantity can be upper-bounded\n\n\\begin{aligned}\n\\exp \\left[\n    s \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n        s \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& \\stackrel{(1)}{\\leq} \\mathbb{E}_{\\sigma} \\left[\n    \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n        s \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& = \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\mathbb{E}_{\\sigma} \\left[\n    \\prod_{i = 1}^{n} \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& \\stackrel{(2)}{=} \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\end{aligned}\n\nExplanations in the derivations\n\n\nfollows since \\exp is non-negative and therefore \\sup \\leq \\sum.\n\n\nfollows because of the independence between \\sigma_{i}.\n\n\nSince \\mathbb{E}_{\\sigma_{i} a_{i}} = 0, we can apply Hoeffding’s lemma with \\mu = 0\n\n\\begin{aligned}\n\\exp \\left[\n    s \\sigma_{i} a_{i}\n\\right]\n& \\leq \\exp \\left[\n    \\frac{ s^{2} (2 a_{i})^{2}}{ 8 }\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ s^{2} a_{i}^{2} }{ 2 }\n\\right],\n\\end{aligned}\n\nand therefore\n\n\\begin{aligned}\n\\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\exp \\left[\n    \\frac{ s^{2} a_{i}^{2} }{ 2 }\n\\right]\n\\\\\n& = \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sum_{i = 1}^{n} a_{i}^{2}\n\\right]\n\\\\\n& \\leq \\lvert \\mathcal{A} \\rvert \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}\n\\right].\n\\end{aligned}\n\nwhere the last inequality follows because \\sum_{i = 1}^{n} f (a_{i}) \\leq n \\sup_{a_{i}} f (a_{i}), \\forall f.\nCombining all pieces together,\n\n\\begin{aligned}\n\\exp \\left[\n    s \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\lvert \\mathcal{A} \\rvert \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}\n\\right]\n\\\\\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n\\end{aligned}\n\nwhere R^{2} = \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}.\nSince \\log \\frac{ \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 } is a convex function, we can minimize it with respect to s\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n& = 0\n\\\\\n- \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s^{2} } + \\frac{ R^{2} }{ 2 }\n& = 0\n\\\\\ns\n& = \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R }.\n\\end{aligned}\n\nPlugging it back\n\n\\begin{aligned}\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n\\\\\n& = \\frac{\n    \\log \\lvert \\mathcal{A} \\rvert\n}{\n    \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R }\n} + \\frac{\n    \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R } R^{2}\n}{\n    2\n}\n\\\\\n& = R \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert }\n\\\\\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ R \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ n },\n\\end{aligned}\n\nwhere the last equation is derived by dividing both sides by n.\n\n\n\n\n\n22.5.2 Connection with VC theory\n\nLemma 22.3 The Rademacher complexity of the hypothesis class \\mathcal{H} with the finite VC dimension d is upper-bounded\n\n\\mathrm{Rad}_{n} (\\mathcal{H})\n\\leq \\sqrt{\\frac{ 2 d \\log \\frac{ e n }{ d } }{ n }}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf \\mathcal{H} has a finite VC dimension, its projection to any sample \\mathcal{S} with size n is finite. Since each \\mathcal{H} (\\mathcal{S}) can be seen as a set of vectors of length n, we can replace the set of vectors \\mathcal{A} in Lemma 22.2 with \\mathcal{H} (\\mathcal{S}).\nSince h^{2} (z_{i}) = 1, \\forall z_{i}, \\forall h \\in \\mathcal{H},\n\nR = \\sup_{h \\in \\mathcal{H}} \\sqrt{\\sum_{i = 1}^{n} h^{2} (z_{i})} = \\sqrt{n}.\n\nso we have\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{H} (\\mathcal{S}))\n\\leq \\frac{ \\sqrt{n} \\sqrt{2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert} }{ n }\n= \\sqrt{\\frac{ 2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert }{ n }}.\n\nBy the definition of growth function and Sauer’s lemma \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert \\leq \\Pi_{\\mathcal{H}} (\\mathcal{S}) \\leq \\left( \\frac{ e }{ d } n \\right)^{d}, where d is the VC dimension of \\mathcal{H}, we can derive\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{H})\n\\leq \\sqrt{\\frac{ 2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert }{ n }}\n\\leq \\sqrt{\\frac{ 2 \\log \\Pi_{\\mathcal{H}} (\\mathcal{n}) }{ n }}\n\\leq \\sqrt{\\frac{ 2 d \\log \\frac{ e n }{ d } }{ n }}."
  }
]