[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Joeyonng’s Notebook",
    "section": "Welcome",
    "text": "Welcome\nHere is my notebook where I used to share my summaries on general machine learning methods.\nThe goal is to make my notes formal, comprehensive and most importantly, easy-to-read.\nThis repository is far from complete and I will continue to cover more topics as I learn more in my PhD journey.\nFeel free to point any error you found in my notes. Any suggestions will be much appreciated!."
  },
  {
    "objectID": "index.html#update-history",
    "href": "index.html#update-history",
    "title": "Joeyonng’s Notebook",
    "section": "Update History",
    "text": "Update History\n\n12/16/2023: add “Probability and Statistics” section with some TODOs.\n12/05/2023: add “Linear Algebra” section with some TODOs.\n12/02/2023: website init with “Learning Theory” section."
  },
  {
    "objectID": "notations.html#notations",
    "href": "notations.html#notations",
    "title": "Notations and Facts",
    "section": "Notations",
    "text": "Notations\n\nMathematical definitions\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\nx\nScalar\nVariables are scalars (numbers).\n\n\n\\mathbf{x}\nVector\nBold non-capitalized variables are vectors.\n\n\n\\hat{\\mathbf{x}}\nUnit vector\nVectors that have a hat are unit vectors.\n\n\n\\mathbf{X}\nMatrix\nBold capitalized variables are matrices.\n\n\nX\nRandom variable\nCapitalized variables are random variables.\n\n\n\\mathcal{X}\nSet\nCalligraphic variables are sets.\n\n\n\n\n\nMathematical operations\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbf{a} \\cdot \\mathbf{b}\nDot product\nDot product between vector \\mathbf{a} and \\mathbf{b} (same as \\mathbf{a}^{T} \\mathbf{b}).\n\n\n\\mathbf{A}\\mathbf{b}\nMatrix vector product\nMatrix product between matrix \\mathbf{A} and vector \\mathbf{b} (column matrix).\n\n\n\\mathbf{a}^{T}\nVector transpose\nThe transposed vector is a matrix of size 1 \\times d\n\n\n\\mathbf{A}^{T}\nMatrix transpose\nTranspose a matrix.\n\n\n\n\n\nMathematical indexing\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbf{A}_{i, j}\nMatrix element selection\nSelect the scalar at row i and column j of matrix \\mathbf{A}.\n\n\n\\mathbf{A}_{i, *}\nMatrix row selection\nSelect the vector at row i of matrix \\mathbf{A}.\n\n\n\\mathbf{A}_{*, j}\nMatrix column selection\nSelect the vector at column j of matrix \\mathbf{A}.\n\n\n\\mathbf{a}_{i}\nVector element selection\nSelect the scalar at index i of vector \\mathbf{a}.\n\n\n\n\n\nOthers\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbb{1}_{cond}\nConditional operator\nEvaluates to 1 if cond is true, 0 otherwise."
  },
  {
    "objectID": "notations.html#facts",
    "href": "notations.html#facts",
    "title": "Notations and Facts",
    "section": "Facts",
    "text": "Facts\n\nLogarithm\n\nProduct\n \\ln(xy) = \\ln(x) + \\ln(y) \nQuotient\n \\ln \\left( \\frac{x}{y} \\right) = \\ln(x) - \\ln(y) \nLog of power\n \\ln(x^{y}) = y \\ln(x) \nLog reciprocal\n \\ln \\left( \\frac{1}{x} \\right) = \\ln(1) - \\ln(x) = 0 - \\ln(x) = -\\ln(x) \n\n\n\nLinear algebra\n\nThe squared norm of vector \\mathbf{x}\n \\lVert \\mathbf{x} \\rVert^{2} = \\mathbf{x} \\cdot \\mathbf{x} = \\mathbf{x}^{T} \\mathbf{x}\nThe squared norm of a vector difference between \\mathbf{a} and \\mathbf{b}\n \\lVert \\mathbf{a} - \\mathbf{b} \\rVert^{2} = (\\mathbf{a} - \\mathbf{b})^{T} (\\mathbf{a} - \\mathbf{b}) = \\mathbf{a}^{T}\\mathbf{a} - 2 \\mathbf{a}^T\\mathbf{b} + \\mathbf{b}^{T}\\mathbf{b} \nThe matrix form of the dot product between two vectors \\mathbf{a} and \\mathbf{b} with a coefficient \\lambda\n \\lambda(\\mathbf{a} \\cdot \\mathbf{b}) = \\mathbf{a}^{T} \\mathbf{\\Lambda} \\mathbf{b} \nwhere \\mathbf{\\Lambda} is a diagonal matrix with value \\lambda.\nThe transpose of the product of two matrices \\mathbf{A} and \\mathbf{B}\n (\\mathbf{A}\\mathbf{B})^{T} = \\mathbf{B}^{T}\\mathbf{A}^{T}"
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#fields",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#fields",
    "title": "1  Fields and Spaces",
    "section": "Fields",
    "text": "Fields\nA field is a set \\mathbb{F}, equipped with two operations addition + and multiplication \\cdot, obeying the rules (axioms) listed below.\n\nAxioms of fields\nFor all x, y, and z in the field \\mathbb{F} (\\forall x, y, z \\in \\mathbb{F}), we have:\n\nClosure under addition and multiplication:\n\n  x + y \\in \\mathbb{F},\n  \n\n  x \\cdot y \\in \\mathbb{F}.\n  \nCommutativity of addition and multiplication:\n\n  x + y = y + x,\n  \n\n  x \\cdot y = y \\cdot x.\n  \nAssociativity of addition and multiplication:\n\n  (x + y) + z = x + (y + z),\n  \n\n  (x \\cdot y) \\cdot z = x \\cdot (y \\cdot z).\n  \nDistributive property of multiplication:\n\n  (x + y) \\cdot z = x \\cdot z + y \\cdot z.\n  \nThere is an element in \\mathbb{F} called “zero” 0 \\in \\mathbb{F} such that\n\n  x + 0 = x,\n  \nand there is another element in \\mathbb{F} called “one” 1 \\in \\mathbb{F}, 1 \\neq 0, such that\n\n  x \\cdot 1 = x.\n  \nFor each x \\in \\mathbb{F}, there is an element in \\mathbb{F} called addictive inverse x_{I} of x such that\n\n  x + x_{I} = 0,\n  \nand if x \\neq 0, there is an element in \\mathbb{F} called multiplicative inverse x^{-1} of x such that\n\n  x \\cdot x^{-1} = 1.\n  \n\n\n\nProperties of fields\n\nZero and one are unique: there is only one “zero” and one “one” in any field \\mathbb{F}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove “zero” is unique by contradiction.\nSuppose there are two different “zero”s 0_{0} and 0_{1}.\nDue to the definition of “zero”,\n\n  \\begin{aligned}\n  0_{1} + 0_{0}\n  & = 0_{1}\n  & [0_{0} \\text{ is \"zero\"}]\n  \\\\\n  0_{0} + 0_{1}\n  & = 0_{0}\n  & [0_{1} \\text{ is \"zero\"}]\n  \\\\\n  \\end{aligned}\n  \nDue to the Commutativity axiom,\n\n  \\begin{aligned}\n  0_{1} + 0_{0}\n  & = 0_{0} + 0_{1}\n  \\\\\n  0_{1}\n  & = 0_{0}\n  \\\\\n  \\end{aligned}\n  \nwhich contradicts to the fact that 0_{0} and 0_{1} (1_{0} and 1_{1}) are different. Thus, there is only a unique “zero” in \\mathbb{F}.\nThe proof that the “one” in any \\mathbb{F} is unique is the same as above by replacing every addition + with multiplication \\cdot and 0_{0}, 0_{1} with 1_{0}, 1_{1}.\n\n\n\nAddictive and multiplicative inverse of every element are unique: there is only one addictive inverse and multiplicative inverse of every element (other than “zero” for multiplicative inverse) in any field \\mathbb{F}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the addictive inverse is unique by contradiction.\nSuppose there are two different addictive inverses of x: x_{1} and x_{2}.\nBy following the definition of addictive inverse,\n\n  (x + x_{1}) + x_{2} = 0 + x_{2} = x_{2}.\n  \nAlso,\n\n  \\begin{aligned}\n  (x + x_{1}) + x_{2}\n  & = x_{2}\n  \\\\\n  x + (x_{1} + x_{2})\n  & = x_{2}\n  & [\\text{associativity}]\n  \\\\\n  x + (x_{2} + x_{1})\n  & = x_{2}\n  & [\\text{commutativity}]\n  \\\\\n  (x + x_{2}) + x_{1}\n  & = x_{2}\n  & [\\text{associativity}]\n  \\\\\n  x_{1}\n  & = x_{2}\n  \\end{aligned}\n  \nwhich contradicts to the fact that x_{1} and x_{2} are different. Thus, the addictive inverse of every element in any field is unique.\nThe proof that the multiplicative inverse of every element in any field is unique is the same as above by replacing every addition + with multiplication \\cdot.\n\n\n\nFor every x in \\mathbb{F}, x \\cdot 0 = 0\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider\n\n  \\begin{aligned}\n  x \\cdot 0 + x \\cdot 0\n  & = x \\cdot (0 + 0)\n  & [\\text{distributive property}]\n  \\\\\n  & = x \\cdot 0\n  & [\\text{definition of } 0]\n  \\end{aligned}\n  \nBy the definition of addictive inverse, there must exist an addictive inverse y of x \\cdot 0 in \\mathbb{F} such that\n\n  \\begin{aligned}\n  x \\cdot 0 + y\n  & = 0\n  \\\\\n  (x \\cdot 0 + x \\cdot 0) + y\n  & = 0\n  \\\\\n  x \\cdot 0 + (x \\cdot 0 + y)\n  & = 0\n  & [\\text{associativity}]\n  \\\\\n  x \\cdot 0 + 0\n  & = 0\n  & [\\text{addictive inverse}]\n  \\\\\n  x \\cdot 0\n  & = 0\n  & [\\text{definition of 0}]\n  \\end{aligned}\n  \n\n\n\nx_{I} = 1_{I} \\cdot x\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n  \\begin{aligned}\n  (x + x_{I}) + 1_{I} \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  (x_{I} + x) + 1_{I} \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + (x + 1_{I} \\cdot x)\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + (1 \\cdot x + 1_{I} \\cdot x)\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + (1  + 1_{I}) \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + 0 \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + 0\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I}\n  & = 1_{I} \\cdot x\n  \\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#vector-spaces",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#vector-spaces",
    "title": "1  Fields and Spaces",
    "section": "Vector spaces",
    "text": "Vector spaces\nA vector space, defined over a field \\mathbb{F}, is a non-empty set \\mathcal{V} (whose members are called vectors), equipped with two operations: vector addition + and scalar multiplication \\cdot, obeying the rules (axioms) listed below.\n\nAxioms of vector spaces\nFor all u, v, and w in the vector space \\mathcal{V} (\\forall u, v, w \\in \\mathcal{V}) and for all \\alpha and \\beta in the field \\mathbb{F}, we have\n\nClosure under vector addition and scalar multiplication\n\n\nu + v \\in \\mathcal{V},\n\n\n\\alpha \\cdot v \\in \\mathcal{V}.\n\n\nCommutativity of vector addition:\n\n  u + v = v + u.\n  \nNote that there is no requirement of the commutativity of scalar multiplication in the definition of the vector space.\nAssociativity of vector addition and scalar multiplication:\n\n  (u + v) + \\mathbf{w} = u + (v + \\mathbf{w}),\n  \n\n  (\\alpha \\cdot \\beta) \\cdot v = \\alpha \\cdot (\\beta \\cdot v).\n  \nNote that in the left hand side of the second equation, the first dot is field multiplication while the second one is the scalar multiplication, but the in the right hand side both dots are scalar multiplication.\nDistributive property of scalar multiplication:\n\n  \\alpha \\cdot (u + v) = \\alpha \\cdot u + \\alpha \\cdot v,\n  \n\n  (\\alpha + \\beta) \\cdot v = \\alpha \\cdot v + \\beta \\cdot v.\n  \nThere is an element in \\mathcal{V} called “zero” vector 0 \\in \\mathcal{V} such that\n\n  v + 0 = v,\n  \nand the definition of “one” in the field 1 \\in \\mathbb{F} is applied in the vector space\n\n  1 \\cdot v = v.\n  \nNote that there is no requirement for the existence of “one” vector in the definition of the vector space.\nFor each v \\in \\mathcal{V}, there is an element in \\mathcal{V} called addictive inverse vector v_{I} of v such that\n\n  v + v_{I} = 0.\n  \nNote that there is no requirement for the scalar multiplicative inverse in the definition of the vector space.\n\n\n\nLinear combination\nLet \\mathcal{V} be a vector space over the field \\mathbb{F}. Given a set of vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} and a set of field elements \\alpha_{1}, \\dots, \\alpha_{n} \\in \\mathbb{F}, the vector u is a linear combination of v_{1}, \\dots, v_{n} with \\alpha_{1}, \\dots, \\alpha_{n} as coefficients if\n\nu = \\sum_{i=1}^{n} \\alpha_{i} v_{i}."
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#subspaces",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#subspaces",
    "title": "1  Fields and Spaces",
    "section": "Subspaces",
    "text": "Subspaces\nGiven a vector space \\mathcal{V} over a field \\mathbb{F}, a subspace \\mathcal{W} of \\mathcal{V} is a non-empty subset of \\mathcal{V} (\\mathcal{W} \\subseteq \\mathcal{V}) that follows the closure axioms.\nFor all u and v in the subspace \\mathcal{W} (\\forall u, v \\in \\mathcal{W}) and for all \\alpha in the field \\mathbb{F} (\\forall \\alpha \\in \\mathbb{F}), we have\n\nu + v \\in \\mathcal{W},\n\n\n\\alpha \\cdot v \\in \\mathcal{W}.\n\n\nProperties of subspace\nLet \\mathcal{U} and \\mathcal{V} be the subspaces of a vector space over \\mathbb{F}.\n\n0 \\in \\mathcal{W}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the subspace \\mathcal{W} cannot be empty, there is at least an element v \\in \\mathcal{W}.\nSince 1 \\in \\mathbb{F} and 1_{I} \\in \\mathbb{F},\n\n  1_{I} \\cdot v = v_{I} \\in \\mathcal{W},\n  \naccording to the axiom of the closure under scalar multiplication.\nAccording to the axiom of the closure under vector addition,\n\n  v + v_{I} = 0 \\in \\mathcal{W}.\n  \nThus, “zero” vector must be in \\mathcal{W}.\n\n\n\n\\mathcal{W} is also a vector space.\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy virtue of the closure axioms, all axioms of the vector space are obeyed by \\mathcal{W}.\nSince all elements in \\mathcal{W} are also in the vector space \\mathcal{V}, the axioms of\n\nclosure\ncommutativity\nassociativity\ndistributive property\n1 \\cdot v = v\n\nin \\mathcal{W} follow directly from those in \\mathcal{V}\nThe existence of “zero” vector and addictive inverse are proved in the proof above.\n\n\n\nThe subspace\n\n  \\mathcal{W} + \\mathcal{U} = \\left\\{\n      w + u \\mid w \\in \\mathcal{W}, u \\in \\mathcal{U}\n  \\right\\}\n  \nis also a subspace.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince 0 \\in \\mathcal{W} and 0 \\in \\mathcal{U}, then 0 \\in \\mathcal{W} + \\mathcal{U}. Thus, \\mathcal{W} + \\mathcal{U} is non-empty.\nSuppose a, b \\in \\mathcal{W} + \\mathcal{U}, then there exists elements a_{1} \\in \\mathcal{W}, a_{2} \\in \\mathcal{U} such that a = a_{1} + a_{2} and b_{1} \\in \\mathcal{W}, b_{2} \\in \\mathcal{U} such that b = b_{1} + b_{2}.\nBecause of closure under addition,\n\n  a_{1} + b_{1} \\in \\mathcal{W},\n  \n\n  a_{2} + b_{2} \\in \\mathcal{U}.\n  \nThus, according to the definition of the set addition\n\n  a + b = (a_{1} + b_{1}) + (a_{2} + b_{2}) \\in \\mathcal{W} + \\mathcal{U}.\n  \nAgain, suppose x \\in \\mathcal{W} + \\mathcal{U}, then there exists elements x_{1} \\in \\mathcal{W}, x_{2} \\in \\mathcal{U} such that x = x_{1} + x_{2}.\nBecause of closure under scalar multiplication,\n\n  \\alpha \\cdot x_{1} \\in \\mathcal{W} \\quad \\forall \\alpha \\in \\mathbb{F},\n  \n\n  \\alpha \\cdot x_{2} \\in \\mathcal{U} \\quad \\forall \\alpha \\in \\mathbb{F}.\n  \nThus, according to the definition of the set addition\n\n  \\alpha \\cdot x = \\alpha \\cdot x_{1} + \\alpha \\cdot x_{2} \\in \\mathcal{W} + \\mathcal{U} \\quad \\forall \\alpha \\in \\mathbb{F}.\n  \nThus, \\mathcal{W} + \\mathcal{U} is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n\n\n\nThe subspace\n\n  \\mathcal{W} \\cap \\mathcal{U} = \\left\\{\n      v \\mid v \\in \\mathcal{W}, v \\in \\mathcal{U}\n  \\right\\}\n  \nis also a subspace.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince 0 \\in \\mathcal{W} and 0 \\in \\mathcal{U}, then 0 \\in \\mathcal{W} \\cap \\mathcal{U}. Thus, \\mathcal{W} \\cap \\mathcal{U} is non-empty.\nSuppose a, b \\in \\mathcal{W} \\cap \\mathcal{U}, then because of the closure axiom of the subspaces\n\n  a + b \\in \\mathcal{W},\n  \\\\\n  a + b \\in \\mathcal{U}.\n  \nThus, by definition,\n\n  a + b \\in \\mathcal{W} \\cap \\mathcal{U}.\n  \nAgain, suppose x \\in \\mathcal{W} \\cap \\mathcal{U}, then because of the closure axiom of the subspaces,\n\n  \\alpha \\cdot x \\in \\mathcal{W}, \\forall \\alpha \\in \\mathbb{F},\n  \\\\\n  \\alpha \\cdot x \\in \\mathcal{U}, \\forall \\alpha \\in \\mathbb{F}.\n  \nThus, by definition, \\alpha \\cdot x \\in \\mathcal{W} \\cap \\mathcal{U}, \\forall \\alpha \\in \\mathbb{F}.\nThus, \\mathcal{W} \\cap \\mathcal{U} is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n\n\n\nThe subspace\n\n  \\mathcal{W} \\cup \\mathcal{U} = \\left\\{\n      v \\mid v \\in \\mathcal{W} \\mathop{\\text{or}} v \\in \\mathcal{U}\n  \\right\\}\n  \nis a subspace if and only if \\mathcal{W} \\subseteq \\mathcal{U} or \\mathcal{U} \\subseteq \\mathcal{W}\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst we prove if \\mathcal{U} \\subseteq \\mathcal{W}, then \\mathcal{U} \\cup \\mathcal{W} is a subspace.\nSince \\mathcal{U} \\subseteq \\mathcal{W},\n\n  \\mathcal{U} \\cup \\mathcal{W} = \\mathcal{W}\n  \nand thus is a subspace.\nThe same argument can be made for \\mathcal{W} \\subseteq \\mathcal{U}.\nThen we prove if \\mathcal{U} \\cup \\mathcal{W} is a subspace, then one subspace is contained in the other by contradiction.\nSuppose \\mathcal{U} \\cup \\mathcal{W} is a subspace, but \\mathcal{U} \\nsubseteq \\mathcal{W} and \\mathcal{W} \\nsubseteq \\mathcal{U}. This means there exists at least two vectors u and w such that\n\n  u \\in \\mathcal{U}, u \\notin \\mathcal{W}\n  \n\n  w \\in \\mathcal{W}, w \\notin \\mathcal{U}.\n  \nSince u, w \\in \\mathcal{U} \\cup \\mathcal{W} and closure under addition property, the vector\n\n  v = u + w\n  \nis also in \\mathcal{U} \\cup \\mathcal{W}, which means it must also be in \\mathcal{U} or \\mathcal{W}.\nIf v \\in \\mathcal{U}, because there must exist an addictive inverse of u \\in \\mathcal{U}, then\n\n  \\begin{aligned}\n  v\n  & = u + w\n  \\\\\n  v + u_{I}\n  & = u + u_{I} + w\n  \\\\\n  w\n  & = v + u_{I}\n  \\\\\n  \\end{aligned}\n  \nwhich shows that w \\in \\mathcal{U} and contradicts to our assumption.\nThe same contradiction can also be found when v \\in \\mathcal{W}."
  },
  {
    "objectID": "Linear Algebra/01_Fields_and_Spaces.html#example-subspaces-of-2-dimensional-real-value-column-vectors",
    "href": "Linear Algebra/01_Fields_and_Spaces.html#example-subspaces-of-2-dimensional-real-value-column-vectors",
    "title": "1  Fields and Spaces",
    "section": "Example: subspaces of 2-dimensional real-value column vectors",
    "text": "Example: subspaces of 2-dimensional real-value column vectors\nThe 2-dimensional real-value column vector space \\mathbb{R}^{2} has 3 types of subspaces\n\nThe subspace of the zero vector only,\n\n\\mathcal{W} = \\left\\{ 0 \\right\\}.\n\nThe subspace of the vector space itself,\n\n\\mathcal{W} = \\mathbb{R}^{2}.\n\nAny “line” that goes through zero vector"
  },
  {
    "objectID": "Linear Algebra/02_Vectors_and_Matrices.html#definitions",
    "href": "Linear Algebra/02_Vectors_and_Matrices.html#definitions",
    "title": "2  Vectors and Matrices",
    "section": "Definitions",
    "text": "Definitions\n\nVectors\nLet \\mathbb{F} be a field. The vector vector space \\mathbb{F}^{n} is defined as the set of all tuples (ordered lists) that have n field elements of \\mathbb{F}\n\n\\mathbb{F}^{n} =\n\\left\\{\n\\begin{bmatrix}\n    x_{1} \\\\\n    \\vdots \\\\\n    x_{n} \\\\\n\\end{bmatrix},\nx_{i} \\in \\mathbb{F}\n\\right\\},\n\nwhere each element member is called a n dimensional column vector.\n\nVector addition: for \\mathbf{x}, \\mathbf{y} \\in \\mathbb{F}^{n}\n\n  \\mathbf{x} + \\mathbf{y} =\n  \\begin{bmatrix}\n      x_{1} + y_{1} \\\\\n      \\vdots \\\\\n      x_{n} + y_{n} \\\\\n  \\end{bmatrix}.\n  \nThat is, the vector addition of the vector space \\mathbb{F}^{n} is defined as the element-wise field addition between two vectors.\nScalar multiplication: for \\alpha \\in \\mathbb{F} and \\mathbf{x} \\in \\mathbb{F}^{n}\n\n  \\alpha \\cdot \\mathbf{x} =\n  \\begin{bmatrix}\n      \\alpha \\cdot x_{1} \\\\\n      \\vdots \\\\\n      \\alpha \\cdot x_{n} \\\\\n  \\end{bmatrix}.\n  \nThat is, the scalar multiplication of the vector space \\mathbb{F}^{n} is defined as the element-wise field multiplication between the field element and the vector.\n“zero” vector and addictive inverse: for \\mathbf{x} \\in \\mathbb{F}\n\n  0 =\n  \\begin{bmatrix}\n      0 \\\\\n      \\vdots \\\\\n      0 \\\\\n  \\end{bmatrix}\n  \\quad\n  \\mathbf{x}_{I} =\n  \\begin{bmatrix}\n      (x_{1})_{I} \\\\\n      \\vdots \\\\\n      (x_{n})_{I} \\\\\n  \\end{bmatrix}.\n  \n\n\n\nMatrices\nLet \\mathbb{F} be a field. The matrix vector space \\mathbb{F}^{m \\times n} is defined as the set of all tables that have m rows and n columns of field elements of \\mathbb{F}\n\n\\mathbb{F}^{m \\times n} =\n\\left\\{\n\\begin{bmatrix}\n    x_{1, 1}, & \\dots & x_{1, n} \\\\\n    \\vdots, & \\dots & \\vdots \\\\\n    x_{m, 1}, & \\dots & x_{m, n} \\\\\n\\end{bmatrix},\nx_{i, j} \\in \\mathbb{F}\n\\right\\},\n\nwhere each element member is called m \\times n dimensional matrix.\nThe definition of vector addition, scalar multiplication, zero, and addictive inverse are the same as n dimensional column vectors."
  },
  {
    "objectID": "Linear Algebra/02_Vectors_and_Matrices.html#multiplications",
    "href": "Linear Algebra/02_Vectors_and_Matrices.html#multiplications",
    "title": "2  Vectors and Matrices",
    "section": "Multiplications",
    "text": "Multiplications\n\nMatrix-vector multiplication\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}, the matrix-vector multiplication is a function that maps from vector space \\mathbf{x} \\in \\mathbb{F}^{n} to \\mathbf{y} \\in \\mathbb{F}^{m}\n\n\\mathbf{y} = \\mathbf{A} \\mathbf{x} =\n\\begin{bmatrix}\n    y_{1} = a_{1, 1} \\cdot x_{1} + \\dots a_{1, n} \\cdot x_{n} \\\\\n    \\vdots \\\\\n    y_{m} = a_{m, 1} \\cdot x_{1} + \\dots a_{m, n} \\cdot x_{n} \\\\\n\\end{bmatrix}.\n\nIf we view \\mathbf{A} as n columns of \\mathbb{F}^{m} vectors,\n\n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{a}_{1} & \\dots & \\mathbf{a}_{n} \\\\\n\\end{bmatrix}\n\nthe vector \\mathbf{y} can be interpreted as the linear combinations of columns of \\mathbf{A} with elements of \\mathbf{x} as coefficients\n\n\\mathbf{y} = \\sum_{i=1}^{n} x_{i} \\mathbf{a}_{i}.\n\n\n\nMatrix multiplication\nGiven two matrices \\mathbf{A} \\in \\mathbb{F}^{m \\times n} and \\mathbf{B} \\in \\mathbb{F}^{n \\times r}, the matrix multiplication is a function that applies vector matrix multiplication on the matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n} and the vector \\mathbf{b}_{i} \\in \\mathbb{F}^{n} (ith column of \\mathbf{B})\n\n\\mathbf{C} = \\mathbf{A} \\mathbf{B} =\n\\begin{bmatrix}\n\\mathbf{c}_{1} = \\mathbf{A} \\mathbf{b}_{1} & \\dots & \\mathbf{c}_{r} = \\mathbf{A} \\mathbf{b}_{r} \\\\\n\\end{bmatrix},\n\nwhich results in a vector \\mathbf{c}_{i} \\in \\mathbb{F}^{m} as the ith column of the \\mathbf{C}.\n\nThe column i of \\mathbf{C} is a linear combination of columns of \\mathbf{A} using the elements of the column i of \\mathbf{B} as coefficients.\nThe row i of \\mathbf{C} is a linear combination of rows of \\mathbf{B} using the elements of row i of \\mathbf{A} as coefficients."
  },
  {
    "objectID": "Linear Algebra/02_Vectors_and_Matrices.html#well-known-subspaces-for-matrix",
    "href": "Linear Algebra/02_Vectors_and_Matrices.html#well-known-subspaces-for-matrix",
    "title": "2  Vectors and Matrices",
    "section": "Well-known Subspaces for Matrix",
    "text": "Well-known Subspaces for Matrix\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}, the matrix-vector multiplication\n\n\\mathbf{y} = \\mathbf{A} \\mathbf{x} =\n\\begin{bmatrix}\n    y_{1} = a_{1, 1} \\cdot x_{1} + \\dots a_{1, n} \\cdot x_{n} \\\\\n    \\vdots \\\\\n    y_{m} = a_{m, 1} \\cdot x_{1} + \\dots a_{m, n} \\cdot x_{n} \\\\\n\\end{bmatrix}\n\nis a function that maps from vector space x \\in \\mathbb{F}^{n} to y \\in \\mathbb{F}^{m}.\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}, we can define several subspaces of \\mathbb{F}^{n} or \\mathbb{F}^{m} using the matrix-vector multiplication of matrix \\mathbf{A}.\n\nNull space\nThe null space of the matrix \\mathbf{A} is the set\n\nN (\\mathbf{A}) = \\left\\{\n    \\mathbf{x} \\in \\mathbb{F}^{n} \\mid \\mathbf{A} \\mathbf{x} = \\mathbf{0} \\in \\mathbb{F}^{m}\n\\right\\},\n\nwhich is the set of the vectors in \\mathbb{F}^{n} that is mapped to 0 \\in \\mathbb{F}^{m} by matrix \\mathbf{A}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathbf{A} 0 = 0 \\in \\mathbb{F}^{n}, \\mathbf{x} = 0 \\in N (A). Thus N (A) is not empty.\nConsider \\mathbf{x}_{1}, \\mathbf{x}_{2} \\in N (A). According to the definition of the N (A), \\mathbf{A} \\mathbf{x}_{1} = 0 and \\mathbf{A} \\mathbf{x}_{2} = 0.\nThen,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x}_{1} + \\mathbf{A} \\mathbf{x}_{2}\n& = 0\n\\\\\n\\mathbf{A} (\\mathbf{x}_{1} + \\mathbf{x}_{2})\n& = 0.\n\\end{aligned}\n\nThus N (\\mathbf{A}) is closed under vector addition\n\n\\mathbf{x}_{1} + \\mathbf{x}_{2} \\in N (\\mathbf{A}).\n\nAlso, for all \\alpha \\in \\mathbb{F},\n\n\\begin{aligned}\n\\alpha \\cdot \\mathbf{A} \\mathbf{x}_{1}\n& = \\alpha \\cdot 0\n\\\\\n\\mathbf{A} (\\alpha \\cdot \\mathbf{x}_{1})\n& = 0.\n\\end{aligned}\n\nThus N (\\mathbf{A}) is closed under scalar multiplication\n\n\\alpha \\cdot \\mathbf{x}_{1} \\in N (\\mathbf{A}).\n\nThus, N (\\mathbf{A}) is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n\n\n\n\n\nRange (image) space\nThe range (image) space of the matrix \\mathbf{A} is the set\n\nR (\\mathbf{A}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\mathbf{A} \\mathbf{x}, \\forall \\mathbf{x} \\in \\mathbb{F}^{n}\n\\right\\},\n\nwhich is the set of vectors in \\mathbb{F}^{m} that can be mapped from \\mathbb{F}^{n} by matrix \\mathbf{A}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathbf{x} = 0 \\in \\mathbb{F}^{n}, \\mathbf{y} = 0 \\in R (A). Thus R (A) is not empty.\nConsider \\mathbf{y}_{1}, \\mathbf{y}_{2} \\in R (A). According to the definition of the R (A), there exists an \\mathbf{x}_{1} \\in \\mathbb{F}^{m} for \\mathbf{y}_{1} and an \\mathbf{x}_{2} \\in \\mathbb{F}^{m} for \\mathbf{y}_{2}.\nThen,\n\n\\begin{aligned}\n\\mathbf{y}_{1} + \\mathbf{y}_{2}\n& = \\mathbf{A} \\mathbf{x}_{1} + \\mathbf{A} \\mathbf{x}_{2}\n\\\\\n& = \\mathbf{A} (\\mathbf{x}_{1} + \\mathbf{x}_{2})\n\\end{aligned}\n\nSince by the closure under addition property,\n\n\\mathbf{x}_{1} + \\mathbf{x}_{2} \\in \\mathbb{F}^{m}\n\nand thus R (\\mathbf{A}) is closed under vector addition\n\n\\mathbf{y}_{1} + \\mathbf{y}_{2} \\in R (\\mathbf{A}).\n\nAlso, for all \\alpha \\in \\mathbb{F},\n\n\\begin{aligned}\n\\alpha \\cdot \\mathbf{y}_{1}\n& = \\alpha \\cdot \\mathbf{A} \\mathbf{x}_{1}\n\\\\\n& = \\mathbf{A} (\\alpha \\cdot \\mathbf{y}_{1})\n\\end{aligned}\n\nAgain, since by the closure under scalar multiplication property,\n\n\\alpha \\cdot \\mathbf{x}_{1} \\in \\mathbb{F}^{m}, \\forall \\alpha \\in \\mathbb{F},\n\nand thus R (\\mathbf{A}) is closed under scalar multiplication\n\n\\alpha \\cdot \\mathbf{y}_{1} \\in R (\\mathbf{A}).\n\nThus, R (\\mathbf{A}) is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n\n\n\n\n\nColumn and row space\nThe column space of the matrix \\mathbf{A} is the set of linear combinations of columns of \\mathbf{A}\n\nC (\\mathbf{A}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\sum_{i=1}^{n} \\alpha_{i} \\cdot \\mathbf{a}_{*, i}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\},\n\nand the row space of the matrix \\mathbf{A} is the set of linear combinations of rows of \\mathbf{A}\n\nC (\\mathbf{A}^{T}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\sum_{i=1}^{n} \\alpha_{i} \\cdot \\mathbf{a}_{i, *}^{T}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\},\n\nwhich is the same as the column space of \\mathbf{A}^{T}.\nBy the definition of matrix-vector multiplication, the column space of \\mathbf{A} is the same as the range space of \\mathbf{A}\n\nC (\\mathbf{A}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\sum_{i=1}^{n} \\alpha_{i} \\cdot \\mathbf{a}_{*, i}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\} = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\mathbf{A} \\mathbf{x}, \\forall \\mathbf{x} \\in \\mathbb{F}^{n}\n\\right\\} = R (\\mathbf{A})."
  },
  {
    "objectID": "Linear Algebra/03_Span_and_Linear_Independence.html#span",
    "href": "Linear Algebra/03_Span_and_Linear_Independence.html#span",
    "title": "3  Span and Linear Independence",
    "section": "Span",
    "text": "Span\nGiven a vector space \\mathcal{V} over a field \\mathbb{F}, the span of a set of vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} is the set of all possible linear combinations of v_{1}, \\dots, v_{m}\n\n\\text{span} (v_{1}, \\dots, v_{n}) = \\left\\{\n    \\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\}.\n\nIf \\mathcal{V} is \\mathbb{F}^{m}, the set vectors v_{1}, \\dots, v_{n} can be viewed as the columns of the matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}. Then\n\n\\text{span} (v_{1}, \\dots, v_{n}) = R (\\mathbf{A}).\n\nSince we have proved R (\\mathbf{A}) is a subspace of \\mathcal{V}, the span of a set of vectors is a subspace of \\mathcal{V}."
  },
  {
    "objectID": "Linear Algebra/03_Span_and_Linear_Independence.html#spanning-set",
    "href": "Linear Algebra/03_Span_and_Linear_Independence.html#spanning-set",
    "title": "3  Span and Linear Independence",
    "section": "Spanning set",
    "text": "Spanning set\nFor a set of vectors \\mathcal{S} = v_{1}, \\dots, v_{n}, if the subspace \\mathcal{W} is the span of \\mathcal{S}\n\n\\mathcal{W} = \\text{span} (\\mathcal{S}),\n\nthen the set of vectors \\mathcal{S} is the spanning set of the subspace \\mathcal{W}.\n\nProperties of the spanning set\nConsider a set of vectors \\mathcal{A} = \\left\\{ v_{1}, \\dots, v_{n} \\right\\} and a subspace \\mathcal{W}.\n\nIf \\mathcal{A} is a spanning set of \\mathcal{W}, the new set\n\n  \\mathcal{A} \\cup \\left\\{\n      u_{1}, \\dots, u_{n}\n  \\right\\}\n  \nis still a spanning set of \\mathcal{W} for arbitrary vectors u_{1}, \\dots, u_{n} \\in \\mathcal{W}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\\mathcal{A} is a spanning set of \\mathcal{W} if and only if\n\nall vectors in \\mathcal{W} are linear combinations of v_{1}, \\dots, v_{n} and\nv_{1}, \\dots, v_{n} \\in \\mathcal{W}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince this statement has “if and only if”, we first prove in the forward direction.\nIf v_{1}, \\dots, v_{n} is a spanning set of \\mathcal{W}, then by the definition of spanning set, the set of all linear combinations of v_{1}, \\dots, v_{n} is the subspace \\mathcal{W}.\nThus, all vectors in the subspace \\mathcal{W} are the linear combinations of v_{1}, \\dots, v_{n}.\nSince every v_{i}, i = 1, \\dots, n is a linear combination of itself, then v_{1}, \\dots, v_{n} are also in \\mathcal{W}.\nThen we prove in the backward direction.\nSince all vectors in \\mathcal{W} are linear combinations of v_{1}, \\dots, v_{n}, by the definition of span,\n\n  \\mathcal{W} \\subseteq \\text{span} (v_{1}, \\dots, v_{n}).\n  \nSince v_{1}, \\dots, v_{n} \\in \\mathcal{W} and the closure property of the subspace, all linear combinations of v_{1}, \\dots, v_{n} are also in \\mathcal{W}, which means that\n\n  \\text{span} (v_{1}, \\dots, v_{n}) \\subseteq \\mathcal{W}.\n  \nThus,\n\n  \\mathcal{W} = \\text{span} (v_{1}, \\dots, v_{n}).\n  \n\n\n\nIf vectors in set \\mathcal{B} are linear combinations of the vectors in \\mathcal{A}, then\n\n  \\text{span} (\\mathcal{B}) \\subseteq \\text{span} (\\mathcal{A}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction. Suppose that the vectors in set \\mathcal{B} are linear combinations of the vectors in \\mathcal{A}, but there exists an vector v \\in \\text{span} (\\mathcal{B}) but v \\notin \\text{span} (\\mathcal{A}).\nSuppose \\mathcal{A} = \\{ a_{1}, \\dots, a_{n} \\}. Since v \\in \\mathcal{B} is a linear combinations of vectors in \\mathcal{A}, there exists a set of coefficients \\alpha_{1}, \\dots, \\alpha_{n} such that\n\n  v = \\sum_{i = 1}^{n} \\alpha_{i} a_{i}.\n  \nwhich by definition of span means v \\in \\text{span} (\\mathbf{A}).\nHowever, we assume that v \\notin \\text{span} (\\mathbf{A}), which raises a contradiction.\n\n\n\nIf \\text{span} (\\mathcal{A}) = \\mathcal{W} and \\text{span} (\\mathcal{B}) = \\mathcal{U}, then\n\n  \\text{span} (\\mathcal{A} \\cup \\mathcal{B}) = \\mathcal{W} + \\mathcal{U},\n  \nwhere\n\n  \\mathcal{W} + \\mathcal{U} = \\left\\{\n      w + u \\mid w \\in \\mathcal{W}, u \\in \\mathcal{U}\n  \\right\\}.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\mathcal{A} = \\{ a_{1}, \\dots, a_{n} \\}, \\mathcal{B} = \\{ b_{1}, \\dots, b_{m} \\}, then\n\n  \\text{span} (\\mathcal{A} \\cup \\mathcal{B}) = \\left\\{\n      \\sum_{i=1}^{n} \\alpha_{i} a_{i} + \\sum_{i=1}^{n} \\beta_{i} b_{i}, \\forall \\alpha_{i}, \\beta_{i} \\in \\mathbb{F}\n  \\right\\}.\n  \nNote that\n\n  w \\in \\mathcal{W} = \\sum_{i=1}^{n} \\alpha_{i} a_{i}, \\forall \\alpha_{i} \\in \\mathbb{F},\n  \n\n  u \\in \\mathcal{U} = \\sum_{i=1}^{n} \\beta_{i} b_{i}, \\forall \\beta_{i} \\in \\mathbb{F}.\n  \nThus,\n\n  \\text{span} (\\mathcal{A} \\cup \\mathcal{B}) = \\left\\{\n      w + u \\mid w \\in \\mathcal{W}, u \\in \\mathcal{U}\n  \\right\\} = \\mathcal{W} + \\mathcal{U}."
  },
  {
    "objectID": "Linear Algebra/03_Span_and_Linear_Independence.html#linear-independence",
    "href": "Linear Algebra/03_Span_and_Linear_Independence.html#linear-independence",
    "title": "3  Span and Linear Independence",
    "section": "Linear independence",
    "text": "Linear independence\nGiven a vector space \\mathcal{V} over a field \\mathbb{F}, a set of non-zero vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} is linearly independent when\n\n\\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i} = 0 \\iff \\alpha_{i} = 0, i = 1, \\dots, n.\n\n\nProperties of linear independence\n\nGiven a matrix \\mathbf{A} whose columns are vectors \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\in \\mathbb{F}^{m} the set of vectors \\mathcal{S} = \\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\} is linearly independent when the null space of \\mathbf{A} only contains \\mathbf{0} \\in \\mathbb{F}^{n}.\n\n  N (\\mathbf{A}) = \\left\\{\n      \\mathbf{0}\n  \\right\\}\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition, N (\\mathbf{A}) = \\{ \\mathbf{0} \\} says that the only set of \\alpha_{1}, \\dots, \\alpha_{n} satisfying\n\n  \\sum_{i = 1}^{n} \\alpha_{i} \\mathbf{A}_{*, i} = 0\n  \nis \\alpha_{1}, \\dots, \\alpha_{n} = 0, which is equivalent to saying the columns of \\mathbf{A} are a linearly independent set.\n\n\n\nIf vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} are linearly independent and the subspace \\mathcal{W} = \\text{span} (v_{1}, \\dots, v_{n}), the set of field elements \\{ \\alpha_{1}, \\dots, \\alpha_{n} \\} that is paired with the set of vectors \\{ v_{1}, \\dots, v_{n} \\} to represent any vector u \\in \\mathcal{W}: u = \\sum_{i=1}^{n} \\alpha_{i} v_{i} is unique.\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof by contradiction. Suppose there is another set of field elements that can be paired with v_{1}, \\dots, v_{n} to represent u\n\n  \\left\\{\n      \\beta_{1}, \\dots, \\beta_{n} \\mid u = \\sum_{i=1}^{n} \\beta_{i} \\cdot v_{i}\n  \\right\\}.\n  \nThen,\n\n  \\begin{aligned}\n  \\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i}\n  & = \\sum_{i=1}^{n} \\beta_{i} \\cdot v_{i}\n  \\\\\n  \\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i} - \\sum_{i=1}^{n} \\beta_{i} \\cdot v_{i}\n  & = 0\n  \\\\\n  \\sum_{i=1}^{n} (\\alpha_{i} - \\beta_{i}) \\cdot v_{i}\n  & = 0\n  \\\\\n  \\alpha_{i} - \\beta_{i}\n  & = 0\n  \\\\\n  \\alpha_{i}\n  & = \\beta_{i},\n  \\end{aligned}\n  \nwhich contradicts to the fact \\alpha_{i} and \\beta_{i} should be different."
  },
  {
    "objectID": "Linear Algebra/03_Span_and_Linear_Independence.html#linear-dependence",
    "href": "Linear Algebra/03_Span_and_Linear_Independence.html#linear-dependence",
    "title": "3  Span and Linear Independence",
    "section": "Linear dependence",
    "text": "Linear dependence\nGiven a vector space \\mathcal{V} over a field \\mathbb{F}, a set of non-zero vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} is linear dependent when there exists an \\alpha_{i} \\neq 0 such that\n\n\\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i} = 0.\n\n(existence-of-linear-combination)=\nIf a set of vectors \\{ v_{1}, \\dots, v_{n} \\} is linearly dependent, there exists an index j (1 \\leq j \\leq n) such that v_{j} is a linear combination of the rest of the vectors:\n\nv_{j} = \\sum_{i=1, i \\neq j}^{n} \\alpha_{i} \\cdot v_{i}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the set \\{ v_{1}, \\dots, v_{n} \\} is linearly dependent, there exists an \\beta_{j} \\neq 0 (1 \\leq j \\leq n) such that\n\n\\beta_{1} \\cdot v_{1} + \\dots \\beta_{j} \\cdot v_{j} + \\dots \\beta_{n} \\cdot v_{n} = 0.\n\nThus,\n\n\\begin{aligned}\n\\beta_{j} \\cdot v_{j}\n& = - \\sum_{i=1, i \\neq j} \\beta_{i} \\cdot v_{i}\n\\\\\nv_{j}\n& = - \\beta_{j}^{-1} \\cdot \\sum_{i=1, i \\neq j} \\beta_{i} \\cdot v_{i}\n\\\\\nv_{j}\n& = \\sum_{i=1, i \\neq j} - \\beta_{j}^{-1} \\beta_{i} \\cdot v_{i}\n\\\\\nv_{j}\n& = \\sum_{i=1, i \\neq j} \\alpha_{i} \\cdot v_{i}\n& [\\text{rewrite } - \\beta_{j}^{-1} \\beta_{i} = \\alpha_{i}]\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/04_Basis_and_Dimension.html#basis",
    "href": "Linear Algebra/04_Basis_and_Dimension.html#basis",
    "title": "4  Basis and Dimension",
    "section": "Basis",
    "text": "Basis\nA set of vectors v_{1}, \\dots, v_{k} is a basis of a subspace \\mathcal{W} of a vector space \\mathcal{V} over a field \\mathbb{F}, if\n\n\\mathcal{W} = \\text{span} (v_{1}, \\dots, v_{k}), and\nv_{1}, \\dots, v_{k} are linearly independent.\n\n\nProperties of basis\n\nIf \\mathcal{B} = \\{ b_{1}, \\dots, b_{n} \\} is a basis of the subspace \\mathcal{W}, \\mathcal{B} is a minimal spanning set for \\mathcal{W}. That is, there is no vector in \\mathcal{B} that can be removed such that \\mathcal{B} is still a spanning set.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove by contradiction. Assume \\mathcal{B} is a basis of the subspace \\mathcal{W} but is not a minimal spanning set, which means that the vectors in \\mathcal{B} are linear independent and \\mathcal{B} contains at least one vector b such that \\hat{\\mathcal{B}} = \\mathcal{B} \\setminus \\{ b \\} is still a spanning set.\nTherefore, there exists a set of coefficients \\alpha_{1}, \\dots, \\alpha_{n - 1} such that b is a linear combination of the vectors in \\hat{\\mathcal{B}}\n\n  b = \\sum_{b_{i} \\in \\hat{\\mathcal{B}}} \\alpha_{i} b_{i}.\n  \nHowever, this means that the vectors in \\mathcal{B} are linear dependent, which violates our assumption.\n\n\n\nIf \\mathcal{B} = \\{ b_{1}, \\dots, b_{n} \\} is a basis of the subspace \\mathcal{W}, \\mathcal{B} is a maximal linearly independent subset of \\mathcal{W}. That is, there is no vector in \\mathcal{W} that can be added to \\mathcal{B} such that \\mathcal{B} is still a linearly independent set.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove by contradiction. Assume \\mathcal{B} is a basis of the subspace \\mathcal{W} but is not a maximal linearly independent set, which means that \\mathcal{B} is a spanning set of \\mathcal{W} and there exists a vector b \\in \\mathcal{W} that can be added to \\mathcal{B} such that \\hat{\\mathcal{B}} = \\mathcal{B} \\cup \\{ b \\} is still a linearly independent set.\nHowever, since \\mathcal{B} is a spanning set of \\mathcal{W}, there exists a set of coefficients \\alpha_{1}, \\dots, \\alpha_{n} such that b is a linear combination of the vectors in \\mathcal{B}\n\n  b = \\sum_{b_{i} \\in \\mathcal{B}} \\alpha_{i} b_{i}.\n  \nwhich means that the vectors in \\hat{\\mathcal{B}} are linear dependent, which violates our assumption.\n\n\n\nExistence of basis\nA subspace may NOT have a basis e.g. \\mathcal{W} = \\{ 0 \\} has no linearly independent vector, but a subspace must have a basis if it has a finite spanning set. That is, every finite spanning set of a subspace contains a basis.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose the subspace \\mathcal{W} has a finite spanning set of\n\n  \\mathcal{A}_{0} = \\{ v_{i}, \\dots, v_{n} \\}.\n  \nIf the spanning set is linearly independent, the set \\{ v_{i}, \\dots, v_{n} \\} is a basis of \\mathcal{W}.\nIf the spanning set is not linearly independent, then there exists j (1 \\leq j \\leq n) such that\n\n  v_{j} = \\sum_{i=1, i \\neq j}^{n} \\alpha_{i} v_{i},\n  \nand the set\n\n  \\mathcal{A}_{1} = \\{ v_{1}, \\dots, v_{n} \\} \\setminus \\{ v_{j} \\}\n  \nis still a spanning set.\nWe can continue removing such v_{j} if the resulting set \\mathcal{A}_{i} is not linearly independent.\nSince the resulting set \\mathcal{A}_{i} is always a spanning set of \\mathcal{W}, we will eventually get to the step i where the \\mathcal{A}_{i} is linearly dependent and \\mathcal{A}_{i} will be the basis for the subspace \\mathcal{W}.\n\n\n\nCardinality of basis\nThe numbers of elements of all bases of a given subspace are the same.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\mathcal{A} is a basis of a subspace \\mathcal{S} and \\mathcal{B} is another basis of \\mathcal{S}. Thus, \\mathcal{A} and \\mathcal{B} are both linearly independent and spanning sets.\nSince \\mathcal{A} is a spanning set and \\mathcal{B} is linearly independent,\n\n  \\lvert \\mathcal{A} \\rvert \\geq \\lvert \\mathcal{B} \\rvert.\n  \nSince \\mathcal{B} is a spanning set and \\mathcal{A} is linearly independent,\n\n  \\lvert \\mathcal{A} \\rvert \\leq \\lvert \\mathcal{B} \\rvert.\n  \nThus,\n\n  \\lvert \\mathcal{A} \\rvert = \\lvert \\mathcal{B} \\rvert.\n  \n\n\n\nExtension of a basis\nIf the subspace \\mathcal{U} is a subset of a finite dimensional subspace \\mathcal{V} and \\{ b_{1}, \\dots, b_{k} \\} is a basis of \\mathcal{U}, then there exists an extension of \\{ b_{1}, \\dots, b_{k} \\}\n\n  \\{ b_{1}, \\dots, b_{k}, b_{k + 1}, \\dots, b_{n} \\}\n  \nthat is a basis for \\mathcal{V}\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/04_Basis_and_Dimension.html#dimension",
    "href": "Linear Algebra/04_Basis_and_Dimension.html#dimension",
    "title": "4  Basis and Dimension",
    "section": "Dimension",
    "text": "Dimension\nThe dimension of a subspace is the common cardinality of its all bases.\n\nProperties of dimension\n\nIf a subspace \\mathcal{U} is a subset of a finite dimensional subspace \\mathcal{V}, then\n\n  \\text{dim} (\\mathcal{U}) \\leq \\text{dim} (\\mathcal{V}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose there is a basis\n\n  \\mathcal{A} = \\{ b_{1}, \\dots, b_{n} \\}\n  \nfor \\mathcal{U}.\nSince \\mathcal{U} \\subseteq \\mathcal{V}, there are 2 cases.\nIn the case where \\mathcal{U} = \\mathcal{V}, \\mathcal{A} is also a basis of \\mathcal{V} and thus,\n\n  \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}).\n  \nIn the case where \\mathcal{U} \\subset \\mathcal{V}, there is at least 1 vector x \\in \\mathcal{V} such that\n\n  x \\notin \\text{span} (\\mathcal{A}).\n  \n\n\n\nIf a subspace \\mathcal{U} is a subset of a finite dimensional subspace \\mathcal{V} and \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}), then\n\n  \\mathcal{U} = \\mathcal{V}.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathcal{U} \\subset \\mathcal{V} and according to the property of extension of the basis, there exists a basis b_{\\mathcal{U}} for \\mathcal{U} and b_{\\mathcal{V}} for \\mathcal{V} such that\n\n  \\mathcal{B}_{\\mathcal{U}} \\subset \\mathcal{B}_{\\mathcal{V}}.\n  \nHowever, since \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}), it must follows that\n\n  \\mathcal{B}_{\\mathcal{U}} = \\mathcal{B}_{\\mathcal{V}}.\n  \nSince \\mathcal{U} and \\mathcal{V} shares the same basis,\n\n  \\mathcal{U} = \\mathcal{V}.\n  \n\n\n\nGiven \\mathcal{U} and \\mathcal{V} are subspaces, then\n\n  \\text{dim} (\\mathcal{U} + \\mathcal{V}) = \\text{dim} (\\mathcal{U}) + \\text{dim} (\\mathcal{V}) - \\text{dim} (\\mathcal{U} \\cap \\mathcal{V}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\text{dim} (\\mathcal{U} \\cap \\mathcal{V}) = k, \\text{dim} (\\mathcal{U}) = m, and \\text{dim} (\\mathcal{V}) = n.\nAssume there is a basis\n\n  \\mathcal{B} = \\{ b_{1}, \\dots, b_{k} \\}\n  \nfor \\mathcal{U} \\cap \\mathcal{V}.\nSince \\mathcal{U} \\cap \\mathcal{V} \\subseteq \\mathcal{U} and \\mathcal{U} \\cap \\mathcal{V} \\subseteq \\mathcal{V}, there exists a basis \\mathcal{A} for \\mathcal{U} and \\mathcal{C} for \\mathcal{V}\n\n  \\mathcal{A} = \\{ b_{1}, \\dots, b_{k}, a_{k + 1}, \\dots, a_{m} \\}\n  \n\n  \\mathcal{C} = \\{ b_{1}, \\dots, b_{k}, c_{k + 1}, \\dots, c_{n} \\}\n  \nas extensions of \\mathcal{B}, according to extension of basis property.\nConsider the set\n\n  \\mathcal{A} \\cup \\mathcal{C} = \\{ b_{1}, \\dots, b_{k}, a_{k + 1}, \\dots, a_{m}, c_{k + 1}, \\dots, c_{n} \\}\n  \nwe will prove that it is a basis of \\mathcal{U} + \\mathcal{V}.\nFirst, consider all x \\in \\mathcal{U} + \\mathcal{V}, which can be represented using the basis of \\mathcal{U} and the basis of \\mathcal{V}\n\n  \\begin{aligned}\n  x\n  & = \\sum_{i=1}^{k} \\alpha_{i} b_{i} + \\sum_{i=k+1}^{m} \\alpha_{i} a_{i} + \\sum_{i=1}^{k} \\beta_{i} b_{i} + \\sum_{i=k+1}^{m} \\beta_{i} c_{i}\n  \\\\\n  & = \\sum_{i=1}^{k} (\\alpha_{i} + \\beta_{i}) b_{i} + \\sum_{i=k+1}^{m} \\alpha_{i} a_{i} + \\beta_{i} c_{i}.\n  \\end{aligned}\n  \nThus, any vector x \\in \\mathcal{U} + \\mathcal{V} is a linear combination of vectors in \\mathcal{A} \\cup \\mathcal{C} and we have\n\n  \\mathcal{U} + \\mathcal{V} \\subseteq \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right).\n  \nConversely, all x \\in \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right) can be written as the linear combinations of vectors in \\mathcal{A} and \\mathcal{C}, and thus\n\n  \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right) \\subseteq \\mathcal{U} + \\mathcal{V}.\n  \nThus,\n\n  \\mathcal{U} + \\mathcal{V} = \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right).\n  \nNext, we will prove the vectors in \\mathcal{A} \\cup \\mathcal{C} are linearly independent by contradiction.\nConsider x \\in \\mathcal{U} \\cap \\mathcal{V}, which can be represented by the basis of \\mathcal{U} \\cap \\mathcal{V}, \\mathcal{U} and, \\mathcal{V} at the same time\n\n  \\begin{aligned}\n  x\n  & = \\sum_{i=1}^{k} \\alpha_{i} b_{i} + \\sum_{i=k+1}^{m} \\alpha_{i} a_{i}\n  \\\\\n  & = \\sum_{i=1}^{k} \\beta_{i} b_{i} + \\sum_{i=k+1}^{n} \\beta_{i} c_{i}\n  \\\\\n  & = \\sum_{i=1}^{k} \\epsilon_{i} b_{i}.\n  \\end{aligned}\n  \nSuppose x = 0, then because"
  },
  {
    "objectID": "Linear Algebra/04_Basis_and_Dimension.html#types-of-the-subspaces",
    "href": "Linear Algebra/04_Basis_and_Dimension.html#types-of-the-subspaces",
    "title": "4  Basis and Dimension",
    "section": "Types of the subspaces",
    "text": "Types of the subspaces\nWe can classify the types of the subspaces based on their dimensions. For example, in \\mathbb{R}^{n} there are n types of subspaces\n\n0 dimension subspace: \\{ 0 \\}.\n1 dimension subspaces.\n…\nn dimension subspace: \\mathbb{R}^{n} itself."
  },
  {
    "objectID": "Linear Algebra/05_Linear_Map_and_Rank.html#linear-map",
    "href": "Linear Algebra/05_Linear_Map_and_Rank.html#linear-map",
    "title": "5  Linear Map and Rank",
    "section": "Linear Map",
    "text": "Linear Map\nLet \\mathcal{U} and \\mathcal{V} be the vector spaces over the same filed \\mathbb{F}. A map T: \\mathcal{U} \\to \\mathcal{V} is linear if\n\nT (u + v) = T (u) + T (v) \\quad u, v \\in \\mathcal{U}, T (u), T (v) \\in \\mathcal{V}, and\nT (\\alpha \\cdot v) = \\alpha \\cdot T (v) \\quad \\alpha \\in \\mathbb{F}, v \\in \\mathcal{u}, T (v) \\in \\mathcal{V}.\n\n\nProperties of linear map\nLet \\mathcal{U} and \\mathcal{V} be two vector spaces over the same filed \\mathbb{F}, and suppose T is a linear map T: \\mathcal{U} \\to \\mathcal{V}.\n\nA linear map must satisfy T (0) = 0.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction. Suppose there exists a linear map such that\n\n  T (0) \\neq 0.\n  \nSuppose v = 0 and according to the definition of the linear map\n\n  \\begin{aligned}\n  T (\\alpha \\cdot v)\n  & = \\alpha \\cdot T (v), \\forall \\alpha \\in \\mathbb{F}\n  \\\\\n  T (0)\n  & = \\alpha \\cdot T (0), \\forall \\alpha \\in \\mathbb{F}.\n  \\end{aligned}\n  \nSince T (0) \\neq 0, we can divide both sides by T (0)\n\n  \\alpha = 1, \\forall \\alpha \\in \\mathbb{F},\n  \nwhich raises a contradiction.\n\n\n\nThere exists a matrix representation of T.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\text{dim} (\\mathcal{U}) = n and \\text{dim} (\\mathcal{V}) = m.\nLet \\{ u_{1}, \\dots, u_{n} \\} be a basis of \\mathcal{U}, and \\{ v_{1}, \\dots, v_{m} \\} be a basis of \\mathcal{V}.\nSuppose any vector x \\in \\mathcal{U} satisfies that\n\n  x = \\sum_{i=1}^{n} \\alpha_{i} \\cdot u_{i},\n  \nand the mapped vector T (x) \\in \\mathcal{V} satisfies that\n\n  T (x) = \\sum_{j=1}^{m} \\beta_{j} \\cdot v_{j}.\n  \nSince T (u_{i}) \\in \\mathcal{V}, we have\n\n  T (u_{i}) = \\sum_{j=1}^{m} c_{i, j} \\cdot v_{j}, \\forall i = 1, \\dots, n.\n  \nBy the definition of the linear map,\n\n  \\begin{aligned}\n  T (x)\n  & = T \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} \\cdot u_{i}\n  \\right)\n  \\\\\n  & = \\sum_{i=1}^{n} \\alpha_{i} \\cdot T (u_{i}).\n  \\\\\n  & = \\sum_{i=1}^{n} \\alpha_{i} \\sum_{j=1}^{m} c_{i, j} \\cdot v_{j}\n  \\\\\n  & = \\sum_{j=1}^{m} \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} c_{i, j}\n  \\right) \\cdot v_{j}.\n  \\\\\n  \\end{aligned}\n  \nBecause of the unique representation property,\n\n  \\begin{aligned}\n  T(x) = \\sum_{j=1}^{m} \\beta_{j} \\cdot v_{j}\n  & = \\sum_{j=1}^{m} \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} c_{i, j}\n  \\right) \\cdot v_{j}\n  \\\\\n  \\beta_{j}\n  & = \\sum_{i=1}^{n} \\alpha_{i} c_{i, j} \\quad \\forall j = 1, \\dots, m,\n  \\end{aligned}\n  \nwhich can be represented in the matrix form\n\n  \\begin{bmatrix}\n  \\beta_{1} \\\\\n  \\vdots \\\\\n  \\beta_{m} \\\\\n  \\end{bmatrix}\n  =\n  \\begin{bmatrix}\n  c_{1, 1} & \\dots & c_{n, 1}  \\\\\n  \\vdots & \\dots & \\vdots \\\\\n  c_{1, m} & \\dots & c_{n, m} \\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n  \\alpha_{1} \\\\\n  \\vdots \\\\\n  \\alpha_{n} \\\\\n  \\end{bmatrix}.\n  \nHence, given any vector x \\in \\mathcal{U} that has basis coefficients in a given basis \\{ u_{1}, \\dots, u_{n} \\}\n\n  \\mathbf{a} = \\begin{bmatrix}\n  \\alpha_{1} \\\\\n  \\vdots \\\\\n  \\alpha_{n} \\\\\n  \\end{bmatrix},\n  \nthere is a mapped vector T (x) \\in \\mathcal{V} with basis coefficients in a given basis \\{ v_{1}, \\dots, v_{n} \\}\n\n  \\mathbf{b} = \\begin{bmatrix}\n  \\beta_{1} \\\\\n  \\vdots \\\\\n  \\beta_{m} \\\\\n  \\end{bmatrix},\n  \nwhere\n\n  \\mathbf{b} = \\mathbf{C} \\mathbf{a}."
  },
  {
    "objectID": "Linear Algebra/05_Linear_Map_and_Rank.html#generalization-of-null-space-and-range-space",
    "href": "Linear Algebra/05_Linear_Map_and_Rank.html#generalization-of-null-space-and-range-space",
    "title": "5  Linear Map and Rank",
    "section": "Generalization of null space and range space",
    "text": "Generalization of null space and range space\nSince every linear map has a matrix representation, the null space and range space defined using matrix can be redefined using linear map.\nGiven a map T: \\mathcal{U} \\to \\mathcal{V}, the null space (kernel) is\n\nN (T) = \\left\\{\n    x \\in \\mathcal{U} \\mid T (x) = 0\n\\right\\},\n\nand the range space (column space) is\n\nR (T) = \\left\\{\n    y \\in \\mathcal{V} \\mid y \\in T (x), \\forall x \\in \\mathcal{U}\n\\right\\}."
  },
  {
    "objectID": "Linear Algebra/05_Linear_Map_and_Rank.html#rank",
    "href": "Linear Algebra/05_Linear_Map_and_Rank.html#rank",
    "title": "5  Linear Map and Rank",
    "section": "Rank",
    "text": "Rank\nConsider matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}, the rank of matrix \\mathbf{A} is\n\n\\text{rank} (\\mathbf{A}) = \\text{dim} (R (\\mathbf{A})).\n\n\nProperties of rank\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n},\n\nIf \\text{rank} (\\mathbf{A}) = n, the columns of \\mathbf{A} are linearly independent.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\nrank-nullity theorem: \\text{rank} (\\mathbf{A}) + \\text{dim} (N (\\mathbf{A})) = n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathbf{A} represents a linear transform T: \\mathcal{U} \\to \\mathcal{V} with \\mathrm{dim} (\\mathcal{U}) = m, \\mathrm{dim} (\\mathcal{V}) = n the rank-nullity theorem can also be stated as\n\n  \\mathrm{dim} (R (T)) + \\mathrm{dim} (N (T)) = \\mathrm{dim} (\\mathcal{U}).\n  \nBy assuming that \\text{dim} (N (T)) = k, there exists a basis \\{ b_{1}, \\dots, b_{k} \\} of N (T).\nSince by definition of the null space, N (T) is a subspace of \\mathcal{U}, then according to the property of the basis, there exists a set of vectors \\{ b_{k + 1}, \\dots, b_{n} \\} in \\mathcal{U} such that\n\n  \\{ b_{1}, \\dots, b_{k}, b_{k + 1}, \\dots, b_{n} \\}\n  \nis a basis of \\mathcal{U}.\nConsider any x \\in \\mathcal{U},\n\n  \\begin{aligned}\n  T (x)\n  & = T \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} b_{i}\n  \\right)\n  & [\\{ b_{1}, \\dots, b_{i} \\} \\text{ is a basis of } \\mathcal{U}]\n  \\\\\n  & = \\sum_{i=1}^{n} \\alpha_{i} T (b_{i})\n  & [\\text{T is linear}]\n  \\\\\n  \\end{aligned}\n  \nSince \\{ b_{1}, \\dots, b_{k} \\} is a basis of N (T), they are also in N (T). According to the definition of the null space\n\n  T (b_{i}) = 0 \\quad i = 1, \\dots, k.\n  \nThen we can show that every T (x) is a linear combination of T (b_{k+1}) \\dots, T (b_{n}):\n\n  \\begin{aligned}\n  T (x)\n  & = \\sum_{i=1}^{n} \\alpha_{i} T (b_{i})\n  \\\\\n  & = \\sum_{i=1}^{k} \\alpha_{i} T (b_{i}) + \\sum_{i=k+1}^{n} \\alpha_{i} T (b_{i})\n  \\\\\n  & = 0 + \\sum_{i=k+1}^{n} \\alpha_{i} T (b_{i})\n  \\\\\n  & = \\sum_{i=k+1}^{n} \\alpha_{i} T (b_{i})\n  \\end{aligned}.\n  \nSince the definition of the range space is\n\n  R (T) = \\{ y \\mid y = T(x), \\forall x \\in \\mathcal{U} \\},\n  \nall vectors in R (T) are linear combinations of T (b_{k+1}) \\dots, T (b_{n}).\nAlso, since b_{k+1}, \\dots, b_{n} \\in \\mathcal{U}, we have\n\n  T (b_{k+1}), \\dots, T (b_{n}) \\in R (T).\n  \nby the definition of the range space.\nThus, by the property of span\n\n  R (T) = \\text{span} (T (b_{k+1}), \\dots, T (b_{n})).\n  \nThen we will show that T (b_{k+1}), \\dots, T (b_{n}) are linearly independent by supposing a set of \\beta_{k+1}, \\dots, \\beta_{n} such that\n\n  \\begin{aligned}\n  \\sum_{i=k+1}^{n} \\beta_{i} T (b_{i})\n  & = 0\n  \\\\\n  T \\left(\n      \\sum_{i=k+1}^{n} \\beta_{i} b_{i}\n  \\right)\n  & = 0\n  & [\\text{T is linear}]\n  \\\\\n  \\sum_{i=k+1}^{n} \\beta_{i} b_{i}\n  & \\in N (T)\n  & [\\text{def of null space}]\n  \\\\\n  \\end{aligned}\n  \nSince \\{ b_{1}, \\dots, b_{k} \\} is also a basis of the N (T), there exists a set of \\gamma_{1}, \\dots, \\gamma_{k} such that\n\n  \\begin{aligned}\n  \\sum_{i=k+1}^{n} \\beta_{i} b_{i}\n  & = \\sum_{i=j}^{k} \\gamma_{j} b_{j}\n  \\\\\n  \\sum_{i=k+1}^{n} \\beta_{i} b_{i} + \\sum_{j=1}^{k} - \\gamma_{j} b_{j}\n  & = 0.\n  \\end{aligned}\n  \nSince \\{ b_{1}, \\dots, b_{n} \\} are linearly independent, it must hold that\n\n  \\beta_{i} = \\gamma_{j} = 0 \\quad i = k + 1, \\dots, n, j = 1, \\dots, k.\n  \nThus, we have shown that\n\n  \\sum_{i=k+1}^{n} \\beta_{i} T (b_{i}) = 0 \\iff \\beta_{i} = 0 \\quad i = k + 1, \\dots, n,\n  \nwhich means the vectors T (b_{k + 1}), \\dots, T (b_{n}) are linearly independent and therefore is a basis of R (T).\nFinally, since\n\n\\{ b_{1}, \\dots, b_{k} \\} is a basis of N (T),\n\\{ b_{k + 1}, \\dots, b_{n} \\} is a basis of R (T), and\n\\{ b_{1}, \\dots, b_{n} \\} is a basis of \\mathcal{U},\n\nwe have the rank-nullity theorem\n\n  \\text{dim} (N (T)) + \\text{dim} (R (T)) = \\text{dim} (\\mathcal{U}).\n  \n\n\n\n\\text{rank} (\\mathbf{A}) \\leq \\min (m, n). If \\text{rank} (\\mathbf{A}) = \\min (m, n), matrix is called a full (row or column) rank matrix.\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the rank property\n\n  \\text{dim} (N (\\mathbf{\\mathbf{A}})) = n - \\text{rank} (\\mathbf{A}).\n  \nSince by definition, the number of vectors in a basis is non-negative\n\n  \\text{dim} (N (\\mathbf{\\mathbf{A}})) \\geq 0,\n  \nwe have\n\n  \\text{rank} (\\mathbf{A}) \\leq n.\n  \nAlso, since R (\\mathbf{A}) is a subspace of \\mathbb{F}^{m} and the property\n\n  \\text{rank} (\\mathbf{A}) = \\text{dim} (R (\\mathbf{A})) \\leq m.\n  \nThus,\n\n  \\text{rank} (\\mathbf{A}) \\leq \\min(m, n).\n  \n\n\n\n\\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T})\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\mathbf{A} \\in \\mathbb{F}^{m \\times n}. Let \\{ \\mathbf{a}_{1}, \\dots, \\mathbf{a}_{n} \\in \\mathbb{F}^{m} \\} be the columns of \\mathbf{A}\n\n  \\mathbf{A} =\n  \\begin{bmatrix}\n  \\mathbf{a}_{1} & \\dots & \\mathbf{a}_{n}\n  \\end{bmatrix}.\n  \nLet \\text{rank} (\\mathbf{A}) = r.\nBy the definition of the range space, the columns of \\mathbf{A} are in its range space\n\n  \\mathbf{a}_{1}, \\dots, \\mathbf{a}_{n} \\in R (\\mathbf{A})\n  \nand thus the columns of \\mathbf{A} are linear combinations of any basis of R (\\mathbf{A}).\nSuppose there exists a basis of R (\\mathbf{A}) as columns of \\mathbf{B}\n\n  \\mathbf{B} =\n  \\begin{bmatrix}\n  \\mathbf{b}_{1} & \\dots & \\mathbf{b}_{r}\n  \\end{bmatrix}.\n  \nand a matrix \\mathbf{C} \\in \\mathbb{F}^{r \\times n}\n\n  \\mathbf{C} =\n  \\begin{bmatrix}\n  c_{1, 1} & \\dots & c_{n, 1} \\\\\n  \\vdots & \\dots & \\vdots \\\\\n  c_{1, r} & \\dots & c_{n, r} \\\\\n  \\end{bmatrix}\n  \nsuch that the columns of \\mathbf{A} are linear combinations of the basis \\mathbf{B} using the columns of \\mathbf{C} as cofficients\n\n  \\mathbf{A} = \\mathbf{B} \\mathbf{C}.\n  \nBy the definition of matrix multiplication, the rows of \\mathbf{A} are also linear combinations of rows of \\mathbf{C} using the rows of \\mathbf{B} as coefficients. Thus, by the spanning set property,\n\n  R (\\mathbf{A}^{T}) \\subseteq R (\\mathbf{C}^{T}).\n  \nThen, by the dimension property,\n\n  \\begin{aligned}\n  \\text{dim} (R (\\mathbf{A}^{T}))\n  & \\leq \\text{dim} (R (\\mathbf{C}^{T}))\n  \\\\\n  \\text{rank} (\\mathbf{A}^{T})\n  & \\leq \\text{rank} (\\mathbf{C}^{T}).\n  \\end{aligned}\n  \nSince \\mathbf{C}^{T} \\in \\mathbb{F}^{n \\times n}, according to the rank property\n\n  \\text{rank} (\\mathbf{C}^{T}) \\leq r = \\text{rank} (\\mathbf{A}).\n  \nThus,\n\n  \\text{rank} (\\mathbf{A}^{T}) \\leq \\text{rank} (\\mathbf{A}).\n  \nFinally, we can use the same argument to prove\n\n  \\text{rank} (\\mathbf{A}) \\leq \\text{rank} (\\mathbf{A}^{T}).\n  \nby substituting \\mathbf{A} as \\mathbf{A}^{T}.\nSince \\text{rank} (\\mathbf{A}^{T}) \\leq \\text{rank} (\\mathbf{A}) and \\text{rank} (\\mathbf{A}) \\leq \\text{rank} (\\mathbf{A}^{T}) at the same time, we can prove\n\n  \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T}).\n  \n\n\n\nIf \\mathbf{A} \\in \\mathbb{R}^{m \\times n}, then\n\n  \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\nIf \\mathbf{A} \\in \\mathbb{R}^{m \\times n} and \\mathbf{B} \\in \\mathbb{R}^{m \\times m} and \\mathbf{C} \\in \\mathbb{R}^{n \\times n} are full rank matrices, then\n\n  \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{B} \\mathbf{A}) = \\text{rank} (\\mathbf{A} \\mathbf{C}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/05_Linear_Map_and_Rank.html#rank-and-matrix-inverse",
    "href": "Linear Algebra/05_Linear_Map_and_Rank.html#rank-and-matrix-inverse",
    "title": "5  Linear Map and Rank",
    "section": "Rank and matrix inverse",
    "text": "Rank and matrix inverse\nSuppose \\mathbf{A} \\in \\mathbb{F}^{m \\times n}. There exists a left inverse matrix \\mathbf{B} \\in \\mathbb{F}^{n \\times m} of \\mathbf{A} such that\n\n\\mathbf{B} \\mathbf{A} = \\mathbf{I}_{n \\times n},\n\nif and only if \\text{rank} (\\mathbf{A}) = n.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTo prove this, we first prove that there exists a matrix \\mathbf{C} such that\n\n\\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m} \\iff \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (A).\n\nFirst notice that\n\n\\mathbf{I}_{m \\times m} =\n\\begin{bmatrix}\n\\mathbf{e}_{1} & \\dots & \\mathbf{e}_{m}\n\\end{bmatrix}\n\nwhere \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} \\in \\mathbb{F}^{m}.\nThus, \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} is a linear combination of the columns of \\mathbf{A}. By the definition of the range space, we can see\n\n\\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} \\in R (\\mathbf{A}).\n\nConversely, since \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} \\in R (\\mathbf{A}), each \\mathbf{e}_{i}, i = 1, \\dots, m is a linear combination of the columns in \\mathbf{A}.\nThus, there exists a set of coefficients c_{j, i}, j = 1, \\dots, n for the linear combination of \\mathbf{e}_{i}, and the matrix of these coefficients is\n\n\\mathbf{C} =\n\\begin{bmatrix}\nc_{1, 1} & \\dots & c_{1, m} \\\\\n\\vdots & c_{j, i} & \\vdots \\\\\nc_{n, 1} & \\dots & c_{n, m} \\\\\n\\end{bmatrix}.\n\nThen we prove that\n\n\\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (\\mathbf{A}) \\iff \\text{rank} (\\mathbf{A}) = m.\n\nAgain notice that \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} is the standard basis of \\mathbb{F}^{m}.\nSince R (\\mathbf{A}) \\subseteq \\mathbb{F}^{m}, all vectors in R (\\mathbf{A}) are linear combinations of \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m}.\nSince we know that \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (\\mathbf{A}), according to the spanning set property,\n\nR (\\mathbf{A}) = \\text{span} (\\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m}).\n\nSince by definition \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} are linearly independent, the set \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} is a basis for R (\\mathbf{A}).\nAccording to the cardinality of basis,\n\n\\text{rank} (\\mathbf{A}) = m.\n\nConversely, since we know \\text{rank} (\\mathbf{A}) = m = \\text{dim} (\\mathbb{F}^{m} and R (\\mathbf{A}) \\subseteq \\mathbb{F}^{m},\n\nR (\\mathbf{A}) = \\mathbb{F}^{m}.\n\nSince by definition \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in \\mathbb{F}^{m},\n\n\\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (\\mathbf{A}).\n\nIn conclusion, we have proven that there exists a matrix \\mathbf{C} such that\n\n\\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m} \\iff \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (A) \\iff \\text{rank} (\\mathbf{A}) = m.\n\n\n\n\nThere exists a right inverse matrix \\mathbf{C} \\in \\mathbb{F}^{n \\times m} of \\mathbf{A} such that\n\n\\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m}.\n\nif and only if \\text{rank} (\\mathbf{A}) = m.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first notice that\n\n\\begin{aligned}\n(\\mathbf{B} \\mathbf{A})^{T}\n& = \\mathbf{I}_{n \\times n}^{T}\n\\\\\n\\mathbf{A}^{T} \\mathbf{B}^{T}\n& = \\mathbf{I}_{n \\times n}.\n\\end{aligned}\n\nBy applying the proof above, we can prove that\n\n\\text{rank} (\\mathbf{A}^{T}) = n.\n\nAccording to the rank property,\n\n\\text{rank} (\\mathbf{A}^{T}) = \\text{rank} (\\mathbf{A}) = n.\n\n\n\n\nIf \\mathbf{A} \\in \\mathbb{F}^{n \\times n} is a square matrix and has full rank (\\text{rank}(\\mathbf{A}) = n), then \\mathbf{A} has a unique inverse matrix \\mathbf{A}^{-1} such that\n\n\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/06_Inner_Product_and_Norm.html#inner-product",
    "href": "Linear Algebra/06_Inner_Product_and_Norm.html#inner-product",
    "title": "6  Inner Product and Norm",
    "section": "Inner Product",
    "text": "Inner Product\nA vector space \\mathcal{V} over field \\mathbb{R} or \\mathbb{C} is a inner product space if there exists a function called inner product, denoted\n\n\\langle \\cdot, \\cdot \\rangle: \\mathcal{V} \\times \\mathcal{V} \\to \\mathbb{R}\n\nwith the following properties:\n\nLinearity of first argument \\langle \\alpha x + \\alpha y, z \\rangle = \\alpha \\langle x, z \\rangle + \\alpha \\langle y, z \\rangle,\nConjugate symmetry \\langle x, y \\rangle = \\overline{\\langle y, x \\rangle},\nPositive semi-definiteness \\langle x, x \\rangle \\geq 0 and \\langle x, x \\rangle = 0 \\iff x = 0.\n\n\nProperties of inner product\n\n\\langle x, \\alpha y + \\alpha z \\rangle = \\overline{\\alpha} \\langle x, y \\rangle + \\overline{\\alpha} \\langle x, z \\rangle\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n  \\begin{aligned}\n  \\langle x, \\alpha y + \\alpha z \\rangle\n  & = \\overline{\\langle \\alpha y + \\alpha z, x \\rangle}\n  & [\\text{Conjugate sym}]\n  \\\\\n  & = \\overline{\\alpha \\langle y, x \\rangle + \\alpha \\langle z, x \\rangle}\n  & [\\text{Linearity of 1st}]\n  \\\\\n  & = \\overline{\\alpha} \\overline{\\langle y, x \\rangle} + \\overline{\\alpha} \\overline{\\langle z, x \\rangle}\n  \\\\\n  & = \\overline{\\alpha} \\langle x, y \\rangle + \\overline{\\alpha} \\langle x, z \\rangle\n  \\end{aligned}\n  \n\n\n\nIf \\langle x, y \\rangle = 0 for all x \\in \\mathcal{V}, then y = 0.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/06_Inner_Product_and_Norm.html#norm",
    "href": "Linear Algebra/06_Inner_Product_and_Norm.html#norm",
    "title": "6  Inner Product and Norm",
    "section": "Norm",
    "text": "Norm\nA norm of a vector in a inner product vector space \\mathcal{V} is a function\n\n\\lVert \\cdot \\rVert \\to \\mathbb{R}^{+}\n\nwith the following properties:\n\n\\lVert \\alpha v \\rVert = \\lvert \\alpha \\rvert \\lVert v \\rVert, \\alpha \\in \\mathbb{F}, v \\in \\mathcal{V},\n\\lVert v \\rVert = 0 \\iff v = 0.\nTriangle inequality: \\lVert v_{1} + v_{2} \\rVert \\leq \\lVert v_{1} \\rVert + \\lVert v_{2} \\rVert,\n\nA vector space \\mathcal{V} equipped with a norm is called a normed vector space.\n\nLp-norm\nGiven \\mathcal{V} = \\mathbb{R}^{n}, define \\lVert \\cdot \\rVert_{p}: \\mathcal{V} \\to \\mathbb{R}^{+} as\n\n\\lVert \\mathbf{v} \\rVert_{p} = \\left(\n    \\sum_{i=1}^{n} \\lvert v_{i} \\rvert^{p}\n\\right)^{\\frac{1}{p}}, \\quad \\mathbf{v} \\in \\mathcal{V},\n\nfor p \\geq 1.\n\nL_{1} norm:\n\n  \\lVert \\mathbf{v} \\rVert_{1} = \\sum_{i = 1}^{n} \\lvert v_{i} \\rvert\n  \nL_{2} norm:\n\n  \\lVert \\mathbf{v} \\rVert_{2} = \\left( \\sum_{i = 1}^{n} \\lvert v_{i} \\rvert^{2} \\right)^{\\frac{1}{2}}"
  },
  {
    "objectID": "Linear Algebra/06_Inner_Product_and_Norm.html#norm-induced-by-inner-product",
    "href": "Linear Algebra/06_Inner_Product_and_Norm.html#norm-induced-by-inner-product",
    "title": "6  Inner Product and Norm",
    "section": "Norm induced by inner product",
    "text": "Norm induced by inner product\nGiven an inner product vector space \\mathcal{V}, a norm \\lVert \\cdot \\rVert_{ip}: \\mathcal{V} \\to R^{+} induced by its inner product is\n\n\\lVert v \\rVert_{ip} = \\sqrt{\\langle v, v \\rangle}, \\quad v \\in \\mathcal{V}.\n\n\nFor the vector space \\mathbb{C}^{n}, the norm induced by inner product is the same as L_{2} norm\n\n  \\lVert \\mathbf{v} \\rVert_{ip} = \\sqrt{ \\sum_{i=1}^{n} v_{i} v_{i} } = \\sqrt{ \\sum_{i=1}^{n} \\lvert v_{i} \\rvert^{2} } = \\lVert \\mathbf{v} \\lVert_{2}\n  \nCauchy-Schwarz Inequality: let \\mathcal{V} be an inner product space. Then, for any u, v \\in \\mathcal{V}, we have\n\n  \\lvert \\langle u, v \\rangle \\rvert \\leq \\lVert u \\rVert_{ip} \\lVert v \\rVert_{ip}."
  },
  {
    "objectID": "Linear Algebra/07_Orthogonality_and_Orthogonal_Matrix.html#orthogonality",
    "href": "Linear Algebra/07_Orthogonality_and_Orthogonal_Matrix.html#orthogonality",
    "title": "7  Orthogonality and Orthogonal Matrix",
    "section": "Orthogonality",
    "text": "Orthogonality\nA set of non-zero vectors v_{1}, \\dots, v_{k} are orthogonal if\n\n\\langle v_{i}, v_{j} \\rangle = 0, \\quad \\forall i \\neq j.\n\n\nProperties of orthogonality\nIf v_{1}, \\dots, v_{k} are orthogonal vectors,\n\nv_{1}, \\dots, v_{k} are also linearly independent.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\nSuppose \\mathcal{S} is a subspace with \\text{dim} (S) = n and v_{1}, \\dots, v_{k} \\in \\mathcal{S}, then the set \\{ v_{1}, \\dots, v_{k} \\} forms a basis of \\mathcal{S}. Then, \\{ v_{1}, \\dots, v_{k} \\} is an orthogonal basis of \\mathcal{S}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/07_Orthogonality_and_Orthogonal_Matrix.html#representing-vectors-using-orthogonal-basis",
    "href": "Linear Algebra/07_Orthogonality_and_Orthogonal_Matrix.html#representing-vectors-using-orthogonal-basis",
    "title": "7  Orthogonality and Orthogonal Matrix",
    "section": "Representing vectors using orthogonal basis",
    "text": "Representing vectors using orthogonal basis\nSuppose \\mathcal{S} is a subspace and \\{ v_{1}, \\dots, v_{n} \\} is an orthogonal basis of \\mathcal{S}, any vector v \\in \\mathcal{S} can be represented using \\{ v_{1}, \\dots, v_{n} \\}:\n\nv = \\sum_{i=1}^{n} \\alpha_{i} v_{i},\n\nwhere\n\n\\alpha_{i} =\n\\frac{\n    \\langle v, v_{i} \\rangle\n}{\n    \\lVert v_{i} \\rVert_{ip}^{2}\n}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/07_Orthogonality_and_Orthogonal_Matrix.html#orthogonal-matrix",
    "href": "Linear Algebra/07_Orthogonality_and_Orthogonal_Matrix.html#orthogonal-matrix",
    "title": "7  Orthogonality and Orthogonal Matrix",
    "section": "Orthogonal matrix",
    "text": "Orthogonal matrix\nA set of vectors v_{1}, \\dots, v_{k} are orthonormal if all vectors in the set are orthogonal to each other, and each vector has the inner product norm of 1.\nA square real (complex) matrix \\mathbf{U} is orthogonal (unitary) if and only if \\mathbf{U} has orthonormal columns.\n\nProperties of orthogonal matrix\n\nThe matrix \\mathbf{U} is orthogonal if and only if \\mathbf{U}^{H} = \\mathbf{U}^{-1}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition, \\mathbf{U} has orthogonal columns and thus linearly independent columns. By the rank property, \\mathbf{U} has a unique inverse matrix \\mathbf{U}^{-1} such that\n\n  \\mathbf{U}^{-1} \\mathbf{U} = \\mathbf{I}_{n \\times n}.\n  \nSince by definition we know\n\n  \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{I}_{n \\times n},\n  \nthen it must follow that\n\n  \\mathbf{U}^{H} = \\mathbf{U}^{-1}.\n  \nThe reverse can be proved backward following the procedure above.\n\n\n\nThe matrix \\mathbf{U} is orthogonal if and only if \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{H} = \\mathbf{I}_{n \\times n}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nFollowing the unitary matrix property, the inverse \\mathbf{U}^{-1} can be both left and right inverse\n\n  \\mathbf{U}^{-1} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{-1} = \\mathbf{I}.\n  \nReplacing \\mathbf{U}^{-1} with \\mathbf{U}^{H}, we have the results:\n\n  \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{H} = \\mathbf{I}.\n  \n\n\n\nThe matrix \\mathbf{U} is orthogonal if and only if \\mathbf{U} \\mathbf{x} doesn’t change the length of \\mathbf{x}:\n\n  \\lVert \\mathbf{U} \\mathbf{x} \\rVert = \\lVert \\mathbf{x} \\rVert.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\n\n  \\begin{aligned}\n  \\lVert \\mathbf{U} \\mathbf{x} \\rVert\n  & = \\sqrt{\\lVert \\mathbf{U} \\mathbf{x} \\rVert^{2}}\n  \\\\\n  & = \\sqrt{\\mathbf{x}^{H} \\mathbf{U}^{H} \\mathbf{U} \\mathbf{x}}\n  \\\\\n  & = \\sqrt{\\mathbf{x}^{H} \\mathbf{I} \\mathbf{x}}\n  & [\\mathbf{U}^{H} \\mathbf{U} = \\mathbf{I}]\n  \\\\\n  & = \\sqrt{\\mathbf{x}^{H} \\mathbf{x}}\n  \\\\\n  & = \\sqrt{\\lVert \\mathbf{x} \\rVert^{2}}\n  \\\\\n  & = \\lVert \\mathbf{x} \\rVert\n  \\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html#complementary-subspaces",
    "href": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html#complementary-subspaces",
    "title": "8  Complementary Subspaces and Projection",
    "section": "Complementary Subspaces",
    "text": "Complementary Subspaces\nSubspaces \\mathcal{X}, \\mathcal{Y} of a vector space \\mathcal{V} are complementary if\n\n\\mathcal{V} = \\mathcal{X} + \\mathcal{Y},\n\nand\n\n\\mathcal{X} \\cap \\mathcal{Y} = 0,\n\nin which case \\mathcal{V} is the direct sum of \\mathcal{X} and \\mathcal{Y} and is denoted as\n\n\\mathcal{V} = \\mathcal{X} \\oplus \\mathcal{Y}.\n\n\nProperties of complementary subspaces\n\n\\mathcal{X} and \\mathcal{Y} are complementary if and only if there exist unique vectors x \\in \\mathcal{X} and y \\in \\mathcal{Y} such that\n\n  v = x + y,\n  \nfor each v \\in \\mathcal{V}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose there are two pairs of x_{1}, x_{2} \\in \\mathcal{X} and y_{1}, y_{2} \\in \\mathcal{Y} such that\n\n  v = x_{1} + y_{1} = x_{2} + y_{2}.\n  \nThen\n\n  \\begin{aligned}\n  x_{1} + y_{1}\n  & = x_{2} + y_{2}\n  \\\\\n  x_{1} - x_{2}\n  & = y_{2} - y_{1}\n  \\\\\n  \\end{aligned}\n  \nAccording to the definition of the subspace\n\n  \\begin{aligned}\n  x_{1}, x_{2} \\in \\mathcal{X}\n  & \\Rightarrow x_{1} - x_{2} \\in \\mathcal{X}\n  \\\\\n  y_{1}, y_{2} \\in \\mathcal{X}\n  & \\Rightarrow y_{1} - y_{2} \\in \\mathcal{Y}.\n  \\end{aligned}\n  \nwhich means\n\n  x_{1} - x_{2} = y_{2} - y_{1} \\Rightarrow x_{1} - x_{2} \\in \\mathcal{Y}.\n  \nThus,\n\n  x_{1} - x_{2} \\in \\mathcal{X} \\cap \\mathcal{Y}.\n  \nHowever, since by definition \\mathcal{X} \\cap \\mathcal{Y} = 0,\n\n  \\begin{aligned}\n  x_{1} - x_{2}\n  & = 0.\n  \\\\\n  x_{1}\n  & =  x_{2}.\n  \\end{aligned}\n  \nSimilar argument can be made for y_{1} and y_{2}.\n\n\n\nSuppose \\mathcal{X} has a basis \\mathcal{B}_{\\mathcal{X}} and \\mathcal{Y} has a basis \\mathcal{B}_{\\mathcal{Y}}. Then \\mathcal{X} and \\mathcal{Y} are complementary if and only if\n\n  \\mathcal{B}_{\\mathcal{X}} \\cap \\mathcal{B}_{\\mathcal{Y}} = \\emptyset\n  \nand\n\n  \\mathcal{B}_{\\mathcal{X}} \\cup \\mathcal{B}_{\\mathcal{Y}}\n  \nis a basis for \\mathcal{V}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html#projection",
    "href": "Linear Algebra/08_Complementary_Subspaces_and_Projection.html#projection",
    "title": "8  Complementary Subspaces and Projection",
    "section": "Projection",
    "text": "Projection\nSuppose that \\mathcal{X} and \\mathcal{Y} are complementary subspaces in \\mathcal{V}. Thus, there exists unique x \\in \\mathcal{X} and y \\in \\mathcal{Y} such that v = x + y for every vector v \\in \\mathcal{V}\nThen the unique linear operator \\mathbf{P} \\in \\mathbb{C}^{n \\times n} defined by\n\n\\mathbf{P} v = x\n\nis the projection matrix of \\mathcal{V} onto \\mathcal{X} along \\mathcal{Y} and x \\in \\mathcal{X} is the projection of v \\in \\mathcal{V} onto \\mathcal{X} along \\mathcal{Y}.\n\nProperties of projection matrix\n\n\\mathbf{I} - \\mathbf{P} is the complementary projection matrix of \\mathbf{v} onto \\mathcal{Y} along \\mathcal{X}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of projection matrix,\n\n  \\begin{aligned}\n  v\n  & = x + y\n  \\\\\n  & = \\mathbf{P} v + y.\n  \\end{aligned}\n  \nThus,\n\n  \\begin{aligned}\n  y\n  & = v - \\mathbf{P} v\n  \\\\\n  & = (\\mathbf{I} - \\mathbf{P})v.\n  \\end{aligned}\n  \n\n\n\nR (\\mathbf{P}) = N (\\mathbf{I} - \\mathbf{P}) = \\mathcal{X} and N (\\mathbf{P}) = R (\\mathbf{I} - \\mathbf{P}) = \\mathcal{Y}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince v \\in \\mathcal{V} and \\mathcal{X},\n\n  \\mathbf{P} v = x \\Rightarrow R (\\mathbf{P}) = \\mathcal{X}.\n  \nAlso, for all v \\in N (\\mathbf{I} - \\mathbf{P}),\n\n  \\begin{aligned}\n  (\\mathbf{I} - \\mathbf{P}) v\n  & = 0\n  \\\\\n  v - \\mathbf{P} v\n  & = 0\n  \\\\\n  v - x\n  & = 0\n  \\\\\n  v\n  & = x\n  \\end{aligned}\n  \nwhich means v \\in \\mathcal{X} \\Rightarrow \\mathcal{V} \\subseteq N (\\mathbf{I} - \\mathbf{P}).\nSince N (\\mathbf{I} - \\mathbf{P}) \\subseteq \\mathcal{V},\n\n  N (\\mathbf{I} - \\mathbf{P}) = \\mathcal{V}.\n  \nThe proof for N (\\mathbf{P}) = R (\\mathbf{I} - \\mathbf{P}) = \\mathcal{Y} is the same.\n\n\n\nFor x \\in \\mathcal{X}, y \\in \\mathcal{Y},\n\n  \\mathbf{P} x = x,\n  \nand\n\n  \\mathbf{P} y = 0.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\nA linear operator \\mathbf{P} on \\mathcal{V} is a projection matrix if and only if \\mathbf{P} is idempotent (\\mathbf{P} = \\mathbf{P}^{2}).\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of the projector \n  \\mathbf{P}^{2} v = \\mathbf{P} \\mathbf{P} v = \\mathbf{P} x.\n  \nAccording to the property of projection matrix\n\n  \\mathbf{P} x = x = \\mathbf{P} v.\n  \nThus, \n  \\mathbf{P} v = \\mathbf{P}^{2} v \\Rightarrow \\mathbf{P} = \\mathbf{P}^{2}.\n  \n\n\n\nIf \\mathcal{V} = \\mathbb{R}^{n} or \\mathbb{C}^{n}, then \\mathbf{P} is given by\n\n  \\mathbf{P} =\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{0}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{Y}\n  \\end{bmatrix}^{-1}\n  =\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{Y}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\mathbf{I} & \\mathbf{0} \\\\\n      \\mathbf{0} & \\mathbf{0}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{Y}\n  \\end{bmatrix}^{-1}\n  \nwhere the columns of \\mathbf{X} and \\mathbf{Y} are respective bases for \\mathcal{X} and \\mathcal{Y}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\\mathbf{P} is unique for a given \\mathcal{X} and \\mathcal{Y}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-complement",
    "href": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-complement",
    "title": "9  Orthogonal Complement and Decomposition",
    "section": "Orthogonal Complement",
    "text": "Orthogonal Complement\nFor a subset \\mathcal{M} of an inner-product space \\mathcal{V}, the orthogonal complement \\mathcal{M}^{\\perp} of \\mathcal{M} is defined to be the set of all vectors in \\mathcal{V} that are orthogonal to every vector in \\mathcal{M}\n\n\\mathcal{M}^{\\perp} = \\left\\{\n    x \\in \\mathcal{V} \\mid \\langle x, m \\rangle = 0, \\forall m \\in \\mathcal{M}\n\\right\\}.\n\nThe set \\mathcal{M}^{\\perp} is a subspace even if \\mathcal{M} is not.\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe set \\mathcal{M}^{\\perp} is closed under addition and multiplication.\nSuppose x, y \\in \\mathcal{M}^{\\perp}, which means\n\n\\langle x, m \\rangle = 0, \\forall m \\in \\mathcal{M},\n\n\n\\langle y, m \\rangle = 0, \\forall m \\in \\mathcal{M}.\n\nClosed under addition:\n\n\\langle x + y, m \\rangle = \\langle x, m \\rangle + \\langle y, m \\rangle = 0, \\forall m \\in \\mathcal{M}.\n\nClosed under multiplication:\n\n\\langle \\alpha x, m \\rangle = \\alpha \\langle x, m \\rangle = 0, \\forall m \\in \\mathcal{M}, \\forall \\alpha \\in \\mathbb{F}."
  },
  {
    "objectID": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-complementary-subspaces",
    "href": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-complementary-subspaces",
    "title": "9  Orthogonal Complement and Decomposition",
    "section": "Orthogonal Complementary Subspaces",
    "text": "Orthogonal Complementary Subspaces\nWhen \\mathcal{M} is a subspace of a finite-dimensional inner-product space \\mathcal{V}, \\mathcal{M} and \\mathcal{M}^{\\perp} are complementary subspaces in \\mathcal{V}\n\n\\mathcal{V} = \\mathcal{M} \\oplus \\mathcal{M}^{\\perp}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\nProperties of orthogonal complementary subspaces\nSuppose \\mathcal{M} is a subspace of an n-dimensional inner-product space \\mathcal{V}.\n\n\\text{dim} (\\mathcal{M}) + \\text{dim} (\\mathcal{M}^{\\perp}) = n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathcal{M}^{T} and \\mathcal{M} are complementary subspaces, the property of complementary subspace shows that\n\n  \\mathcal{B}_{\\mathcal{M}} \\cup \\mathcal{B}_{\\mathcal{M}^{\\perp}} = \\mathcal{B}_{\\mathcal{V}}\n  \nand\n\n  \\mathcal{B}_{\\mathcal{M}} \\cap \\mathcal{B}_{\\mathcal{M}^{\\perp}} = \\emptyset.\n  \nThus,\n\n  \\begin{aligned}\n  \\lvert \\mathcal{B}_{\\mathcal{M}} \\rvert + \\lvert \\mathcal{B}_{\\mathcal{M}^{\\perp}} \\rvert\n  & = \\lvert \\mathcal{B}_{\\mathcal{V}} \\rvert\n  \\\\\n  \\text{dim} (\\mathcal{M}) + \\text{dim} (\\mathcal{M}^{\\perp})\n  & = n\n  \\end{aligned}\n  \n\n\n\n\\mathcal{M}^{\\perp^{\\perp}} = \\mathcal{M}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathcal{M}^{\\perp^{\\perp}} \\subseteq \\mathcal{V},\n\n  x \\in \\mathcal{M}^{\\perp^{\\perp}} \\Rightarrow x \\in \\mathcal{V}.\n  \nSince \\mathcal{M} and \\mathcal{M}^{\\perp} are complementary subspaces, every x \\in \\mathcal{V} can be uniquely represented by m \\in \\mathcal{M} and n \\in \\mathcal{M}^{\\perp}\n\n  x = m + n.\n  \nSince \\mathcal{M}^{\\perp^{\\perp}} \\perp \\mathcal{M}^{\\perp}, by definition\n\n  \\begin{aligned}\n  0\n  & = \\langle x, n \\rangle\n  \\\\\n  & = \\langle m + n, n \\rangle\n  \\\\\n  & = \\langle m, n \\rangle + \\langle n, n \\rangle\n  \\\\\n  & = \\langle n, n \\rangle\n  & [\\mathcal{M} \\perp \\mathcal{M}^{\\perp} \\Rightarrow \\langle m, n \\rangle = 0].\n  \\end{aligned}\n  \nBy the definition of the inner product,\n\n  \\langle n, n \\rangle = 0 \\Rightarrow n = 0.\n  \nThus, for each x \\in \\mathcal{M}^{\\perp^{\\perp}}\n\n  x = m \\in \\mathcal{M} \\Rightarrow \\mathcal{M}^{\\perp^{\\perp}} \\subseteq \\mathcal{M}.\n  \nBy the property\n\n  \\text{dim} (\\mathcal{M}) = n - \\text{dim} (\\mathcal{M}^{\\perp}) = \\text{dim} (\\mathcal{M}^{\\perp^{\\perp}}).\n  \nThen by the property,\n\n  \\mathcal{M} = \\mathcal{M}^{\\perp^{\\perp}}."
  },
  {
    "objectID": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-decomposition",
    "href": "Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-decomposition",
    "title": "9  Orthogonal Complement and Decomposition",
    "section": "Orthogonal Decomposition",
    "text": "Orthogonal Decomposition\nFor every \\mathbf{A} \\in \\mathbb{R}^{m \\times n},\n\nR (\\mathbf{A}) \\perp N (\\mathbf{A}^{H}),\n\n\nN (\\mathbf{A}) \\perp R (\\mathbf{A}^{H}),\n\nwhich means that every matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n} produces an orthogonal decomposition of \\mathbb{R}^{m} and \\mathbb{R}^{n} in the sense that\n\n\\mathbb{R}^{m} = R (\\mathbf{A}) \\oplus R (\\mathbf{A})^{\\perp} = R (\\mathbf{A}) \\oplus N (\\mathbf{A}^{H}),\n\n\n\\mathbb{R}^{n} = N (\\mathbf{A}) \\oplus N (\\mathbf{A})^{\\perp} = N (\\mathbf{A}) \\oplus R (\\mathbf{A}^{H}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\mathbf{A} \\in \\mathbb{R}^{m \\times n} and \\mathbf{x} \\in R (\\mathbf{A})^{\\perp}.\nConsider every \\mathbf{y} \\in \\mathbb{R}^{n}, we have\n\n\\begin{aligned}\n\\\\\n\\mathbf{x} \\in R (\\mathbf{A})^{\\perp} \\iff \\langle \\mathbf{A} \\mathbf{y}, \\mathbf{x} \\rangle\n& = 0\n\\\\\n(\\mathbf{A} \\mathbf{y})^{H} \\mathbf{x}\n& = 0\n\\\\\n\\mathbf{y}^{H} \\mathbf{A}^{H} \\mathbf{x}\n& = 0\n\\\\\n\\langle \\mathbf{y}, \\mathbf{A}^{H} \\mathbf{x} \\rangle\n& = 0.\n\\\\\n\\end{aligned}\n\nAccording to the property of inner product,\n\n\\langle \\mathbf{y}, \\mathbf{A}^{H} \\mathbf{x} \\rangle = 0 \\iff \\mathbf{A}^{H} \\mathbf{x} = 0 \\iff \\mathbf{x} \\in N (\\mathbf{A}^{H})\n\nThus,\n\nR (\\mathbf{A})^{\\perp} = N (\\mathbf{A}^{H}).\n\nReplacing \\mathbf{A} with \\mathbf{A}^{H} above, we can get\n\n\\begin{aligned}\nR (\\mathbf{A}^{H})^{\\perp}\n& = N (\\mathbf{A}).\n\\\\\nR (\\mathbf{A}^{H})\n& = N (\\mathbf{A})^{\\perp}.\n\\\\\n\\end{aligned}\n\nSince R (\\mathbf{A}) is a subspace in \\mathbb{R}^{m} and N (\\mathbf{A}) is a subspace in \\mathbb{R}^{n},\n\nR (\\mathbf{A}) \\oplus R (\\mathbf{A})^{\\perp} = R (\\mathbf{A}) \\oplus N (\\mathbf{A}^{H}) = \\mathbb{R}^{m},\n\n\nN (\\mathbf{A}) \\oplus N (\\mathbf{A})^{\\perp} = N (\\mathbf{A}) \\oplus R (\\mathbf{A}^{H}) = \\mathbb{R}^{n}."
  },
  {
    "objectID": "Linear Algebra/10_Singular_Value_Decomposition_and_Pseudoinverse.html#urv-factorization",
    "href": "Linear Algebra/10_Singular_Value_Decomposition_and_Pseudoinverse.html#urv-factorization",
    "title": "10  SVD and Pseudoinverse",
    "section": "URV Factorization",
    "text": "URV Factorization\nFor each \\mathbf{A} \\in \\mathbb{R}^{m \\times n} of rank r, there are orthogonal matrices \\mathbf{U}_{m \\times m} and \\mathbf{V}_{n \\times n} and a nonsingular matrix \\mathbf{C}_{r \\times r} such that\n\n\\mathbf{A} = \\mathbf{U} \\mathbf{R} \\mathbf{V}^{T} = \\mathbf{U}\n\\begin{bmatrix}\n\\mathbf{C}_{r \\times r} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}_{m \\times n}\n\\mathbf{V}^{T}.\n\n\nThe first r columns in \\mathbf{U} are an orthonormal basis for R (\\mathbf{A}).\nThe last m - r columns of \\mathbf{U} are an orthonormal basis for N (\\mathbf{A}^{T}).\nThe first r columns in \\mathbf{V} are an orthonormal basis for R (\\mathbf{A}^{T}).\nThe last m - r columns of \\mathbf{V} are an orthonormal basis for N (\\mathbf{A}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose vector spaces \\mathbb{R}^{m} and \\mathbb{R}^{n} have orthonormal bases \\{ \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{m} \\} and \\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\}, respectively.\nGiven any matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n}, the orthogonal decomposition theorem shows that the following subspaces are complementary in \\mathbb{R}^{m} and \\mathbb{R}^{n}.\n\nR (\\mathbf{A}) \\oplus N (\\mathbf{A}^{T}) = \\mathbb{R}^{m},\n\n\nR (\\mathbf{A}^{T}) \\oplus N (\\mathbf{A}) = \\mathbb{R}^{n}.\n\nAccording to the property of the complementary subspaces, we can separate the basis for \\mathbb{R}^{m} into the basis for R (\\mathbf{A}) and N (\\mathbf{A}^{T}), and the basis for \\mathbb{R}^{n} into the basis for N (\\mathbf{A}) and R (\\mathbf{A}^{T}).\nSince \\mathrm{rank} (\\mathbf{A}) = r, suppose the bases are separated in the following way:\n\n\\mathcal{B}_{R (\\mathbf{A})} = \\{ \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{r} \\},\n\\mathcal{B}_{N (\\mathbf{A}^{T})} = \\{ \\mathbf{u}_{r + 1}, \\dots , \\mathbf{u}_{m} \\},\n\\mathcal{B}_{R (\\mathbf{A}^{T})} = \\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{r} \\},\n\\mathcal{B}_{N (\\mathbf{A})} = \\{ \\mathbf{v}_{r + 1}, \\dots , \\mathbf{v}_{n} \\}.\n\nNow we treat the bases for \\mathbb{R}^{m} and \\mathbb{R}^{n} as the columns of the matrices \\mathbf{U} and \\mathbf{V}:\n\n\\mathbf{U} =\n\\begin{bmatrix}\n\\mathbf{u}_{1} & \\dots & \\mathbf{u}_{m}\n\\end{bmatrix},\n\n\n\\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{v}_{1} & \\dots & \\mathbf{v}_{n}\n\\end{bmatrix},\n\nand define\n\n\\mathbf{R} = \\mathbf{U}^{T} \\mathbf{A} \\mathbf{V}.\n\nNote that\n\n\\mathbf{A} \\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{A} \\mathbf{v}_{n}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{A} \\mathbf{v}_{r} & 0 & \\dots & 0\n\\end{bmatrix}\n\n\n\\mathbf{U}^{T} \\mathbf{A} = (\\mathbf{A}^{T} \\mathbf{U})^{T} =\n\\begin{bmatrix}\n\\mathbf{A}^{T} \\mathbf{u}_{1} & \\dots & \\mathbf{A}^{T} \\mathbf{u}_{n}\n\\end{bmatrix}^{T} =\n\\begin{bmatrix}\n\\mathbf{A}^{T} \\mathbf{u}_{1} & \\dots & \\mathbf{A}^{T} \\mathbf{u}_{r} & 0 & \\dots & 0\n\\end{bmatrix}^{T}\n\nsince \\mathbf{v}_{r + 1}, \\dots, \\mathbf{v}_{n} \\in N (\\mathbf{A}), and \\mathbf{u}_{r + 1}, \\dots, \\mathbf{u}_{m} \\in N (\\mathbf{A}^{T}).\nThus, \\mathbf{R} has mostly 0 except the top left corner:\n\n\\mathbf{R} = \\mathbf{U}^{T} \\mathbf{A} \\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{u}_{1}^{T} \\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{u}_{1}^{T} \\mathbf{A} \\mathbf{v}_{r} & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\mathbf{u}_{r}^{T} \\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{u}_{r}^{T} \\mathbf{A} \\mathbf{v}_{r} & 0 & \\dots & 0 \\\\\n0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{C} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}.\n\nThen we can see \\mathbf{A} can always be decomposed in terms of \\mathbf{U}, \\mathbf{R}, \\mathbf{V} because of the property of orthogonal matrix\n\n\\begin{aligned}\n\\mathbf{U} \\mathbf{R} \\mathbf{V}^{T}\n& = \\mathbf{U} \\mathbf{U}^{T} \\mathbf{A} \\mathbf{V} \\mathbf{V}^{T}\n\\\\\n& = \\mathbf{U} \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{V} \\mathbf{V}^{-1}\n\\\\\n& = \\mathbf{A}.\n\\end{aligned}\n\nTo see why \\mathbf{C} is always non-singular, we first note that\n\n\\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{U}^{T} \\mathbf{A} \\mathbf{V}) = r\n\nbecause the multiplication by a full-rank square matrix preserves rank.\nThus,\n\n\\text{rank} (\\mathbf{C}) = \\text{rank} \\left(\n    \\begin{bmatrix}\n    \\mathbf{C} & 0 \\\\\n    0 & 0 \\\\\n    \\end{bmatrix}\n\\right) = \\text{rank} (\\mathbf{U}^{T} \\mathbf{A} \\mathbf{V}) = r."
  },
  {
    "objectID": "Linear Algebra/10_Singular_Value_Decomposition_and_Pseudoinverse.html#singular-value-decomposition-svd",
    "href": "Linear Algebra/10_Singular_Value_Decomposition_and_Pseudoinverse.html#singular-value-decomposition-svd",
    "title": "10  SVD and Pseudoinverse",
    "section": "Singular Value Decomposition (SVD)",
    "text": "Singular Value Decomposition (SVD)\nSingular value decomposition is a special case of the URV factorization where the \\mathbf{C} matrix is a diagonal matrix with increasing values in the diagonal.\nFor each \\mathbf{A} \\in \\mathbb{C}^{m \\times n} of rank r, there are orthogonal matrices \\mathbf{U} \\in \\mathbb{R}^{m \\times m} and \\mathbf{V} \\in \\mathbb{R}^{n \\times n}, and a diagonal matrix \\mathbf{D} \\in \\mathbb{C}^{r \\times r} = \\text{diag} (\\sigma_{1}, \\dots, \\sigma_{r}) such that\n\n\\mathbf{A} = \\mathbf{U}\n\\begin{bmatrix}\n\\mathbf{D} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\mathbf{V}^{H}\n\nwith\n\n\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots, \\geq \\sigma_{r},\n\nwhere\n\nthe columns in \\mathbf{U} and \\mathbf{V} are singular vectors and\nthe diagonal values of \\mathbf{D} (\\sigma_{i}) are singular values."
  },
  {
    "objectID": "Linear Algebra/10_Singular_Value_Decomposition_and_Pseudoinverse.html#pseudoinverse",
    "href": "Linear Algebra/10_Singular_Value_Decomposition_and_Pseudoinverse.html#pseudoinverse",
    "title": "10  SVD and Pseudoinverse",
    "section": "Pseudoinverse",
    "text": "Pseudoinverse\nA generalized inverse for any matrix can be defined using URV factorization or SVD.\nGiven a URV factorization of matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n}\n\n\\mathbf{A} = \\mathbf{U}\n\\begin{bmatrix}\n\\mathbf{C} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\mathbf{V}^{T}\n\nthe pseudoinverse of \\mathbf{A} is defined as\n\n\\mathbf{A}^{\\dagger} = \\mathbf{V}\n\\begin{bmatrix}\n\\mathbf{C}^{-1} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\mathbf{U}^{T}.\n\nThe pseudoinverse of matrix \\mathbf{A} can also be stated as a matrix \\mathbf{A}^{\\dagger} that satisfies the following:\n\n\\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A},\n\n\n\\mathbf{A}^{\\dagger} \\mathbf{A} \\mathbf{A}^{\\dagger} = \\mathbf{A}^{\\dagger},\n\nwhere \\mathbf{A} \\mathbf{A}^{\\dagger} and \\mathbf{A}^{\\dagger} \\mathbf{A} are symmetric matrix:\n\n(\\mathbf{A} \\mathbf{A}^{\\dagger})^{T} = \\mathbf{A} \\mathbf{A}^{\\dagger},\n\n\n(\\mathbf{A}^{\\dagger} \\mathbf{A})^{T} = \\mathbf{A}^{\\dagger} \\mathbf{A}.\n\n\nProperties of pseudoinverse\n\nThe pseudoinverse \\mathbf{A}^{\\dagger} of \\mathbf{A} is unique.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction. Suppose that there are 2 pseudoinverse \\mathbf{B}, \\mathbf{C} for \\mathbf{A}.\n\n  \\begin{aligned}\n  \\mathbf{A} \\mathbf{B}\n  & = (\\mathbf{A} \\mathbf{C} \\mathbf{A}) \\mathbf{B}\n  & [\\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A}]\n  \\\\\n  & = (\\mathbf{C}^{T} \\mathbf{A}^{T}) (\\mathbf{B}^{T} \\mathbf{A}^{T})\n  & [(\\mathbf{A} \\mathbf{A}^{\\dagger})^{T} = \\mathbf{A} \\mathbf{A}^{\\dagger}]\n  \\\\\n  & = \\mathbf{C}^{T} (\\mathbf{A}^{T} \\mathbf{B}^{T} \\mathbf{A}^{T})\n  \\\\\n  & = \\mathbf{C}^{T} (\\mathbf{A} \\mathbf{B} \\mathbf{A})^{T}\n  \\\\\n  & = \\mathbf{C}^{T} \\mathbf{A}^{T}\n  \\\\\n  & = \\mathbf{A} \\mathbf{C}\n  \\\\\n  \\end{aligned}\n  \n\n  \\begin{aligned}\n  \\mathbf{B} \\mathbf{A}\n  & = \\mathbf{B} (\\mathbf{A} \\mathbf{C} \\mathbf{A})\n  & [\\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A}]\n  \\\\\n  & = (\\mathbf{A}^{T} \\mathbf{B}^{T}) (\\mathbf{A}^{T} \\mathbf{C}^{T})\n  & [(\\mathbf{A} \\mathbf{A}^{\\dagger})^{T} = \\mathbf{A} \\mathbf{A}^{\\dagger}]\n  \\\\\n  & = (\\mathbf{A}^{T} \\mathbf{B}^{T} \\mathbf{A}^{T}) \\mathbf{C}^{T}\n  \\\\\n  & = (\\mathbf{A} \\mathbf{B} \\mathbf{A})^{T} \\mathbf{C}^{T}\n  \\\\\n  & = \\mathbf{A}^{T} \\mathbf{C}^{T}\n  \\\\\n  & = \\mathbf{C} \\mathbf{A}\n  \\\\\n  \\end{aligned}\n  \nThus,\n\n  \\begin{aligned}\n  \\mathbf{B}\n  & = \\mathbf{B} \\mathbf{A} \\mathbf{B}\n  & [\\mathbf{A}^{\\dagger} \\mathbf{A} \\mathbf{A}^{\\dagger} = \\mathbf{A}^{\\dagger}]\n  \\\\\n  & = \\mathbf{C} \\mathbf{A} \\mathbf{B}\n  & [\\mathbf{C} \\mathbf{A} = \\mathbf{B} \\mathbf{A}]\n  \\\\\n  & = \\mathbf{C} \\mathbf{A} \\mathbf{C}\n  & [\\mathbf{A} \\mathbf{B} = \\mathbf{A} \\mathbf{C}]\n  \\\\\n  & = \\mathbf{C}\n  \\end{aligned}\n  \n\n\n\nGiven a full rank matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n}.\nIf m &gt; n, the left inverse of \\mathbf{A} is the pseudoinverse of \\mathbf{A} and is written as\n\n  (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T}.\n  \nIf m &lt; n, the right inverse of \\mathbf{A} is the pseudoinverse of \\mathbf{A} and is written as\n\n  \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} .\n  \nIf m = n, the inverse of \\mathbf{A} is the pseudoinverse of \\mathbf{A}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first prove that (\\mathbf{A}^{T} \\mathbf{A})^{-1} (when m &gt; n) and (\\mathbf{A} \\mathbf{A}^{T})^{-1} (when m &lt; n) exist.\nSince \\mathbf{A} is a full rank matrix,\n\n  \\text{rank} (\\mathbf{A}) = \\min (m, n).\n  \nAccording to the property of rank,\n\n  \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}) = \\text{rank} (\\mathbf{A} \\mathbf{A}^{T}) = \\text{rank} (\\mathbf{A}) = \\text{min} (m, n).\n  \nThus, \\mathbf{A}^{T} \\mathbf{A} (m &gt; n) and \\mathbf{A} \\mathbf{A}^{T} (m &lt; n) are non-singular.\n\nif m &gt; n, \\mathbf{A}^{T} \\mathbf{A} \\in \\mathbb{R}^{n \\times n} is full rank because \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}) = \\min (m, n) = n,\nif m &lt; n, \\mathbf{A} \\mathbf{A}^{T} \\in \\mathbb{R}^{m \\times m} is full rank because \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}) = \\min (m, n) = m.\n\nwe prove that (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} when (m &gt; n) is the pseudoinverse of \\mathbf{A}.\n\n  \\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} = \\mathbf{A} \\mathbf{I}_{n \\times n} = \\mathbf{A}\n  \n\n  \\mathbf{A}^{\\dagger} \\mathbf{A} \\mathbf{A}^{\\dagger} = (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} = \\mathbf{I}_{n \\times n} \\mathbf{A}^{\\dagger} = \\mathbf{A}^{\\dagger}\n  \n\n  \\mathbf{A}^{\\dagger} \\mathbf{A} = (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} = \\mathbf{I}_{n \\times n} = \\mathbf{A}^{T} \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} = \\mathbf{A}^{T} (\\mathbf{A}^{\\dagger})^{\\mathbf{T}}= (\\mathbf{A}^{\\dagger} \\mathbf{A})^{T}\n  \n\n  \\mathbf{A} \\mathbf{A}^{\\dagger} = \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} = (\\mathbf{A}^{\\dagger})^{T} \\mathbf{A}^{T} = (\\mathbf{A} \\mathbf{A}^{\\dagger})^{T}\n  \nThe case that \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} when (m &lt; n) can be proved similarly.\nThen we prove that (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} is the left inverse of \\mathbf{A} when m &gt; n:\n\n  \\mathbf{A}^{\\dagger} \\mathbf{A} = (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} = \\mathbf{I}_{n \\times n}.\n  \nThe case that \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} is the right inverse of \\mathbf{A} when m &lt; n can be proved similarly.\nThus we have proved that (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} (m &gt; n) and \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} (m &lt; n) are the left and right inverse of \\mathbf{A}.\nWhen m = n, we have both\n\n  \\mathbf{A} \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{A},\n  \n\n  \\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A},\n  \nbut since \\mathbf{A}^{-1} and \\mathbf{A}^{\\dagger} are both unique, it must be that\n\n  \\mathbf{A}^{-1} = \\mathbf{A}^{\\dagger}."
  },
  {
    "objectID": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#orthogonal-projection",
    "href": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#orthogonal-projection",
    "title": "11  Orthogonal and Affine Projection",
    "section": "Orthogonal projection",
    "text": "Orthogonal projection\nSuppose \\mathcal{M} is a subspace in the vector space \\mathcal{V}. Since \\mathcal{M}^{\\perp} is a orthogonal complementary subspace of \\mathcal{M}, we have \\mathcal{M} \\oplus \\mathcal{M}^{\\perp} = \\mathcal{V}.\nFor v \\in \\mathcal{V}, let v = m + n, where m \\in \\mathcal{M} and n \\in \\mathcal{M}^{\\perp}. We define a linear operator \\mathbf{P}_{\\mathcal{M}} \\in \\mathbb{C}^{n \\times n} such that\n\n\\mathbf{P}_{\\mathcal{M}} v = m.\n\nwhere\n\nm is the orthogonal projection of v onto \\mathcal{M} along \\mathcal{M}^{\\perp},\n\\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of v onto \\mathcal{M} along \\mathcal{M}^{\\perp}.\n\n\nProperties of orthogonal projection\nSuppose m is the orthogonal projection of v on the subspace \\mathcal{M}\n\nm = \\mathbf{P}_{\\mathcal{M}} v.\n\n\nm always exists and is unique.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\nOrthogonality of the difference.\n\n  \\langle v - m, x \\rangle = 0, \\quad \\forall x \\in \\mathcal{M}\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nSince n \\in \\mathcal{M}^{\\perp}, by definition of the orthogonal complement of subspaces,\n\n  \\langle n, x \\rangle = 0, \\quad \\forall x \\in \\mathcal{M}.\n  \nSince by definition v = m + n,\n\n  \\langle v - m, x \\rangle = 0, \\quad \\forall x \\in \\mathcal{M}.\n  \n\n\n\nClosest point theorem: m is the closest point in \\mathcal{M} to v in terms of the ip norm:\n\n  m = \\min_{x \\in \\mathcal{M}} \\lVert v - x \\rVert.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the property of orthogonal projection, the projection m always exists.\nThus, we can add and subtract m,\n\n  \\begin{aligned}\n  \\lVert v - x \\rVert^{2}\n  & = \\lVert v - m + m - x \\rVert^{2} \\quad \\forall x \\in \\mathcal{M}\n  \\\\\n  & = \\langle v - m + m - x, v - m + m - x \\rangle\n  \\\\\n  & = \\langle v - m, v - m + m - x \\rangle + \\langle m - x, v - m + m - x \\rangle\n  \\\\\n  & = \\lVert v - m \\rVert^{2} + \\langle v - m, m - x \\rangle + \\langle m - x, v - m \\rangle + \\lVert m - x \\rVert^{2}\n  \\\\\n  \\end{aligned}\n  \nSince m - x \\in \\mathcal{M}, according to the property of orthogonal projection,\n\n  \\langle v - m, m - x \\rangle = \\langle m - x, v - m \\rangle = 0.\n  \nThus,\n\n  \\begin{aligned}\n  \\lVert v - x \\rVert^{2}\n  & = \\lVert v - m \\rVert^{2} + \\lVert m - x \\rVert^{2}\n  \\\\\n  \\lVert v - x \\rVert^{2}\n  & \\geq \\lVert v - m \\rVert^{2}\n  & [\\lVert m - x \\rVert^{2} \\geq 0]\n  \\end{aligned}\n  \nwhich shows that\n\n  \\lVert v - m \\rVert^{2}\n  \nminimizes the distances between v to all vectors x in the subspace \\mathcal{M}."
  },
  {
    "objectID": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#orthogonal-projection-matrix",
    "href": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#orthogonal-projection-matrix",
    "title": "11  Orthogonal and Affine Projection",
    "section": "Orthogonal projection matrix",
    "text": "Orthogonal projection matrix\nIf \\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of v onto the subspace \\mathcal{M}, and the columns of \\mathbf{M} \\in \\mathbb{C}^{n \\times r} are r bases for \\mathcal{M}, then\n\n\\mathbf{P}_{\\mathcal{M}} = \\mathbf{M} (\\mathbf{M}^{H} \\mathbf{M})^{-1} \\mathbf{M}^{H}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\text{rank} (\\mathbf{M}^{H} \\mathbf{H}) = \\text{rank} (\\mathcal{M}) = r and \\mathbf{M}^{H} \\mathbf{H} \\in \\mathbb{C}^{r \\times r}, according to the property of rank, (\\mathbf{M}^{H} \\mathbf{H})^{-1} exists.\nSuppose the columns of \\mathbf{N} \\in \\mathbb{C}^{n \\times s} are orthonormal bases for \\mathcal{M}^{\\perp}, where s = n - r according to the property of complementary subspaces.\nThus, we can construct the below matrix\n\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}.\n\nAccording to the definition of complementary subspaces, \\mathbf{N}^{T} \\mathbf{M} = 0 and \\mathbf{M}^{T} \\mathbf{N} = 0.\nThus,\n\n\\begin{aligned}\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{N} \\\\\n\\end{bmatrix}\n& =\n\\begin{bmatrix}\n    \\mathbf{I}_{r \\times r} & \\mathbf{0} \\\\\n    \\mathbf{0} & \\mathbf{I}_{s \\times s} \\\\\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{N} \\\\\n\\end{bmatrix}^{-1}\n& =\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}.\n\\end{aligned}\n\nSince \\mathcal{M} and \\mathcal{M}^{\\perp} are complementary subspaces, by the definition of projector,\n\n\\begin{aligned}\n\\mathbf{P}_{\\mathcal{M}}\n& =\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{N}\n\\end{bmatrix}^{-1} \\\\\n& =\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}\n\\\\\n& = \\mathbf{M} (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H}.\n\\end{aligned}\n\n\n\n\n\nProperties of orthogonal projection matrix\n\n\\mathbf{P} is a orthogonal projection matrix if and only if\n\n  R (\\mathbf{P}) = N (\\mathbf{P})^{\\perp}.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\n\\mathbf{P} is a orthogonal projection matrix if and only if\n\n  \\mathbf{P}^{H} = \\mathbf{P}.\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the orthogonal decomposition theorem,\n\n  \\begin{aligned}\n  R (\\mathbf{P})^{\\perp}\n  & = N (\\mathbf{P}^{H})\n  \\\\\n  R (\\mathbf{P})\n  & = N (\\mathbf{P}^{H})^{\\perp}.\n  \\\\\n  \\end{aligned}\n  \nAccording to the property of the orthogonal projection matrix,\n\n  \\begin{aligned}\n  N (\\mathbf{P})^{\\perp}\n  & = N (\\mathbf{P}^{H})^{\\perp}\n  \\\\\n  \\mathbf{P}\n  & = \\mathbf{P}^{H}.\n  \\\\\n  \\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#application-least-square-problem",
    "href": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#application-least-square-problem",
    "title": "11  Orthogonal and Affine Projection",
    "section": "Application: least square problem",
    "text": "Application: least square problem\nConsider the problem of solving a system of linear equation for \\mathbf{x} \\in \\mathbb{C}^{n} given \\mathbf{y} \\in \\mathbb{C}^{m} and \\mathbf{A} \\in \\mathbb{C}^{m \\times n},\n\n\\mathbf{y} = \\mathbf{A} \\mathbf{x}.\n\nThis problem has a solution only when \\mathbf{y} \\in R (\\mathbf{A}). When it has no solution, the objective is changed to solve the least square problem\n\n\\mathbf{x}^{*} = \\min_{\\mathbf{x} \\in \\mathbb{C}^{n}} \\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2}^{2}\n\nso that \\mathbf{A} \\mathbf{x} can be as close to \\mathbf{y} as possible.\nSolving the least square problem is the same as solving an orthogonal projection problem,\n\n\\begin{aligned}\n\\mathbf{x}^{*}\n& = \\min_{\\mathbf{x} \\in \\mathbb{C}^{n}} \\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2}^{2}\n\\\\\n& = \\min_{\\mathbf{x} \\in \\mathbb{C}^{n}} \\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2}\n& [\\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2} \\geq 0]\n\\\\\n\\mathbf{z}^{*}\n& = \\min_{\\mathbf{z} \\in R (\\mathbf{A})} \\lVert \\mathbf{y} - \\mathbf{z} \\rVert_{2}\n& [\\mathbf{z} = \\mathbf{A} \\mathbf{x}, \\mathbf{z}^{*} = \\mathbf{A} \\mathbf{x}^{*}].\n\\end{aligned}\n\nwhich is the problem of finding the closest point of \\mathbf{y} on R (\\mathbf{A}),\n\n\\begin{aligned}\n\\mathbf{z}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}\n\\\\\n\\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}.\n\\end{aligned}\n\nWe can then deduce the system of normal equations:\n\n\\mathbf{A} \\mathbf{x}^{*} = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y} \\iff \\mathbf{A}^{H} \\mathbf{A} \\mathbf{x}^{*} = \\mathbf{A}^{H} \\mathbf{y}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst multiplying both ends by \\mathbf{P}_{R (\\mathbf{A})} and use the projector property,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}\n\\\\\n\\mathbf{P}_{R (\\mathbf{A})} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})}^{2} \\mathbf{y}\n\\\\\n\\mathbf{P}_{R (\\mathbf{A})} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}.\n& [\\mathbf{P}_{R (\\mathbf{A})}^{2} = \\mathbf{P}_{R (\\mathbf{A})}]\n\\\\\n\\end{aligned}\n\nThen,\n\n\\begin{aligned}\n\\mathbf{P}_{R (\\mathbf{A})} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}.\n\\\\\n\\mathbf{P}_{R (\\mathbf{A})} (\\mathbf{A} \\mathbf{x}^{*} - \\mathbf{y})\n& = 0\n\\\\\n\\end{aligned}\n\nwhich shows that\n\n\\mathbf{A} \\mathbf{x}^{*} - \\mathbf{y} \\in N (\\mathbf{P}_{R (\\mathbf{A})}).\n\nAccording to the property of the orthogonal projection matrix,\n\nN (\\mathbf{P}_{R (\\mathbf{A})}) = R (\\mathbf{A})^{\\perp} = N (\\mathbf{A}^{H}),\n\nwhich means\n\n\\begin{aligned}\n\\mathbf{A}^{H} (\\mathbf{A} \\mathbf{x}^{*} - \\mathbf{y})\n& = 0\n\\\\\n\\mathbf{A}^{H} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{A}^{H} \\mathbf{y}.\n\\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#sec-affine-space",
    "href": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#sec-affine-space",
    "title": "11  Orthogonal and Affine Projection",
    "section": "Affine space",
    "text": "Affine space\nA set of vectors in the vector space \\mathcal{V} forms an affine space \\mathcal{A} if they are the sums of the vectors in a subspace \\mathcal{M} \\subset \\mathcal{V} and a non-zero vector v \\in \\mathcal{V},\n\n\\mathcal{A} = v + \\mathcal{M}.\n\nThat is, all vectors in \\mathcal{A} are sums of vectors in \\mathcal{M} and v.\n\n\\mathcal{A} is a subspace as it does NOT necessarily contain 0 vector.\n\\mathcal{A} can visualized as the subspace \\mathcal{M} translated away from origin through v."
  },
  {
    "objectID": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#sec-affine-projection",
    "href": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#sec-affine-projection",
    "title": "11  Orthogonal and Affine Projection",
    "section": "Affine projection",
    "text": "Affine projection\nAlthough affine spaces are not a subspaces, the concept of orthogonal projection can also be applied to affine spaces.\nGiven a vector b \\in \\mathcal{V} and an affine space \\mathcal{A}, the affine projection a \\in \\mathcal{A} is the orthogonal projection of b onto the affine space \\mathcal{A} and can be expressed as\n\na = v + \\mathbf{P}_{\\mathcal{M}} (b - v),\n\nwhere \\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of \\mathcal{M}.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the affine space \\mathcal{A} is the set of vectors translated from \\mathcal{M} through the vector v, the affine projection a of b onto \\mathcal{A} is the translated version of orthogonal projection of the translated version of b onto the translated version of \\mathcal{A}.\nThus, the affine projection onto \\mathcal{A} is a orthogonal projection onto \\mathcal{M} with a translated input and output.\nBy the definition of the orthogonal projection,\n\na - v = \\mathbf{P}_{\\mathcal{M}} (b - v)\n\nwhere a - v is the translated version of a, b - v the translated version of b, and \\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of subspace \\mathcal{M}.\nThus, the affine projection a of b onto \\mathcal{A} can be expressed as\n\na = v + \\mathbf{P}_{\\mathcal{M}} (b - v)."
  },
  {
    "objectID": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#hyperplanes",
    "href": "Linear Algebra/11_Orthogonal_and_Affine_Projection.html#hyperplanes",
    "title": "11  Orthogonal and Affine Projection",
    "section": "Hyperplanes",
    "text": "Hyperplanes\nAn affine space \\mathcal{H} = \\mathbf{v} + \\mathcal{M} \\subseteq \\mathbb{R}^{n} for which \\text{dim} (\\mathcal{M}) = n - 1 is called a hyperplane, and is usually expressed as the set\n\n\\mathcal{H} = \\{ \\mathbf{x} | \\mathbf{w}^{T} \\mathbf{x} = \\beta \\}\n\nwhere \\beta is a scalar and \\mathbf{w} is a non-zero vector.\nIn this case, the hyperplane can be viewed as the subspace\n\n\\mathcal{M} = \\mathbf{w}^{\\perp}\n\ntranslated by the vector\n\n\\mathbf{v} = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe set \\mathcal{H} contains the general solutions of the linear system \\mathbf{w}^{T} \\mathbf{x} = \\beta.\nA particular solution to the linear system is\n\n\\begin{aligned}\n\\mathbf{x}\n& = (\\mathbf{w}^{T})^{-1} \\beta\n\\\\\n& = (\\mathbf{w}^{T})^{T} (\\mathbf{w}^{T} (\\mathbf{w}^{T})^{T})^{-1} \\beta\n\\\\\n& = \\mathbf{w} (\\mathbf{w}^{T} \\mathbf{w})^{-1} \\beta\n\\\\\n& = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n& [\\beta, \\mathbf{w}^{T} \\mathbf{w} \\text{ are scalars}].\n\\end{aligned}\n\nAccording to the orthogonal decomposition theorem, the general solution to the linear system can be expressed as\n\n\\begin{aligned}\nN (\\mathbf{w}^{T})\n& = R (\\mathbf{w})^{\\perp}\n\\\\\n& = \\{ \\mathbf{x} \\mid \\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0, \\forall \\mathbf{y} \\in R (\\mathbf{w}) \\}\n& [\\text{def of perp}]\n\\\\\n& = \\{ \\mathbf{x} \\mid \\langle \\mathbf{x}, \\alpha \\mathbf{w} \\rangle = 0, \\forall \\alpha \\in \\mathbb{R} \\}\n& [\\text{def of } R (\\mathbf{w})]\n\\\\\n& = \\{ \\mathbf{x} \\mid \\alpha \\langle \\mathbf{x}, \\mathbf{w} \\rangle = 0, \\forall \\alpha \\in \\mathbb{R} \\}\n& [\\text{def of } \\langle \\cdot, \\cdot \\rangle]\n\\\\\n& = \\{ \\mathbf{x} \\mid \\langle \\mathbf{x}, \\mathbf{w} \\rangle = 0 \\}\n\\\\\n& = \\mathbf{w}^{\\perp}.\n\\end{aligned}\n\nSince the general solution of any linear system is the sum of a particular solution and the general solution of the associated homogeneous equation, the set of general solutions is expressed as\n\n\\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w} + \\mathbf{w}^{\\perp}.\n\nThus,\n\n\\mathcal{H} = \\mathbf{v} + \\mathcal{M},\n\nwhere\n\n\\mathbf{v} = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w},\n\n\n\\mathcal{M} = \\mathbf{w}^{\\perp}.\n\nTODO: explain the general solution of any linear system.\n\n\n\nThe orthogonal projection \\mathbf{a} of a point \\mathbf{b} \\in \\mathbb{R}^{n} onto the hyperplane \\mathcal{H} is given by\n\n\\mathbf{a} = \\mathbf{b} - \\left(\n    \\frac{\n        \\mathbf{w}^{T} \\mathbf{b} - \\beta\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\right) \\mathbf{w}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince we know that \\mathcal{H} is an affine space with \\mathcal{W} = \\mathbf{w}^{\\perp} and \\mathbf{v} = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}, the affine projection can be expressed as\n\n\\begin{aligned}\n\\mathbf{a}\n& = \\mathbf{v} + \\mathbf{P}_{\\mathcal{M}} (\\mathbf{b} - \\mathbf{v}),\n\\\\\n& = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w} + \\mathbf{P}_{\\mathbf{w}^{\\perp}} \\left(\n    \\mathbf{b} - \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\right),\n\\end{aligned}\n\nAccording to the property of projection matrix,\n\n\\mathbf{P}_{\\mathbf{w}^{\\perp}} = \\mathbf{I} - \\mathbf{P}_{\\mathbf{w}}.\n\nDenoted the basis of subspace \\mathcal{M} as the columns of matrix \\mathbf{M}, the orthogonal projection matrix onto \\mathcal{M} is\n\n\\begin{aligned}\n\\mathbf{P}_{\\mathcal{M}}\n& = \\mathbf{M} (\\mathbf{M}^{T} \\mathbf{M})^{-1} \\mathbf{M}^{T}\n\\\\\n\\mathbf{P}_{\\mathbf{w}}\n& = \\mathbf{w} (\\mathbf{w}^{T} \\mathbf{w})^{-1} \\mathbf{w}^{T}\n\\\\\n\\mathbf{P}_{\\mathbf{w}}\n& = \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}}.\n\\end{aligned}\n\nThus,\n\n\\mathbf{P}_{\\mathbf{w}^{\\perp}} = \\mathbf{I} - \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}}.\n\nPlugging it back into the top equation\n\n\\begin{aligned}\n\\mathbf{a}\n& = \\frac{\n    \\beta\n}{\n    \\mathbf{w}^{T} \\mathbf{w}\n} \\mathbf{w} + \\mathbf{P}_{\\mathbf{w}^{\\perp}} \\left(\n    \\mathbf{b} - \\frac{\n        \\beta\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    } \\mathbf{w}\n\\right),\n\\\\\n& = \\frac{\n    \\beta\n}{\n    \\mathbf{w}^{T} \\mathbf{w}\n} \\mathbf{w} + \\left(\n    \\mathbf{I} - \\frac{\n        \\mathbf{w} \\mathbf{w}^{T}\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\right) \\left(\n    \\mathbf{b} - \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\right),\n\\\\\n& = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w} + \\left(\n    \\mathbf{b}\n    - \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n    - \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{b}\n    + \\frac{\\mathbf{w}^{T} \\mathbf{w} \\beta}{\\mathbf{w}^{T} \\mathbf{w} \\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\right)\n\\\\\n& = \\mathbf{b}\n- \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{b}\n+ \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/12_Determinants_and_Eigensystems.html#determinants",
    "href": "Linear Algebra/12_Determinants_and_Eigensystems.html#determinants",
    "title": "12  Determinants and Eigensystems",
    "section": "Determinants",
    "text": "Determinants\nFor an matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n}, the determinant of A is defined to be the scalar\n\n\\text{det} (\\mathbf{A}) = \\lvert \\mathbf{A} \\rvert = \\sum_{p \\in \\mathcal{P}} \\sigma (p) \\prod_{i=1}^{n} a_{i, p_{i}}\n\nwhere\n\n\\mathcal{P} is the set of all permutations of the set of (1, 2, \\dots, n),\n\\sigma (p) is the parity of the permutation p.\n\nNote that each term a1p1a2p2 · · · anpn in (6.1.1) contains exactly one entry from each row and each column of A."
  },
  {
    "objectID": "Linear Algebra/12_Determinants_and_Eigensystems.html#eigensystems",
    "href": "Linear Algebra/12_Determinants_and_Eigensystems.html#eigensystems",
    "title": "12  Determinants and Eigensystems",
    "section": "Eigensystems",
    "text": "Eigensystems\nFor a square matrix \\mathbf{A} \\in \\mathbb{R}^{n \\times n}, (\\lambda, \\mathbf{x}) is an eigenpair for \\mathbf{A} if\n\n\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}\n\nwhere\n\nthe scalar \\lambda is an eigenvalue for \\mathbf{A},\nthe non-zero vector \\mathbf{x} is an eigenvector associated with \\lambda for \\mathbf{A}."
  },
  {
    "objectID": "Linear Algebra/12_Determinants_and_Eigensystems.html#eigenspace",
    "href": "Linear Algebra/12_Determinants_and_Eigensystems.html#eigenspace",
    "title": "12  Determinants and Eigensystems",
    "section": "Eigenspace",
    "text": "Eigenspace\nGiven a eigenvalue \\lambda,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x}\n& = \\lambda \\mathbf{x}\n\\\\\n(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{x}\n& = 0\n\\\\\n\\mathbf{x}\n& \\in N (\\mathbf{A} - \\lambda \\mathbf{I}),\n\\\\\n\\end{aligned}\n\nwhich shows that the set\n\n\\{ \\mathbf{x} \\neq 0 \\mid \\mathbf{x} \\in N (\\mathbf{A} - \\lambda \\mathbf{I}) \\}\n\nis the set of all eigenvectors associated with the \\lambda, and N (\\mathbf{A} - \\lambda \\mathbf{I}) is an eigenspace for \\mathbf{A}.\nBecause of \\mathbf{x} \\neq \\mathbf{0} and rank-nullity theorem,\n\nN (\\mathbf{A} - \\lambda \\mathbf{I}) \\neq \\{ \\mathbf{0} \\}\n\\Rightarrow \\text{rank} (\\mathbf{A} - \\lambda \\mathbf{I}) &lt; n\n\nwhich, according to the property of rank, indicates that \\mathbf{A} - \\lambda \\mathbf{I} is a singular matrix, that is,\n\n\\text{det} (\\mathbf{A} - \\lambda \\mathbf{I}) = 0."
  },
  {
    "objectID": "Linear Algebra/12_Determinants_and_Eigensystems.html#characteristic-polynomial-and-equation",
    "href": "Linear Algebra/12_Determinants_and_Eigensystems.html#characteristic-polynomial-and-equation",
    "title": "12  Determinants and Eigensystems",
    "section": "Characteristic polynomial and equation",
    "text": "Characteristic polynomial and equation\nThe characteristic polynomial of \\mathbf{A} \\in \\mathbb{R}^{n \\times n} is a polynomial function of \\lambda\n\np (\\lambda) = \\text{det} (\\mathbf{A} - \\lambda \\mathbf{I}).\n\n\nThe degree (highest exponent in the expression) of p (\\lambda) is n,\nthe leading term in p (\\lambda) is (-1)^{n} \\lambda^{n}.\n\nThe characteristic equation for \\mathbf{A} is\n\np (\\lambda) = 0.\n\n\nThe eigenvalues of \\mathbf{A} are the solutions of the characteristic equation.\nThus, \\mathbf{A} has n eigenvalues, but some may be complex numbers (even if the entries of \\mathbf{A} are real numbers), and some eigenvalues may be repeated."
  },
  {
    "objectID": "Linear Algebra/12_Determinants_and_Eigensystems.html#multiplicities",
    "href": "Linear Algebra/12_Determinants_and_Eigensystems.html#multiplicities",
    "title": "12  Determinants and Eigensystems",
    "section": "Multiplicities",
    "text": "Multiplicities\nThe set of all distinct eigenvalues, denoted by \\sigma (\\mathbf{A}), is the spectrum of \\mathbf{A}. Let \\lambda_{1}, \\dots, \\lambda_{n} \\in \\sigma (\\mathbf{A}) be the spectrum of \\mathbf{A}. Then\n\nthe algebraic multiplicity of an eigenvalue \\lambda_{i} is the number of times it appears as the root of p (\\lambda),\n\n\\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}) = a_{i} \\iff \\dots + (\\lambda - \\lambda_{i})^{a_{i}} + \\dots = 0.\n\nThat is, the algebraic multiplicity of \\lambda_{i} is the number of times \\lambda_{i} has repeated in all eigenvalues.\nthe geometric multiplicity of an eigenvalue \\lambda_{i} is the number of the dimension of eigenspace associated with \\lambda_{i},\n\n  \\text{geo mult}_{\\mathbf{A}} (\\lambda_{i}) = \\text{dim} N (\\mathbf{A} - \\lambda_{i} \\mathbf{I}).\n  \nThat is, the geometric multiplicity of \\lambda_{i} is the number of linearly independent eigenvectors associated with \\lambda_{i}.\n\n\nSpecial multiplicities\n\nWhen \\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}) = 1, \\lambda_{i} is called a simple eigenvalue, since there can only be one unique eigenvector associated with this eigenvalue.\nEigenvalues such that \\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}) = \\text{geo mult}_{\\mathbf{A}} (\\lambda_{i}) are called semi-simple eigenvalues of A, as all eigenvectors associated with the eigenvalues that have the value of \\lambda_{i} are linearly independent.\n\n\n\nProperties of multiplicities\n\nFor every \\mathbf{A} \\in \\mathbb{C}^{n \\times n}, and for each \\lambda_{i} \\in \\sigma(\\mathbf{A}),\n\n  \\text{geo mult}_{\\mathbf{A}} (\\lambda_{i}) \\leq \\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}).\n  \n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/12_Determinants_and_Eigensystems.html#independent-eigenvectors",
    "href": "Linear Algebra/12_Determinants_and_Eigensystems.html#independent-eigenvectors",
    "title": "12  Determinants and Eigensystems",
    "section": "Independent eigenvectors",
    "text": "Independent eigenvectors\nLet \\{ \\lambda_{1}, \\dots, \\lambda_{k} \\} be a set of distinct eigenvalues for \\mathbf{A}.\n\nIf \\{ (\\lambda_{1}, \\mathbf{x}_{1}), \\dots, (\\lambda_{k}, \\mathbf{x}_{k}) \\} is a set of eigenpairs for \\mathbf{A}, then \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k} \\} is a linearly independent set.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction.\nSuppose \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k} \\} is linearly dependent, but has been reordered so that the first r eigenvectors \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k} \\} is linearly independent.\nThus, the r + 1th eigenvector is\n\n  \\mathbf{x}_{r + 1} = \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i}.\n  \nMultiply the both sides by the \\mathbf{A} - \\lambda_{r + 1} \\mathbf{I} to get\n\n  (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\mathbf{x}_{r + 1} = (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i}.\n  \nSince (\\lambda_{r + 1}, x_{r + 1}) is an eigenpair,\n\n  \\begin{aligned}\n  (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\mathbf{x}_{r + 1}\n  & = 0\n  \\\\\n  (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i}\n  & = 0\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\mathbf{x}_{i}\n  & = 0\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\mathbf{A} \\mathbf{x}_{i} - \\lambda_{r + 1} \\mathbf{x}_{i})\n  & = 0\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\lambda_{i} \\mathbf{x}_{i} - \\lambda_{r + 1} \\mathbf{x}_{i})\n  & = 0\n  & [\\mathbf{A} \\mathbf{x}_{i} = \\lambda_{i} \\mathbf{x}_{i}]\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\lambda_{i}  - \\lambda_{r + 1}) \\mathbf{x}_{i}\n  & = 0\n  \\\\\n  \\end{aligned}\n  \nSince \\{ \\mathbf{1}, \\dots, \\mathbf{x}_{r} \\} are linearly independent,\n\n  \\alpha_{i} (\\lambda_{i}  - \\lambda_{r + 1}) = 0, \\forall i = 1, \\dots, r\n  \nbut since we assume \\lambda_{i}, \\dots, \\lambda_{r + 1}, \\dots, \\lambda_{n} are different,\n\n  \\lambda_{i} \\neq \\lambda_{r + 1} \\Rightarrow \\alpha_{i} = 0, \\forall i = 1, \\dots, r,\n  \nwhich means\n\n  \\mathbf{x}_{r + 1} = \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i} = 0.\n  \nHowever, eigenvectors are all non-zero vectors and thus an contradiction occurs.\n\n\n\nIf \\mathcal{B}_{i} is a basis for N (\\mathbf{A} - \\lambda_{i} \\mathbf{I}), then \\mathcal{B} = \\mathcal{B}_{1} \\cup \\dots, \\cup \\mathcal{B}_{k} is a linearly independent set.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Linear Algebra/13_Similarity_and_Diagonalization.html#similarity",
    "href": "Linear Algebra/13_Similarity_and_Diagonalization.html#similarity",
    "title": "13  Similarity and Diagonalization",
    "section": "Similarity",
    "text": "Similarity\nTwo square matrices \\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{n \\times n} are similar if there exists a non-singular matrix \\mathbf{P} such that\n\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} = \\mathbf{B},\n\n\n\\mathbf{A} = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}.\n\nSimilar matrices have the same eigenvalues.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose \\lambda_{1}, \\dots, \\lambda_{n} are eigenvalues of \\mathbf{B}.\n\n\\begin{aligned}\n\\text{det} (\\mathbf{B} - \\lambda \\mathbf{I})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} - \\lambda \\mathbf{I})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} - \\lambda \\mathbf{P}^{-1} \\mathbf{P})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1} (\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{P})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1}) \\text{det} (\\mathbf{A} - \\lambda \\mathbf{I}) \\text{det} (\\mathbf{P})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1}) \\text{det} (\\mathbf{P}) \\text{det} (\\mathbf{A} - \\lambda \\mathbf{I})\n& = 0\n\\\\\n\\text{det} (\\mathbf{A} - \\lambda \\mathbf{I})\n& = 0\n& [\\text{det} (\\mathbf{P}^{-1}) = \\frac{1}{\\text{det} (\\mathbf{P})}]\n\\\\\n\\end{aligned}\n\n\n\n\n\nOrthogonally (unitarily) similar\nTwo matrices \\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{n \\times n} are unitarily similar if there exists an unitary matrix \\mathbf{U} such that\n\n\\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{B},\n\nwhich, according to the property of orthogonal matrix, can also be written as\n\n\\mathbf{U}^{T} \\mathbf{A} \\mathbf{U} = \\mathbf{B}."
  },
  {
    "objectID": "Linear Algebra/13_Similarity_and_Diagonalization.html#diagonalization",
    "href": "Linear Algebra/13_Similarity_and_Diagonalization.html#diagonalization",
    "title": "13  Similarity and Diagonalization",
    "section": "Diagonalization",
    "text": "Diagonalization\nA square matrix \\mathbf{A} \\in \\mathbb{R}^{n \\times n} is diagonalizable if \\mathbf{A} is similar to a diagonal matrix:\n\n\\mathbf{A} = \\mathbf{P} \\Lambda \\mathbf{P}^{-1}\n\nwhere \\Lambda is a diagonal matrix.\n\nDiagonalization and eigensystems\nA square matrix \\mathbf{A} \\in \\mathbb{R}^{n \\times n} is diagonalizable if and only if \\mathbf{A} has n linearly independent eigenvectors. That is,\n\n\\mathbf{A} = \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{-1}\n\nor\n\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} = \\mathbf{\\Lambda},\n\nwhere\n\nthe columns of \\mathbf{P} \\in \\mathbb{R}^{n \\times n} are n linearly independent eigenvectors,\nthe diagonal values of \\mathbf{\\Lambda} are corresponding eigenvalues.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first prove that If \\mathbf{A} is diagonalizable, then \\mathbf{A} has n linearly independent eigenvectors.\nFirst note that\n\n\\begin{aligned}\n\\mathbf{A}\n& = \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{-1}\n\\\\\n\\mathbf{A} \\mathbf{P}\n& = \\mathbf{P} \\mathbf{\\Lambda}\n\\\\\n\\end{aligned}\n\nSince \\mathbf{\\Lambda} is a diagonal matrix with \\lambda_{1}, \\dots, \\lambda_{n} on its diagonal,\n\n\\mathbf{A} \\mathbf{P}_{*, i} = \\mathbf{P}_{*, i} \\lambda_{i} = \\lambda_{i} \\mathbf{P}_{*, i}.\n\nThus, (\\lambda, \\mathbf{P}_{*, i}) is an eigenpair of \\mathbf{A}, and \\mathbf{A} has n such eigenpairs.\nSince \\mathbf{P} is non-singular (full-rank), \\mathbf{P} has independent columns. Thus, \\mathbf{A} has n independent eigenvectors.\nThen we prove that if \\mathbf{A} has n linearly independent eigenvectors, then \\mathbf{A} is diagonalizable.\nAssume the columns of \\mathbf{P} \\in \\mathbf{C}^{n \\times n} are the eigenvectors of \\mathbf{A}, and \\lambda_{1}, \\dots, \\lambda_{n} are corresponding eigenvalues,\n\n\\mathbf{A} \\mathbf{P}_{*, i} = \\lambda_{i} \\mathbf{P}_{*, i} \\quad \\forall i = 1, \\dots, n.\n\nBy rewriting \\lambda_{1}, \\dots, \\lambda_{n} as diagonals for the diagonal matrix \\mathbf{\\Lambda},\n\n\\mathbf{A} \\mathbf{P} = \\mathbf{P} \\mathbf{\\Lambda}.\n\nSince the columns of the \\mathbf{P} are linearly independent, \\mathbf{P} has full rank and thus there exists the inverse of \\mathbf{P}\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{P}\n& = \\mathbf{P} \\mathbf{\\Lambda}\n\\\\\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P}\n& = \\mathbf{\\Lambda}\n\\end{aligned}\n\nwhich shows that \\mathbf{A} is similar to \\mathbf{\\Lambda} and thus is diagonalizable.\n\n\n\n\n\nDiagonalizability and multiplicities\nTODO"
  },
  {
    "objectID": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#normal-matrices",
    "href": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#normal-matrices",
    "title": "14  Normal and Positive Definite Matrices",
    "section": "Normal Matrices",
    "text": "Normal Matrices\nThe square real (complex) matrix \\mathbf{A} is a normal matrix if and only if\n\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H}.\n\n\nProperties of normal matrices\n\nOrthogonal (unitary) matrices are normal.\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the property of unitary matrices, a matrix \\mathbf{U} is unitary if and only if\n\n  \\mathbf{U}^{-1} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{-1} = \\mathbf{I}.\n  \nThus, unitary matrices are normal.\n\n\n\nDiagonal matrices are normal.\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\mathbf{D} \\in \\mathbb{C}^{n \\times n} as a diagonal matrix with d_{1}, \\dots, d_{n} in its diagonal.\n\n  \\mathbf{D}^{H} \\mathbf{D} = \\sum_{i=1}^{n} d_{i}^{2} = \\mathbf{D} \\mathbf{D}^{H}.\n  \n\n\n\nUnitary similarity preserves normality. That is, if \\mathbf{A} is a normal matrix and is unitarily similar to \\mathbf{B}\n\n  \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{B}\n  \nor\n\n  \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U} = \\mathbf{B},\n  \nthen \\mathbf{B} is also a normal matrix.\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe goal is to prove\n\n  \\mathbf{B}^{H} \\mathbf{B} = \\mathbf{B} \\mathbf{B}^{H}.\n  \nFirst we expand \\mathbf{B}^{H} \\mathbf{B} to have\n\n  \\begin{aligned}\n  \\mathbf{B}^{H} \\mathbf{B}\n  & = (\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U})^{H} (\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U})\n  \\\\\n  & = \\mathbf{U} \\mathbf{A}^{H} \\mathbf{U}^{H} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{U} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  & [TODO]\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{I} \\mathbf{A} \\mathbf{U}\n  & [\\mathbf{U} \\mathbf{U}^{H} = \\mathbf{U} \\mathbf{U}^{-1} = \\mathbf{I}]\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{A} \\mathbf{U}.\n  \\end{aligned}\n  \nSince \\mathbf{A} is a normal matrix,\n\n  \\mathbf{A} \\mathbf{A}^{H} = \\mathbf{A}^{H} \\mathbf{A}.\n  \nContinue from the derivation above,\n\n  \\begin{aligned}\n  \\mathbf{B}^{H} \\mathbf{B}\n  & = \\mathbf{U}^{H} \\mathbf{A} \\mathbf{A}^{H} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{I} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{U} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U} \\mathbf{A}^{H} \\mathbf{U}^{H} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  & [TODO]\n  \\\\\n  & = (\\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{U})^{H} (\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U})\n  \\\\\n  & = \\mathbf{B} \\mathbf{B}^{H}.\n  \\end{aligned}"
  },
  {
    "objectID": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#unitary-diagonalization",
    "href": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#unitary-diagonalization",
    "title": "14  Normal and Positive Definite Matrices",
    "section": "Unitary diagonalization",
    "text": "Unitary diagonalization\nA matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n} is normal if and only if \\mathbf{A} is unitarily similar to a diagonal matrix\n\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H} \\iff \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{\\Lambda}.\n\nwhere \\mathbf{U} is a unitary matrix and \\mathbf{\\Lambda} is a diagonal matrix.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first prove that\n\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H} \\Rightarrow \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{\\Lambda}.\n\nTODO\nThen we prove that\n\n\\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{\\Lambda} \\Rightarrow \\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H}.\n\nAccording to the property of unitary matrices, unitary similarity preserves normality.\nAlso according to property of unitary matrices, all diagonal matrices are normal.\nThus, \\mathbf{A} is normal because \\mathbf{A} is unitarily similar to a diagonal matrix, which is always normal.\n\n\n\nSince the columns of \\mathbf{U} are eigenvectors of \\mathbf{A} and are orthonormal to each other the columns of \\mathbf{U} must be a complete orthonormal set of eigenvectors for \\mathbf{A}, and the diagonal entries of \\mathbf{\\Lambda} are the associated eigenvalues."
  },
  {
    "objectID": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#hermitian-symmetric-matrices",
    "href": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#hermitian-symmetric-matrices",
    "title": "14  Normal and Positive Definite Matrices",
    "section": "Hermitian (symmetric) matrices",
    "text": "Hermitian (symmetric) matrices\nA square complex (real) matrix \\mathbf{A} is hermitian (symmetric) if and only if\n\n\\mathbf{A}^{H} = \\mathbf{A},\n\nwhich implies a hermitian (symmetric) matrix is a normal matrix.\nAll eigenvalues of Hermitian matrices are real.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose (\\lambda, \\mathbf{v}) is a eigenpair for the Hermitian matrix \\mathbf{A}.\n\n\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}.\n\nMultiplying both sides by \\mathbf{v}^{H} on the left to get,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{v}\n& = \\lambda \\mathbf{v}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n& = \\lambda \\mathbf{v}^{H} \\mathbf{v}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n& = \\lambda \\lVert \\mathbf{v} \\rVert^{2}.\n\\\\\n\\end{aligned}\n\nAlternatively we can take the transpose conjugate of both sides, and then multiply both sides by \\mathbf{v} on the right to get,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{v}\n& = \\lambda \\mathbf{v}\n\\\\\n(\\mathbf{A} \\mathbf{v})^{H}\n& = (\\lambda \\mathbf{v})^{H}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A}^{H}\n& = \\lambda^{*} \\mathbf{v}^{H}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A}^{H} \\mathbf{v}\n& = \\lambda^{*} \\mathbf{v}^{H} \\mathbf{v}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A}^{H} \\mathbf{v}\n& = \\lambda^{*} \\lVert \\mathbf{v} \\rVert^{2}\n\\\\\n\\end{aligned}\n\nSince \\mathbf{A} is a hermitian matrix\n\n\\begin{aligned}\n\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n& = \\mathbf{v}^{H} \\mathbf{A}^{H} \\mathbf{v}\n\\\\\n\\lambda \\lVert \\mathbf{v} \\rVert^{2}\n& = \\lambda^{*} \\lVert \\mathbf{v} \\rVert^{2}\n\\\\\n\\lambda\n& = \\lambda^{*},\n\\end{aligned}\n\nwhich means \\lambda is a real number."
  },
  {
    "objectID": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#rayleigh-quotient",
    "href": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#rayleigh-quotient",
    "title": "14  Normal and Positive Definite Matrices",
    "section": "Rayleigh quotient",
    "text": "Rayleigh quotient\nGiven a Hermitian matrix \\mathbf{M} \\in \\mathbb{C}^{n \\times n}, the Rayleigh quotient is a function R_{\\mathbf{M}} (\\mathbf{x}): \\mathbb{C}^{n} \\setminus \\{ 0 \\} \\rightarrow \\mathbb{R}\n\nR_{\\mathbf{M}} (\\mathbf{x}) = \\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n\nthat takes a nonzero vector \\mathbf{x} and returns a real number.\nSince the Hermitian matrix \\mathbf{M} has all real eigenvalues, they can be ordered. Suppose \\lambda_{1}, \\dots, \\lambda_{n} is the eigenvalues in descending orders.\nThen, given a Hermitian matrix, its Rayleigh quotient is upper bounded and lower bounded by maximum and minimum eigenvalues of \\mathbf{M} respectively,\n\n\\lambda_{1} \\geq R_{\\mathbf{M}} (\\mathbf{x}) \\geq \\lambda_{n}.\n\nThat is,\n\n\\lambda_{1} = \\max_{\\mathbf{x} \\neq 0} \\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n},\n\n\n\\lambda_{n} = \\min_{\\mathbf{x} \\neq 0} \\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\mathbf{M} is a Hermitian matrix, we can expand it using unitary diagonalization:\n\n\\begin{aligned}\n\\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n& = \\frac{\n    \\mathbf{x}^{H} \\mathbf{U}^{H} \\mathbf{\\Lambda} \\mathbf{U} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n\\\\\n& = \\frac{\n    \\mathbf{y}^{H} \\mathbf{\\Lambda} \\mathbf{y}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n&\n[\\mathbf{y} = \\mathbf{U} \\mathbf{x}].\n\\end{aligned}\n\nSince \\mathbf{U} is a unitary matrix, according to the property of the unitary matrix,\n\n\\mathbf{y}^{H} \\mathbf{y} = \\mathbf{x}^{H} \\mathbf{x}.\n\nThus,\n\n\\begin{aligned}\n\\frac{\n    \\mathbf{y}^{H} \\mathbf{\\Lambda} \\mathbf{y}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n& =\n\\frac{\n    \\mathbf{y}^{H} \\mathbf{\\Lambda} \\mathbf{y}\n}{\n    \\mathbf{y}^{H} \\mathbf{y}\n}\n\\\\\n& =\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{i} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}.\n\\end{aligned}\n\nSince \\lambda_{1} \\geq \\lambda_{i} \\geq \\lambda_{n}, \\forall i = 1, \\dots, n,\n\n\\lambda_{1}\n=\n\\lambda_{1}\n\\frac{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n=\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{1} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n\\geq\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{i} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n},\n\n\n\\lambda_{n}\n=\n\\lambda_{n}\n\\frac{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n=\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{n} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n\\leq\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{i} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}."
  },
  {
    "objectID": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#positive-definite-matrices",
    "href": "Linear Algebra/14_Normal_and_Positive_Definite_Matrices.html#positive-definite-matrices",
    "title": "14  Normal and Positive Definite Matrices",
    "section": "Positive Definite Matrices",
    "text": "Positive Definite Matrices\nGiven a Hermitian matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n}, it is positive definite if and only if\n\n\\mathbf{x}^{H} \\mathbf{A} \\mathbf{x} &gt; 0,\n\nfor all nonzero vectors \\mathbf{x} \\in \\mathbb{C}^{n}, and it is positive semi-definite if and only if\n\n\\mathbf{x}^{H} \\mathbf{A} \\mathbf{x} \\geq 0,\n\nfor all vectors \\mathbf{x}.\n\nProperties of definite matrices\n\nPositive definite matrix always has full rank.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction.\nSuppose a positive definite matrix \\mathbf{A} does NOT have full rank, which means that there exists at least one non-zero vector \\mathbf{x} \\in N (\\mathbf{A}) such that\n\n  \\mathbf{A} \\mathbf{x} = 0.\n  \nThen, multiplying both sides by \\mathbf{x}^{H},\n\n  \\mathbf{x}^{H} \\mathbf{A} \\mathbf{x} = 0.\n  \nwhich contradicts to the fact that \\mathbf{A} is positive definite.\nThus, positive definite matrix always has full rank.\n\n\n\nA matrix \\mathbf{A} is positive definite (semi-definite) if and only if its eigenvalues are positive (non-negative).\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first prove that a matrix \\mathbf{A} is positive definite (semi-definite) if its all eigenvalues are positive (non-negative).\nSince \\mathbf{A} is a Hermitian matrix, it can be unitarily diagonalized:\n\n  \\mathbf{A} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{H},\n  \nwhere the columns of \\mathbf{U} contains the orthonormal eigenvectors of \\mathbf{A} and diagonal of \\mathbf{\\Lambda} has the corresponding real eigenvalues.\nSince we are told all eigenvalues are positive or non-negative(TODO),\n\n  \\mathbf{\\Lambda} = \\mathbf{\\Lambda}^{\\frac{1}{2}} \\mathbf{\\Lambda}^{\\frac{1}{2}}\n  = \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{\\Lambda}^{\\frac{1}{2}})^{H}\n  \nThus,\n\n  \\begin{aligned}\n  \\mathbf{A}\n  & = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{H}\n  \\\\\n  & = \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{\\Lambda}^{\\frac{1}{2}})^{H} \\mathbf{U}^{H}\n  \\\\\n  & = \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}})^{H}\n  \\\\\n  \\end{aligned}\n  \nMultiplying \\mathbf{A} with \\mathbf{x} to get\n\n  \\begin{aligned}\n  \\mathbf{x}^{H} \\mathbf{A} \\mathbf{x}\n  & = \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{\\Lambda}^{\\frac{1}{2}} \\mathbf{U})^{H} \\mathbf{x}\n  \\\\\n  & = \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}})^{H}\n  \\\\\n  & = \\lVert \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\rVert^{2}\n  & [\\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\in \\mathbb{C}^{n}].\n  \\\\\n  \\end{aligned}\n  \nSince \\lVert \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\rVert^{2} is non-negative for any vector \\mathbf{x}, the matrix \\mathbf{A} is positive semi-definite.\nIf all eigenvalues are positive, there is no zero in \\mathbf{\\Lambda}. Since \\mathbf{\\Lambda}^{\\frac{1}{2}} is a diagonal matrix, the matrix \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} has scaled columns of \\mathbf{U}. Since \\mathbf{U} is a unitary matrix, its columns are all linearly independent and thus \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} also have linearly independent columns. Thus, \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} has full rank and\n\n  N (\\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}}) = \\{ 0 \\}.\n  \nThus, \\lVert \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\rVert^{2} &gt; 0 for any non-zero vector \\mathbf{x}. Therefore, the matrix \\mathbf{A} is positive definite.\nThen we prove that all eigenvalues of positive definite (semi-definite) matrices are positive (non-negative).\nConsider any eigenpair (\\lambda, \\mathbf{v}) of \\mathbf{A}.\nSince \\mathbf{v} is non-zero by the definition of eigenvector, we have\n\n  \\begin{aligned}\n  \\mathbf{A} \\mathbf{v}\n  & = \\lambda \\mathbf{v}\n  \\\\\n  \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n  & = \\lambda \\mathbf{v}^{H} \\mathbf{v}\n  \\\\\n  \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n  & = \\lambda \\lVert \\mathbf{v} \\rVert^{2}\n  \\\\\n  \\lambda\n  & = \\frac{\n      \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n  }{\n      \\lVert \\mathbf{v} \\rVert^{2}\n  }\n  \\end{aligned}\n  \nSince \\mathbf{A} is positive definite (semi-definite),\n\n  \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v} &gt; 0 \\quad (\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v} \\geq 0),\n  \nwhich means\n\n  \\lambda &gt; 0 \\quad (\\lambda \\geq 0).\n  \n\n\n\nTODO \\mathbf{A} = \\mathbf{B}^{H} \\mathbf{B}"
  },
  {
    "objectID": "Probability and Statistics/01_Probability.html#basic-probability-properties",
    "href": "Probability and Statistics/01_Probability.html#basic-probability-properties",
    "title": "15  Probability",
    "section": "Basic probability properties",
    "text": "Basic probability properties\n\nCorollary 15.1 The probability of the union events is\n\n\\mathbb{P} (A \\cup B) = \\mathbb{P} (A) + \\mathbb{P} (B) - \\mathbb{P} (A \\cap B).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince A \\cup B can be partitioned into 3 disjoint sets: A \\setminus B, B \\setminus A, and A \\cap B, we can use the countable additivity in Definition 15.1 to prove the following\n\n\\mathbb{P} (A) = \\mathbb{P} (A \\setminus B) + \\mathbb{P} (A \\cap B) \\\\\n\\mathbb{P} (B) = \\mathbb{P} (B \\setminus A) + \\mathbb{P} (A \\cap B) \\\\\n\\mathbb{P} (A \\cup B) = \\mathbb{P} (A \\setminus B) + \\mathbb{P} (B \\setminus A) + \\mathbb{P} (A \\cap B).\n\nTherefore, we have\n\n\\begin{aligned}\n\\mathbb{P} (A) + \\mathbb{P} (B) - \\mathbb{P} (A \\cap B)\n& = \\mathbb{P} (A \\setminus B) + \\mathbb{P} (B \\setminus A) + \\mathbb{P} (A \\cap B)\n\\\\\n& = \\mathbb{P} (A \\cup B).\n\\end{aligned}\n\n\n\n\n\nCorollary 15.2 (Union bound) The probability of an event that is a union of a finite set of events A_{1}, \\dots, A_{n} is no greater than the sum of the probabilities of all events.\n\n\\mathbb{P} \\left(\n    \\bigcup_{i = 1}^{n} A_{i}\n\\right) \\leq \\sum_{i = 1}^{n} \\mathbb{P} (A_{i}).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to Corollary 15.1,\n\n\\begin{aligned}\n\\mathbb{P} (A \\cup B)\n& = \\mathbb{P} (A) + \\mathbb{P} (B) - \\mathbb{P} (A \\cap B)\n\\\\\n& \\leq \\mathbb{P} (A) + \\mathbb{P} (B)\n& [\\mathbb{P} (A \\cap B) \\geq 0].\n\\end{aligned}\n\nTherefore, we can extend the above fact for A and B to A_{1}, \\dots, A_{n} to derive the union bound."
  },
  {
    "objectID": "Probability and Statistics/01_Probability.html#joint-probability",
    "href": "Probability and Statistics/01_Probability.html#joint-probability",
    "title": "15  Probability",
    "section": "Joint probability",
    "text": "Joint probability\nUsually we refer the probability that the events A, B occur at the same time as the joint probability of A and B\n\n\\mathbb{P} (A \\cap B) = \\mathbb{P} (A, B).\n\n\nConditional probability\n\nDefinition 15.2 (Conditional probability) Let B be an event such that \\mathbb{P} (B) &gt; 0. The conditional probability of the event A given B is defined to be\n\n\\mathbb{P} (A \\mid B) = \\frac{ \\mathbb{P} (A \\cap B) }{ \\mathbb{P} (B) }.\n\n\nNote that \\mathbb{P} (\\cdot \\mid \\cdot ) is also a probability measure and therefore satisfies the axioms of the probability measure in Definition 15.1.\n\nCorollary 15.3 (Chain rule) Given a set of events A_{1}, \\dots, A_{n}, the probability of their intersection can be calculated as\n\n\\mathbb{P} \\left(\n    \\bigcap_{i = 1}^{n} A_{i}\n\\right)\n= \\prod_{j = 1}^{n} \\mathbb{P} \\left(\n    A_{j} \\;\\middle|\\; \\bigcap_{k = j + 1}^{n} A_{k}\n\\right)\n\nwhere \\mathbb{P} \\left( A_{j} \\;\\middle|\\; \\bigcap_{k &gt; n}^{n} A_{k} \\right) = \\mathbb{P} (A_{j}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor the intersection between 2 events A and B, we have\n\n\\mathbb{P} (A \\cap B) = \\mathbb{P} (A \\mid B) \\mathbb{P} (B)\n\nby the definition of Definition 15.2.\nTo apply it for the multiple events, we can treat A = A_{1} and B = \\bigcap_{i = 2}^{n} A_{i} to get\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\bigcap_{i = 1}^{n} A_{i}\n\\right)\n& = \\mathbb{P} \\left(\n    A_{1} \\;\\middle|\\; \\bigcap_{i = 2}^{n} A_{i}\n\\right)\n\\mathbb{P} \\left(\n    \\bigcap_{i = 2}^{n} A_{i}\n\\right)\n\\\\\n& = \\mathbb{P} \\left(\n    A_{1} \\;\\middle|\\; \\bigcap_{i = 2}^{n} A_{i}\n\\right)\n\\mathbb{P} \\left(\n    A_{2} \\;\\middle|\\; \\bigcap_{i = 3}^{n} A_{i}\n\\right)\n\\mathbb{P} \\left(\n    \\bigcap_{i = 3}^{n} A_{i}\n\\right)\n\\\\\n& = \\mathbb{P} \\left(\n    A_{1} \\;\\middle|\\; \\bigcap_{i = 2}^{n} A_{i}\n\\right)\n\\mathbb{P} \\left(\n    A_{2} \\;\\middle|\\; \\bigcap_{i = 3}^{n} A_{i}\n\\right)\n\\dots\n\\mathbb{P} (A_{n})\n\\\\\n& = \\prod_{j = 1}^{n} \\mathbb{P} \\left(\n    A_{j} \\;\\middle|\\; \\bigcap_{k = j + 1}^{n} A_{k}\n\\right).\n\\end{aligned}\n\n\n\n\n\nTheorem 15.1 (Bayes’s theorem) Let A, B be events with nonzero probability. Then,\n\n\\mathbb{P} (A \\mid B) = \\frac{\n    \\mathbb{P} (B \\mid A) \\mathbb{P} (A)\n}{\n    \\mathbb{P} (B)\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis follows directly from Definition 15.2\n\n\\mathbb{P} (A \\mid B)\n= \\frac{ \\mathbb{P} (A \\cap B) }{ \\mathbb{P} (B) }\n= \\frac{ \\mathbb{P} (B \\cap A) }{ \\mathbb{P} (B) }\n= \\frac{ \\mathbb{P} (B \\mid A) \\mathbb{P} (A) }{ \\mathbb{P} (B) }.\n\n\n\n\n\n\nMarginal probability\n\nTheorem 15.2 (Law of total probability) Let A_{1}, \\dots, A_{n} be events that partition \\Omega, that is, A_{1}, \\dots, A_{n} are disjoint and there union is \\Omega, the for any event B,\n\n\\mathbb{P} (B)\n= \\sum_{i = 1}^{n} \\mathbb{P} (B \\cap A_{i})\n= \\sum_{i = 1}^{n} \\mathbb{P} (B \\mid A_{i}) \\mathbb{P} (A_{i}).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince A_{1}, \\dots, A_{n} partition \\Omega,\n\n\\begin{aligned}\nB\n& = B \\cap \\Omega\n& [B \\subseteq \\Omega]\n\\\\\n& = B \\cap \\left(\n    \\bigcup_{i = 1}^{n} A_{i}\n\\right)\n\\\\\n& = \\bigcup_{i = 1}^{n} \\left(\n    B \\cap A_{i}\n\\right).\n\\end{aligned}\n\nBy the axiom of countable additivity in Definition 15.1 and Definition 15.2, we have\n\n\\mathbb{P} (B)\n= \\sum_{i = 1}^{n} \\mathbb{P} (B \\cap A_{i})\n= \\sum_{i = 1}^{n} \\mathbb{P} (B \\mid A_{i}) \\mathbb{P} (A_{i}).\n\n\n\n\n\n\nIndependence\n\nDefinition 15.3 (Independence) Two events A and B are said to be independent if any of the following holds.\n\n\\mathbb{P} (A \\cap B) = \\mathbb{P} (A) \\mathbb{P} (B).\n\\mathbb{P} (A \\mid B) = \\mathbb{P} (A).\n\\mathbb{P} (B \\mid A) = \\mathbb{P} (B).\n\n\n\nDefinition 15.4 (conditional-independence) Two events A and B are said to be conditional independent given an event C if any of the following holds.\n\n\\mathbb{P} (A \\cap B \\mid C) = \\mathbb{P} (A \\mid C) \\mathbb{P} (B \\mid C).\n\\mathbb{P} (A \\mid B \\cap C) = \\mathbb{P} (A \\cap C).\n\\mathbb{P} (B \\mid A \\cap C) = \\mathbb{P} (B \\cap C).\n\n\nWe can use independence properties to calculate the probability of the intersection of independent events by calculating the product of their probabilities."
  },
  {
    "objectID": "Probability and Statistics/02_Random_Variables.html#cumulative-distribution-function",
    "href": "Probability and Statistics/02_Random_Variables.html#cumulative-distribution-function",
    "title": "16  Random Variables",
    "section": "Cumulative distribution function",
    "text": "Cumulative distribution function\nTo determine the probability distribution \\mathbb{P}_{X} (A) of the random variable X for any Borel set A, it suffices to specify\n\n\\mathbb{P} (X \\leq x), \\quad x \\in \\mathbb{R},\n\nsince the probability of any other Borel set can be determined by the axioms of probability.\n\nDefinition 16.2 The cumulative distribution function (cdf) of a random variable X is a function F_{X} (x) that returns the probability that X \\leq x\n\nF_{X} (x) = \\mathbb{P} (X \\leq x), \\quad x \\in \\mathbb{R}.\n\n\nAccording to Definition 16.2, CDF F_{X} (x) for any random variable has the following properties\n\nF_{X} (x) is nonnegative\n\n  F_{X} (x) \\geq 0, \\quad \\forall x \\in \\mathbb{R}.\n  \nF_{X} (x) is monotonically non-decreasing. If x_{1} \\leq x_{2},\n\n  F_{X} (x_{1}) \\leq F_{X} (x_{2}).\n  \nF_{X} (x) has the following limits\n\n  \\lim_{x \\to -\\infty} F_{X} (x) = 0, \\quad \\lim_{x \\to \\infty} F_{X} (x) = 1.\n  \nF_{X} (x) is right continuous\n\n  F_{X} (a^{+}) = \\lim_{x \\to a^{+}} F_{X} (x) = F_{X} (a)."
  },
  {
    "objectID": "Probability and Statistics/02_Random_Variables.html#probability-mass-density-function",
    "href": "Probability and Statistics/02_Random_Variables.html#probability-mass-density-function",
    "title": "16  Random Variables",
    "section": "Probability mass (density) function",
    "text": "Probability mass (density) function\nInstead of using CDFs to specify the probability distributions, they can be defined using probability mass function for discrete random variables and probability density function for continuous random variables.\n\nDefinition 16.3 (Probability mass function) The probability mass function (PMF) of a discrete random variable X is a function p_{X} (x) that returns the probability that X = x\n\np_{X} (x) = \\mathbb{P} (X = x), \\quad x \\in \\mathcal{X}.\n\n\n\nDefinition 16.4 (Probability density function) The probability density function (PDF) of a continuous random variable X is a function f_{X} (x) that\n\n\\begin{aligned}\n\\int_{x_{1}}^{x_{2}} f_{X} (x) \\mathop{dx}\n& = \\mathbb{P} (x_{1} \\leq X \\leq x_{2})\n\\\\\n& = \\mathbb{P} (x_{1} \\leq X &lt; x_{2})\n\\\\\n& = \\mathbb{P} (x_{1} &lt; X \\leq x_{2})\n\\\\\n& = \\mathbb{P} (x_{1} &lt; X &lt; x_{2}), \\quad x_{1}, x_{2} \\in \\mathbb{R}.\n\\end{aligned}\n\n\n\nRemark. With a slight abuse of the notation, we use \\mathbb{P}_{X} (x) to denote the PMF of X if X is a discrete random variable and the PDF if it is a continuous random variable\n\n\\mathbb{P}_{X} (x) =\n\\begin{cases}\np_{X} (x), \\quad X \\text{ is discrete} \\\\\nf_{X} (x), \\quad X \\text{ is continuous},\n\\end{cases}\n\nand then the probability that event A \\in \\mathcal{B} happens can be denoted as\n\n\\sum_{x \\in A} \\mathbb{P}_{X} (x) =\n\\begin{cases}\n\\sum_{x \\in A} p_{X} (x), \\quad X \\text{ is discrete} \\\\\n\\int_{x \\in A} f_{X} (x), \\quad X \\text{ is continuous}.\n\\end{cases}\n\n\nThe properties of PMF and PDF are summarized below\n\nNon-negative\n\n  \\mathbb{P}_{X} (x) \\geq 0.\n  \nNormalization\n\n  \\sum_{x \\in \\mathbb{R}} \\mathbb{P}_{X} (x) = 1.\n  \nRelation to CDF\n\n  F_{X} (x) = \\sum_{t \\leq x} \\mathbb{P}_{X} (t)."
  },
  {
    "objectID": "Probability and Statistics/02_Random_Variables.html#multiple-random-variables",
    "href": "Probability and Statistics/02_Random_Variables.html#multiple-random-variables",
    "title": "16  Random Variables",
    "section": "Multiple random variables",
    "text": "Multiple random variables\nLet (\\Omega, \\mathcal{F}, \\mathbb{P}) be a probability space and consider two measurable mappings\n\n\\begin{aligned}\nX : \\Omega\n& \\rightarrow \\mathbb{R},\n\\\\\nY : \\Omega\n& \\rightarrow \\mathbb{R}.\n\\end{aligned}\n\nIn other words, X and Y are two random variables defined on the common probability space (\\Omega, \\mathcal{F}, \\mathbb{P}). In order to specify the random variables, we need to determine\n\n\\mathbb{P} ((X, Y) \\in A) = \\mathbb{P} (\\omega \\in \\Omega : (X(\\omega), Y(\\omega)) \\in A)\n\nfor every Borel set A \\subseteq \\mathbb{R}^2.\nBy the properties of the Borel \\sigma-field, it can be shown that it suffices to determine the probabilities of the form\n\n\\mathbb{P} (X \\leq x, Y \\leq y), \\quad x, y \\in \\mathbb{R}.\n\nThe latter defines their joint cdf\n\nF_{X,Y} (x, y) = \\mathbb{P} (X \\leq x, Y \\leq y), \\quad x, y \\in \\mathbb{R},\n\nwhere F_{X,Y}(x, y) is the joint cumulative distribution function of X and Y.\nSimilar to the CDF for a single random variable, the joint CDF for multiple random variables has the following properties\n\nF_{X, Y} (x, y) is nonnegative\n\n  F_{X, Y} (x, y) \\geq 0, \\quad \\forall x, y \\in \\mathbb{R}.\n  \nIf x_{1} \\leq x_{2}, y_{1} \\leq y_{2}\n\n  F_{X, Y} (x_{1}, y_{1}) \\leq F_{X, Y} (x_{2}, y_{2}).\n  \nF_{X, Y} (x, y) has the following limits\n\n  \\lim_{x \\to -\\infty} F_{X, Y} (x, y) = \\lim_{y \\to -\\infty} F_{X, Y} (x, y) = 0, \\quad \\lim_{x, y \\to \\infty} F_{X, Y} (x, y) = 1.\n  \nMarginal CDFs\n\n  \\lim_{x \\to \\infty} F_{X, Y} (x, y) = F_{Y} (y), \\quad \\lim_{y \\to \\infty} F_{X, Y} (x, y) = F_{X} (x)."
  },
  {
    "objectID": "Probability and Statistics/02_Random_Variables.html#joint-distribution",
    "href": "Probability and Statistics/02_Random_Variables.html#joint-distribution",
    "title": "16  Random Variables",
    "section": "Joint distribution",
    "text": "Joint distribution\nJoint distributions can also be specified using the joint PMFs for discrete random variables\n\np_{X, Y} (x, y) = \\mathbb{P} (X = x, Y = y), \\quad x \\in \\mathcal{X}, y \\in \\mathcal{Y},\n\nand joint PDFs for continuous random variables.\n\n\\begin{aligned}\n\\int_{x_{1}}^{x_{2}} \\int_{y_{1}}^{y_{2}} f_{X, Y} (x, y) \\mathop{dx} \\mathop{dy}\n& = \\mathbb{P} (x_{1} \\leq X \\leq x_{2}, y_{1} \\leq Y \\leq y_{2})\n\\\\\n& = \\mathbb{P} (x_{1} \\leq X &lt; x_{2}, y_{1} \\leq Y &lt; y_{2})\n\\\\\n& = \\mathbb{P} (x_{1} &lt; X \\leq x_{2}, y_{1} &lt; Y \\leq y_{2})\n\\\\\n& = \\mathbb{P} (x_{1} &lt; X &lt; x_{2}, y_{1} &lt; Y &lt; y_{2}), \\quad x_{1}, x_{2}, y_{1}, y_{2} \\in \\mathbb{R}.\n\\end{aligned}\n\n\nConditional distribution\n\n\\mathbb{P}_{X \\mid Y} (x \\mid y) = \\frac{ \\mathbb{P}_{X, Y} (x, y) }{ \\mathbb{P}_{Y} (y) }.\n\n\n\nMarginal distribution\n\n\\mathbb{P}_{X} (x) = \\sum_{y \\in \\mathcal{Y}} \\mathbb{P}_{X, Y} (x, y)\n\n\n\nIndependence\n\n\\mathbb{P}_{X, Y} (x, y) = \\mathbb{P}_{X} (x) \\mathbb{P}_{Y} (y)"
  },
  {
    "objectID": "Probability and Statistics/02_Random_Variables.html#function-of-random-variables",
    "href": "Probability and Statistics/02_Random_Variables.html#function-of-random-variables",
    "title": "16  Random Variables",
    "section": "Function of random variables",
    "text": "Function of random variables\nTODO"
  },
  {
    "objectID": "Probability and Statistics/03_Expectation.html#conditional-expectation",
    "href": "Probability and Statistics/03_Expectation.html#conditional-expectation",
    "title": "17  Expectation",
    "section": "Conditional expectation",
    "text": "Conditional expectation\n\nDefinition 17.2 (Conditional expectation) Let X, Y be jointly distributed random variables. Then the conditional expectation of X given the event that Y = y is\n\n\\mathbb{E}_{X \\mid Y} [X \\mid y] = \\sum_{x \\in \\mathbb{R}} x \\mathbb{P}_{X \\mid Y} (x \\mid y),\n\nwhich is a function of y.\n\nUsing Corollary 17.2, the conditional expectation of a transformed random variable g (X) is\n\n\\mathbb{E}_{X \\mid Y} [g (X) \\mid y] = \\sum_{x \\in \\mathbb{R}} g (x) \\mathbb{P}_{X \\mid Y} (x \\mid y).\n\n\nTheorem 17.1 (Law of total expectation (LTE)) Let X, Y be jointly distributed random variables. The expectation of g (X) can be calculated from its conditional expectation\n\n\\mathbb{E}_{X} [g (X)] = \\sum_{y \\in \\mathbb{R}} \\mathbb{E}_{X \\mid Y} [g (X) \\mid y] \\mathbb{P}_{Y} (y).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy expanding \\mathbb{E}_{X \\mid Y} [g (X) \\mid y], we can have\n\n\\begin{aligned}\n\\sum_{y \\in \\mathbb{R}} \\mathbb{E}_{X \\mid Y} [g(X) \\mid y] \\mathbb{P}_{Y} (y)\n& = \\sum_{y \\in \\mathbb{R}} \\left(\n    \\sum_{x \\in \\mathbb{R}} g(x) \\mathbb{P}_{X \\mid Y} (x \\mid y)\n\\right) \\mathbb{P}_{Y} (y)\n\\\\\n& = \\sum_{x \\in \\mathbb{R}} \\sum_{y \\in \\mathbb{R}} g (x) \\mathbb{P}_{X \\mid Y} (x \\mid y) \\mathbb{P}_{Y} (y)\n\\\\\n& = \\sum_{x \\in \\mathbb{R}} g (x) \\sum_{y \\in \\mathbb{R}} \\mathbb{P}_{X, Y} (x, y)\n\\\\\n& = \\sum_{x \\in \\mathbb{R}} g (x) \\mathbb{P}_{X} (x)\n\\\\\n& = \\mathbb{E}_{X} [g (X)].\n\\end{aligned}"
  },
  {
    "objectID": "Probability and Statistics/03_Expectation.html#variance",
    "href": "Probability and Statistics/03_Expectation.html#variance",
    "title": "17  Expectation",
    "section": "Variance",
    "text": "Variance\nThe concept of the variance summarizes how much a random variable deviates from its mean on average.\n\nDefinition 17.3 The variance of a random variable X is defined to be\n\n\\mathrm{Var} [X] = \\mathbb{E}_{X} [(X - \\mathbb{E}_{X} [X])^{2}].\n\n\n\nCorollary 17.4 The variance can also be calculated as\n\n\\mathrm{Var} [X] = \\mathbb{E}_{X} [X^{2}] - \\mathbb{E}_{X} [X]^{2}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\mu = \\mathbb{E}_{X} [X]. By Definition 17.3, we have\n\n\\begin{aligned}\n\\mathrm{Var} (X)\n& = \\mathbb{E}_{X} [(X - \\mu)^2]\n\\\\\n& = \\mathbb{E}_{X} [X^2 - 2 \\mu X + \\mu^2]\n\\\\\n& = \\mathbb{E}_{X} [X^2] - 2 \\mu\\mathbb{E} [X] + \\mu^2\n& [\\text{linearity of expectation}]\n\\\\\n& = \\mathbb{E}_{X} [X^2] - 2 \\mathbb{E} [X]^2 + \\mathbb{E} [X]^2\n\\\\\n& = \\mathbb{E}_{X} [X^2] - \\mathbb{E} [X]^2\n\\end{aligned}\n\n\n\n\n\nCorollary 17.5 The variance of the linear transformation of a random variable is\n\n\\mathrm{Var} (a X + b) = a^{2} \\mathrm{Var} (X).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst we show that variance is invariant of the shift. By Definition 17.3,\n\n\\begin{aligned}\n\\mathrm{Var} (X + b)\n& = \\mathbb{E} [((X + b) - \\mathbb{E} [X + b])^{2}]\n\\\\\n& = \\mathbb{E} [(X + b - \\mathbb{E} [X] - b)^2]\n\\\\\n& = \\mathbb{E} [(X - \\mathbb{E} [X])]^2\n\\\\\n& = \\text{Var} (X).\n\\end{aligned}\n\nThen we show that the variance of the scaling of a random variable is squared. By Corollary 17.4,\n\n\\begin{aligned}\n\\text{Var} (aX)\n& = \\mathbb{E} [(aX)^2] - (\\mathbb{E} [aX])^2\n\\\\\n& = \\mathbb{E} [a^2 X^2] - (a \\mathbb{E} [X])^2\n\\\\\n& = a^2 \\mathbb{E} [X^2] - a^2 \\mathbb{E} [X]^2\n\\\\\n& = a^2 (\\mathbb{E} [X^2] - \\mathbb{E} [X]^2)\n\\\\\n& = a^2 \\text{Var} (X).\n\\end{aligned}\n\n\n\n\n\nStandard deviation\nAnother measure of a random variable X’s spread is the standard deviation.\n\nDefinition 17.4 The standard-deviation of a random variable X is\n\n\\sigma_{X} = \\sqrt{\\mathrm{Var}(X)}."
  },
  {
    "objectID": "Probability and Statistics/03_Expectation.html#covariance",
    "href": "Probability and Statistics/03_Expectation.html#covariance",
    "title": "17  Expectation",
    "section": "Covariance",
    "text": "Covariance\nGiven two random variables X, Y with a joint distribution \\mathbb{P}_{X, Y} (x, y), the covariance describes how they are related with each other. If the covariance is positive, increasing one variable generally leads to an increase in the other random variable, and leads to a decrease if it is negative.\n\nDefinition 17.5 (Covariance) Let X, Y be random variables. The covariance between X and Y is\n\n\\mathrm{Cov} [X, Y] = \\mathbb{E}_{X, Y} [(X - \\mathbb{E}_{X} [X]) (Y - \\mathbb{E}_{Y} [Y])].\n\n\n\nRemark. The covariance of the random variable X with itself is its variance\n\n\\mathrm{Cov} [X, X] = \\mathrm{Var} [X].\n\n\n\nCorollary 17.6 The covariance can also be calculated as\n\n\\mathrm{Cov} [X, Y] = \\mathbb{E}_{X, Y} [X Y] - \\mathbb{E}_{X} [X] \\mathbb{E}_{Y} [Y].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\mu_{X} = \\mathbb{E}_{X} [X] and \\mu_{Y} = \\mathbb{E}_{Y} [Y]. By Definition 17.5, we have\n\n\\begin{aligned}\n\\mathrm{Cov} [X, Y]\n& = \\mathbb{E}_{X, Y} [(X - \\mu_{X}) (Y - \\mu_{Y})]\n\\\\\n& = \\mathbb{E}_{X, Y} [X Y - X \\mu_{Y} - \\mu_{X} Y + \\mu_{X} \\mu_{Y}]\n\\\\\n& = \\mathbb{E}_{X, Y} [X Y] - \\mu_{X} \\mu_{Y} - \\mu_{X} \\mu_{Y} + \\mu_{X} \\mu_{Y}\n\\\\\n& = \\mathbb{E}_{X, Y} [X Y] - \\mathbb{E}_{X} [X] \\mathbb{E}_{Y} [Y].\n\\end{aligned}\n\n\n\n\n\nCorollary 17.7 The covariance has the following properties.\n\nInvariant to shifting\n\n\\mathrm{Cov} [X + a, Y] = \\mathrm{Cov} [X, Y].\n\nLinear transformation\n\n\\mathrm{Cov} [a X + b Y, Z] = a \\mathrm{Cov} [X, Z] + b \\mathrm{Cov} [Y, Z].\n\nCovariance of sum of random variables\n\n\\mathrm{Cov} [X + A, Y + B] = \\mathrm{Cov} [X, Y] + \\mathrm{Cov} [X, B] + \\mathrm{Cov} [A, Y] + \\mathrm{Cov} [A, B].\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAll of the 3 properties can be proved from the definition of the variance using Corollary 17.1.\n\n\\begin{aligned}\n\\mathrm{Cov} [X + a, Y]\n& = \\mathbb{E}_{X, Y} [(X + a - \\mathbb{E} [X + a]) (Y - \\mathbb{E} [Y])]\n\\\\\n& = \\mathbb{E}_{X, Y} [(X + a - \\mathbb{E} [X] - a) (Y - \\mathbb{E} [Y])]\n\\\\\n& = \\mathbb{E}_{X, Y} [(X - \\mathbb{E} [X]) (Y - \\mathbb{E} [Y])]\n\\\\\n& = \\mathrm{Cov} [X, Y].\n\\end{aligned}\n\n\n\\begin{aligned}\n\\mathrm{Cov} [aX + bY, Z]\n& = \\mathbb{E}_{X, Y, Z} [(aX + bY - \\mathbb{E}_{X, Y} [aX + bY]) (Z - \\mathbb{E}_{Z} [Z])]\n\\\\\n& = \\mathbb{E}_{X, Y, Z} [a(X - \\mathbb{E}_{X} [X]) (Z - \\mathbb{E}_{Z} [Z]) + b(Y - \\mathbb{E}_{Y} [Y]) (Z - \\mathbb{E}_{Z} [Z])]\n\\\\\n& = a\\mathbb{E}_{X, Z} [(X - \\mathbb{E}_{X} [X]) (Z - \\mathbb{E}_{Z} [Z])] + b\\mathbb{E}_{Y, Z} [(Y - \\mathbb{E}_{Y} [Y]) (Z - \\mathbb{E}_{Z} [Z])]\n\\\\\n& = a\\mathrm{Cov} [X, Z] + b\\mathrm{Cov} [Y, Z].\n\\end{aligned}\n\n\n\\begin{aligned}\n\\mathrm{Cov} [X + A, Y + B] &\n= \\mathbb{E}_{X, A, Y, B} [(X + A - \\mathbb{E}_{X, A} [X + A]) (Y + B - \\mathbb{E}_{Y, B} [Y + B])]\n\\\\\n& = \\mathbb{E}_{X, A, Y, B} [(X - \\mathbb{E}_{X} [X] + A - \\mathbb{E}_{A} [A]) (Y - \\mathbb{E}_{Y} [Y] + B - \\mathbb{E}_{B} [B])]\n\\\\\n& = \\mathbb{E}_{X, Y} [(X - \\mathbb{E}_{X} [X]) (Y - \\mathbb{E}_{Y} [Y])] + \\mathbb{E}_{X, B} [(X - \\mathbb{E}_{X} [X]) (B - \\mathbb{E}_{B} [B])]\n\\\\\n& \\quad + \\mathbb{E}_{A, Y} [(A - \\mathbb{E}_{A} [A]) (Y - \\mathbb{E}_{Y} [Y])] + \\mathbb{E}_{A, B} [(A - \\mathbb{E}_{A} [A]) (B - \\mathbb{E}_{B} [B])]\n\\\\\n& = \\mathrm{Cov} [X, Y] + \\mathrm{Cov} [X, B] + \\mathrm{Cov} [A, Y] + \\mathrm{Cov} [A, B].\n\\end{aligned}\n\n\n\n\n\nCorollary 17.8 If X and Y are independent, their covariance is 0\n\n\\mathrm{Cov} [X, Y] = 0.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to Corollary 17.3, we have that\n\n\\mathbb{E}_{X, Y} [X Y] = \\mathbb{E}_{X} [X] \\mathbb{E}_{Y} [Y].\n\nTherefore according to Corollary 17.6, we have\n\n\\mathrm{Cov} [X, Y] = \\mathbb{E}_{X, Y} [X Y] - \\mathbb{E}_{X} [X] \\mathbb{E}_{Y} [Y] = 0.\n\n\n\n\n\nCorollary 17.9 For any random variables X and Y\n\n\\mathrm{Var} [X + Y] = \\mathrm{Var} [X] + \\mathrm{Var} [Y] + 2 \\mathrm{Cov} [X, Y].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the variance of a random variable is its covariance with itself,\n\n\\begin{aligned}\n\\mathrm{Var} [X + Y]\n& = \\mathrm{Cov} [X + Y, X + Y]\n\\\\\n& = \\mathrm{Cov} [X, X] + \\mathrm{Cov} [X, Y] + \\mathrm{Cov} [Y, X] + \\mathrm{Cov} [Y, Y]\n\\\\\n& = \\mathrm{Var} [X] + \\mathrm{Var} [Y] + 2 \\mathrm{Cov} [X, Y].\n\\end{aligned}\n\nwhere we used the third property in Corollary 17.7 in the second equality.\n\n\n\n\nCorrelation\nThe problem with covariance in describing the relation between two random variables is that its values can be affected by the variance of each individual random variable. THe correlation coefficient calculates the normalized covariance that is invariant with the scaling of the individual random variables.\n\nDefinition 17.6 Let X, Y be random variables. The correlation coefficient between X and Y is\n\n\\rho (X, Y) = \\frac{ \\mathrm{Cov} [X, Y] }{ \\sqrt{\\mathrm{Var} [X]} \\sqrt{\\mathrm{Var} [Y]} }."
  },
  {
    "objectID": "Probability and Statistics/04_Common_Distributions.html#discrete-distributions",
    "href": "Probability and Statistics/04_Common_Distributions.html#discrete-distributions",
    "title": "18  Common Distributions",
    "section": "Discrete distributions",
    "text": "Discrete distributions\n\nBernoulli distribution\nA random variable X \\in \\{0, 1\\} follows the Bernoulli distribution\n\nX \\sim \\mathrm{Ber}(p) \\quad p \\in [0, 1],\n\nif X takes value 1 (success) with probability p and 0 (failure) with probability 1 - p.\n\n\\mathbb{P}_{X}(x) = p^{x} (1 - p)^{1 - x}\n\n\n\\mathbb{E}_{X}[x] = p\n\n\n\\mathrm{Var} [x] = p (1 - p)\n\n\n\nBinomial distribution\nA random variable $X {0, , n} $ follows the binomial distribution\n\nX \\sim \\mathrm{Bin}(n, p) \\quad n \\in \\mathbb{N} \\quad p \\in [0, 1],\n\nif X is the sum of the results of (or number of successes in) n independent and identically distributed Bernoulli trials with probability p.\n\n\\mathbb{P}_{X}(x) = {n \\choose x} p^{x} (1 - p)^{n - x}\n\n\n\\mathbb{E}_{X}[x] = np\n\n\n\\mathrm{Var} [x] = np(1 - p)\n\n\n\nGeometric distribution\nA random variable X \\in \\{ 1, 2, \\dots\\} follows the geometric distribution\n\nX \\sim \\mathrm{Geo}(p) \\quad p \\in [0, 1],\n\nif X is the number of independent Bernoulli trials with parameter p up to and including first success.\n\n\\mathbb{P}_{X}(x) = p (1 - p)^{x - 1}\n\n\n\\mathbb{E}_{X}[x] = \\frac{1}{p}\n\n\n\\mathrm{Var} [x] = \\frac{1}{p^2}\n\n\n\nNegative binomial distribution\nA random variable X \\in \\{ r, r + 1, \\dots \\} follows the negative binomial distribution\n\nX \\sim \\mathrm{NegBio}(r, p) \\quad r \\in \\mathbb{N} \\quad p \\in [0, 1],\n\nif X is the number of independent Bernoulli trials with parameter p up to and including the r successes.\n\n\\mathbb{P}_{X}(x) = {x - 1 \\choose r - 1} p^{r} (1 - p)^{x - r}\n\n\n\\mathbb{E}_{X}[x] = \\frac{r}{p}\n\n\n\\mathrm{Var} [x] = \\frac{r (1 - p)}{(1 - p)^2}\n\n\n\nPoisson distribution\nA random variable X \\in \\mathbb{N} follows the Poisson distribution\n\nX \\sim \\mathrm{Poi}(\\lambda) \\quad \\lambda &gt; 0,\n\nif X is the number of events that occur in one unit of time independently with rate \\lambda per unit time.\n\n\\mathbb{P}_{X}(x) = e^{-\\lambda} \\frac{\\lambda^{x}}{x!}\n\n\n\\mathbb{E}_{X}[x] = \\lambda\n\n\n\\mathrm{Var} [x] = \\lambda"
  },
  {
    "objectID": "Probability and Statistics/04_Common_Distributions.html#continuos-distribution",
    "href": "Probability and Statistics/04_Common_Distributions.html#continuos-distribution",
    "title": "18  Common Distributions",
    "section": "Continuos distribution",
    "text": "Continuos distribution\n\nUniform distribution\nA random variable X \\in \\mathbb{R} follows the Uniform distribution\n\nX \\sim \\mathrm{Unif} (a, b) \\quad a &lt; b,\n\nif X describes an experiment whose outcomes are equally likely in a range.\n\n\\mathbb{P}_{X} (x) =\n\\begin{cases}\n\\begin{aligned}\n& \\frac{ 1 }{ b - a }\n&& \\quad x \\in [a, b]\n\\\\\n& 0\n&& \\quad \\text{otherwise}\n\\end{aligned}\n\\end{cases}\n\n\n\\mathbb{E}_{X} [X] = \\frac{ a + b }{ 2 }\n\n\n\\mathrm{Var} [X] = \\frac{ (b - a)^{2} }{ 12 }\n\n\nF_{X} (x) =\n\\begin{cases}\n\\begin{aligned}\n& 0\n&& \\quad x &lt; a\n\\\\\n& \\frac{ x - a }{ b - a }\n&& \\quad a \\leq x \\leq b\n\\\\\n& 1\n&& \\quad x &gt; b\n\\end{aligned}\n\\end{cases}\n\n\n\nExponential distribution\nA random variable X \\in [0, \\infty] follows the Exponential distribution\n\nX \\sim \\mathrm{Exp} (\\lambda) \\quad \\lambda \\in \\mathbb{R},\n\nif X is the waiting time until the first occurrence of an event in a Poisson Process with parameter \\lambda.\n\n\\mathbb{P}_{X} (x) =\n\\begin{cases}\n\\begin{aligned}\n& \\lambda e^{- \\lambda x}\n&& \\quad x \\geq 0\n\\\\\n& 0\n&& \\quad \\text{otherwise}\n\\end{aligned}\n\\end{cases}\n\n\n\\mathbb{E}_{X} [X] = \\frac{ 1 }{ \\lambda }\n\n\n\\mathrm{Var} (X) = \\frac{ 1 }{ \\lambda^{2} }\n\n\nF_{X} (x) =\n\\begin{cases}\n\\begin{aligned}\n& 1 - e^{- \\lambda x}\n&& \\quad x \\geq 0\n\\\\\n& 0\n&& \\quad \\text{otherwise}\n\\end{aligned}\n\\end{cases}\n\n\n\nGaussian (normal) distribution\nA random variable X \\in \\mathbb{R} follows the Gaussian distribution\n\nX \\sim \\mathcal{N} (\\mu, \\sigma) \\quad \\mu \\in \\mathbb{R}, \\sigma \\in \\mathbb{R},\n\nif X follows the standard bell curve.\n\n\\mathbb{P}_{X} (x) = \\frac{ 1 }{ \\sigma \\sqrt{2 \\pi} } \\exp \\left[ - \\frac{ (x - \\mu)^{2} }{ 2 \\sigma^{2} } \\right]\n\n\n\\mathbb{E}_{X} [X] = \\mu\n\n\n\\mathrm{Var} (X) = \\sigma^{2}"
  },
  {
    "objectID": "Probability and Statistics/05_Moment_Generating_Functions.html#properties-of-mgf",
    "href": "Probability and Statistics/05_Moment_Generating_Functions.html#properties-of-mgf",
    "title": "19  Moment Generating Function",
    "section": "Properties of MGF",
    "text": "Properties of MGF\n\nCorollary 19.2 The MGF of Y = a X + b can be computed as\n\nM_{Y} (s) = e^{b s} M_{X} (a s).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\nM_{Y} (s)\n& = \\mathbb{E}_{X} \\left[\n    e^{s (a X + b)}\n\\right]\n\\\\\n& = e^{b s} \\mathbb{E}_{X} \\left[\n    e^{s a X}\n\\right]\n\\\\\n& = e^{b s} M_{X} (a s).\n\\\\\n\\end{aligned}\n\n\n\n\n\nCorollary 19.3 Suppose X_{1}, \\dots, X_{n} are independent random variables and X = \\sum_{i = 1}^{n} X_{i}. Then we have the MGF of X as the sum of MGFs of X_{i}s,\n\nM_{X} (s) = \\prod_{i = 1}^{n} M_{X_{i}} (s).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\nM_{X} (s)\n& = \\mathbb{E}_{X} [e^{s X}]\n\\\\\n& = \\mathbb{E}_{X} [e^{s \\sum_{i = 1} X_{i}}]\n\\\\\n& = \\mathbb{E}_{X} [\\prod_{i = 1}^{n} e^{s X_{i}}]\n\\\\\n& = \\prod_{i = 1}^{n} [e^{s X_{i}}]\n& [X_{i}\\text{s are independent}]\n\\\\\n& = \\prod_{i = 1}^{n} M_{X_{i}} (s)\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Probability and Statistics/06_Concentration_Inequalities_I.html#markovs-inequality",
    "href": "Probability and Statistics/06_Concentration_Inequalities_I.html#markovs-inequality",
    "title": "20  Concentration Inequalities I",
    "section": "Markov’s inequality",
    "text": "Markov’s inequality\nMarkov’s inequality states that the probability of a random variable larger than t is less than \\frac{\\mu}{t}.\n\nTheorem 20.1 (Markov’s inequality) Let X \\geq 0 be a non-negative random variable with finite mean \\mu. Then for any t &gt; 0,\n\n\\mathbb{P} \\left(\n    X \\geq t\n\\right) \\leq \\frac{ \\mu }{ t }.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAssuming X is a discrete random variable,\n\n\\mu = \\sum_{x} \\mathbb{P}_{X} (x) x = \\sum_{x \\geq t} \\mathbb{P}_{X} (x) x + \\sum_{x &lt; t} \\mathbb{P}_{X} (x) x.\n\nSince X \\geq 0, \\sum_{x &lt; t} \\mathbb{P}_{X} (x) x is non-negative, and therefore\n\n\\begin{aligned}\n\\mu\n& = \\sum_{x \\geq t} \\mathbb{P}_{X} (x) x + \\sum_{x &lt; t} \\mathbb{P}_{X} (x) x.\n\\\\\n& \\geq \\sum_{x \\geq t} \\mathbb{P}_{X} (x)\n\\\\\n& = t \\sum_{x \\geq t} \\mathbb{P}_{X} (x)\n\\\\\n& = t \\mathbb{P}_{X} (x \\geq t)\n\\end{aligned}\n\nWe have Markov’s inequality by rearranging the term\n\n\\mathbb{P}_{X} (x \\geq t) \\leq \\frac{\\mu}{t}."
  },
  {
    "objectID": "Probability and Statistics/06_Concentration_Inequalities_I.html#chebyshevs-inequality",
    "href": "Probability and Statistics/06_Concentration_Inequalities_I.html#chebyshevs-inequality",
    "title": "20  Concentration Inequalities I",
    "section": "Chebyshev’s inequality",
    "text": "Chebyshev’s inequality\nChebyshev’s inequality states that the probability of the difference between a random variable and its mean larger than a is less than \\frac{\\sigma^{2}}{t^{2}}.\n\nTheorem 20.2 (Chebyshev’s inequality) Let X be a random variable with finite mean \\mu and finite variance \\sigma^{2}. Then for any t &gt; 0,\n\n\\mathbb{P} \\left(\n    \\lvert X - \\mu \\rvert \\geq t\n\\right) \\leq \\frac{ \\sigma^{2} }{ t^{2} }.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet Y = (X - \\mathbb{E}_{X} [X])^{2} and apply Markov’s inequality on Y to get\n\n\\begin{aligned}\n\\mathbb{P} (Y \\geq t^{2})\n& \\leq \\frac{ \\mathbb{E}_{Y} [y] }{ t^{2} }\n\\\\\n& = \\frac{ \\mathbb{E}_{X} [(X - \\mathbb{E}_{X} [X])^{2}] }{ a^{2} }\n\\\\\n& = \\frac{ \\mathrm{Var}_{X} [X] }{ t^{2} }.\n\\end{aligned}\n\nNote that\n\n\\mathbb{P} (Y \\geq t^{2}) = \\mathbb{P} \\left(\n    (X - \\mathbb{E}_{X} [X])^{2} \\geq t^{2}\n\\right) = \\mathbb{P} \\left(\n    \\lvert X - \\mathbb{E}_{X} [X] \\rvert \\geq t\n\\right).\n\nTherefore,\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\lvert X - \\mathbb{E}_{X} [X] \\rvert \\geq t\n\\right)\n& \\leq \\frac{ \\mathrm{Var} [X] }{ t^{2} }.\n\\\\\n\\mathbb{P} \\left(\n    \\lvert X - \\mu \\rvert \\geq t\n\\right)\n& \\leq \\frac{ \\sigma^{2} }{ t^{2} }.\n\\end{aligned}\n\n\n\n\n\nAverage of random variables converge to mean\nLet X_{1}, \\dots, X_{n} be independent random variables with finite means \\mu_{1}, \\dots, \\mu_{n} and finite variances \\sigma_{1}^{2}, \\dots, \\sigma_{n}^{2}. Then, for any t &gt; 0\n\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\sum_{i = 1}^{n} X_{i} - \\sum_{i = 1}^{n} \\mu_{i}\n    \\right\\rvert \\geq t\n\\right) \\leq \\frac{\n    \\sum_{i = 1}^{n} \\sigma_{i}^{2}\n}{\n    t^{2}\n}"
  },
  {
    "objectID": "Probability and Statistics/06_Concentration_Inequalities_I.html#chernoff-bound",
    "href": "Probability and Statistics/06_Concentration_Inequalities_I.html#chernoff-bound",
    "title": "20  Concentration Inequalities I",
    "section": "Chernoff bound",
    "text": "Chernoff bound\n\nChernoff’s bounding method\n\nTheorem 20.3 Let X be a random variable on \\mathbb{R}. Then for all the t &gt; 0\n\n\\mathbb{P} (X \\geq t) \\leq \\inf_{s &gt; 0} \\frac{\n    M_{X} (s)\n}{\n    e^{s t}\n} = \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{X} [e^{s X}]\n}{\n    e^{s t}\n}\n\nwhere M_{X} (s) is the MGF of X.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any t &gt; 0, we can apply Markov’s inequality\n\n\\begin{aligned}\n\\mathbb{P} (X \\geq t)\n& = \\mathbb{P} (e^{s X} \\geq e^{s t})\n\\\\\n& \\leq \\frac{ \\mathbb{E}_{X} [e^{s X}] }{ e^{s t} }\n\\\\\n& \\leq \\frac{M_{X} (s) }{ e^{s t} }.\n\\end{aligned}\n\nSince \\mathbb{P} (X \\geq t) \\leq \\frac{ M_{X} (s) }{ e^{s t} } for any s &gt; 0, \\mathbb{P} (X \\geq t) is less than the s that minimizes \\frac{ M_{X} (s) }{ e^{s t} }, which is written as\n\n\\mathbb{P} (X \\geq t) \\leq \\inf_{s &gt; 0} \\frac{M_{X} (s)}{e^{s t}}.\n\n\n\n\n\nRemark. The closed-form solution of the optimization problem\n\n\\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{X} [e^{s X}]\n}{\n    e^{s t}\n} = \\inf_{s &gt; 0} f (s)\n\ncan be obtained by solving f' (s) = 0 if f (s) is a convex function.\n\n\n\nChernoff bound for Bernoulli random variables\nLet X_{1}, \\dots, X_{n} be bounded independent Bernoulli random variables such that X_{i} \\sim \\mathrm{Ber} (p_{i}), \\forall i. Let X = \\sum_{i=1}^{n} X_{i} and \\mu = \\mathbb{E}_{X} (X).\n\nUpper tail: for all \\delta &gt; 0\n\n  \\mathbb{P} (X \\geq (1 + \\delta) \\mu) \\leq \\exp \\left[\n      -\\frac{ \\delta^{2} \\mu }{ 2 + \\delta }\n  \\right]\n  \nLower tail: for all 0 &lt; \\delta &lt; 1\n\n  \\mathbb{P} (X \\leq (1 - \\delta) \\mu) \\leq \\exp \\left[\n      -\\frac{ \\delta^{2} \\mu }{ 2 }\n  \\right]\n  \n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet X_{i} \\sim \\mathrm{Ber} (p) be a Bernoulli variable. Then the MGF of X_{i} can be written as\n\n\\begin{aligned}\nM_{X_{i}} (s)\n& = \\mathbb{E}_{X_{i}} (e^{s X_{i}})\n\\\\\n& = p e^{1 \\times s} + (1 - p) e^{0 \\times s}\n\\\\\n& = 1 + p (e^{s} - 1).\n\\\\\n\\end{aligned}\n\nWe can obtain the following inequality for MGF of X_{i}\n\nM_{X_{i}} (s) = 1 + p (e^{s} - 1) \\leq \\exp[p (e^{s} - 1)],\n\nwhich follows from the fact that 1 + a \\leq e^{a} for any a.\nAccording to the property of the MGF, the same inequality can be obtained for the MGF of X = \\sum_{i = 1}^{n} X_{i}\n\n\\begin{aligned}\n\\mathbb{E}_{X} [e^{s X}]\n& = M_{X} (s)\n\\\\\n& = \\prod_{i = 1}^{n} M_{X_{i}} (s)\n\\\\\n& \\leq \\prod_{i = 1}^{n} \\exp[p (e^{s} - 1)]\n\\\\\n& = \\exp \\left[\n    (e^{s} - 1) \\sum_{i = 1}^{n} p_{i}\n\\right]\n\\\\\n& = \\exp \\left[\n    (e^{s} - 1) \\mu\n\\right]\n\\end{aligned}\n\nwhere \\mu = \\mathbb{E}_{X} [X] = \\sum_{i = 1}^{n} p_{i}.\nBy applying the Chernoff bounding method with t = (1 + \\delta) \\mu, we can derive a upper bound\n\n\\begin{aligned}\n\\mathbb{P} (X \\geq (1 + \\delta) \\mu)\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{X} [e^{s X}]\n}{\n    e^{s (1 + \\delta) \\mu}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\frac{\n    \\exp[(e^{s} - 1) \\mu]\n}{\n    e^{s (1 + \\delta) \\mu}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\exp\\left[\n    (e^{s} - 1) \\mu - (1 + \\delta) \\mu s\n\\right].\n\\end{aligned}\n\nSince \\exp [(e^{s} - 1) \\mu - (1 + \\delta) \\mu s] is a convex function\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\exp\\left[\n    (e^{s} - 1) \\mu - (1 + \\delta) \\mu s\n\\right]\n& = 0\n\\\\\n(\\mu e^{s} - (1 + \\delta) \\mu) \\exp\\left[\n    (e^{s} - 1) \\mu - (1 + \\delta) \\mu s\n\\right]\n& = 0\n\\\\\n\\mu e^{s} - (1 + \\delta) \\mu\n& = 0\n\\\\\ns\n& = \\log [1 + \\delta].\n\\end{aligned}\n\nNow let’s select s = \\log [1 + \\delta] to get the tight upper bound\n\n\\begin{aligned}\n\\frac{\n    \\exp[(e^{s} - 1) \\mu]\n}{\n    e^{s (1 + \\delta) \\mu}\n}\n& = \\frac{\n    \\exp[(e^{\\log[1 + \\delta]} - 1) \\mu]\n}{\n    e^{\\log[1 + \\delta] (1 + \\delta) \\mu}\n}\n\\\\\n& = \\frac{\n    e^{\\delta \\mu}\n}{\n    (1 + \\delta)^{(1 + \\delta) \\mu}\n}\n\\\\\n& = \\left(\n    \\frac{e^{\\delta}}{(1 + \\delta)^{1 + \\delta}}\n\\right)^{\\mu}.\n\\end{aligned}\n\nTaking \\exp \\log on the upper bound to derive\n\n\\begin{aligned}\n\\left(\n    \\frac{ e^{\\delta} }{ (1 + \\delta)^{1 + \\delta} }\n\\right)^{\\mu}\n& = \\exp \\left[\n    \\log \\left[\n        \\left(\n            \\frac{ e^{\\delta} }{ (1 + \\delta)^{1 + \\delta} }\n        \\right)^{\\mu}\n    \\right]\n\\right]\n\\\\\n& = \\exp \\left[\n    \\mu \\left(\n        \\log e^{\\delta} - \\log \\left[\n            (1 + \\delta)^{1 + \\delta}\n        \\right]\n    \\right)\n\\right]\n\\\\\n& = \\exp \\left[\n    \\mu \\left(\n        \\delta - (1 + \\delta) \\log [(1 + \\delta)]\n    \\right)\n\\right].\n\\end{aligned}\n\nSince \\log [1 + x] \\geq \\frac{ x }{ 1 + \\frac{x}{2} } for all x &gt; 0,\n\n\\begin{aligned}\n\\exp \\left[\n    \\mu \\left(\n        \\delta - (1 + \\delta) \\log [(1 + \\delta)]\n    \\right)\n\\right]\n& \\leq \\exp \\left[\n    \\mu \\left(\n        \\delta - \\frac{ (1 - \\delta) \\delta }{ 1 + \\frac{\\delta}{2} }\n    \\right)\n\\right]\n\\\\\n& = \\exp \\left[\n    \\mu \\left(\n        \\frac{ (1 + \\frac{\\delta}{2}) \\delta }{ 1 + \\frac{ \\delta }{ 2 } } - \\frac{ (1 - \\delta) \\delta }{ 1 + \\frac{ \\delta }{ 2 } }\n    \\right)\n\\right]\n\\\\\n& = \\exp \\left[\n    - \\left(\n        \\frac{ \\delta^{2} }{ 2 + \\delta }\n    \\right) \\mu\n\\right].\n\\end{aligned}\n\nPutting all together to derive the upper tail of Chernoff bound\n\n\\mathbb{P} (X \\geq (1 + \\delta) \\mu) \\leq \\exp \\left[\n    - \\left(\n        \\frac{ \\delta^{2} }{ 2 + \\delta }\n    \\right) \\mu\n\\right].\n\nThe proof for the lower tail is similar except that\n\ntaking s = \\log [1 - \\delta] to maximize the lower bound, and\napplies the inequality \\log [1 - x] \\geq -x + \\frac{ x^{2} }{ 2 } in the last step for 0 &lt; x &lt; 1.\n\n\n\n\n\n\nGeneral Chernoff bound\nLet X_{1}, \\dots, X_{n} be bounded independent random variables such that X_{i} \\in [a, b], \\forall i. Let X = \\sum_{i=1}^{n} X_{i} and \\mu = \\mathbb{E}_{X} [X]. Then for \\delta &gt; 0\n\nUpper tail:\n\n  \\mathbb{P} (X \\geq (1 + \\delta) \\mu) \\leq \\exp \\left[\n      -\\frac{ 2 \\delta^{2} \\mu^{2} }{ n (b - a)^{2} }\n  \\right]\n  \nLower tail:\n\n  \\mathbb{P} (X \\leq (1 - \\delta) \\mu) \\leq \\exp \\left[\n      -\\frac{ \\delta^{2} \\mu^{2} }{ n (b - a)^{2} }\n  \\right]"
  },
  {
    "objectID": "Probability and Statistics/07_Convergence.html#sequence-of-random-variables",
    "href": "Probability and Statistics/07_Convergence.html#sequence-of-random-variables",
    "title": "21  Convergence",
    "section": "Sequence of random variables",
    "text": "Sequence of random variables\n\nDefinition 21.2 (Sequence of random variables) A sequence of random variables\n\n\\{ X_{i} \\}_{1}^{n} = X_{1}, \\dots, X_{n}\n\nis the set of n random variables that are defined on the same sample space \\Omega, which means each random variable X_{i} in the sequence is a function from \\Omega to \\mathbb{R}.\n\nUsually, we assume that distributions of the sequence of random variables in a sequence are dependent on their indices i."
  },
  {
    "objectID": "Probability and Statistics/07_Convergence.html#convergence-of-random-variables",
    "href": "Probability and Statistics/07_Convergence.html#convergence-of-random-variables",
    "title": "21  Convergence",
    "section": "Convergence of random variables",
    "text": "Convergence of random variables\n\nConvergence in distribution\nThe distance function in the definition of convergence in distribution is the difference in CDFs of random variables, and the convergence in distribution occurs when the this difference goes to zero at all points.\n\nDefinition 21.3 (Convergence in distribution) A sequence of random variables \\{ X_{i} \\}_{1}^{n} converge in distribution to the random variable X if\n\n\\lim_{n \\to \\infty} F_{X_{n}} (x) = F_{X} (x)\n\nfor every number x \\in \\mathbb{R} at which F_{X} is continuous.\n\n\n\nConvergence in probability\nThe distance function in the definition of convergence in probability is the probability of the absolute difference between random variables.\n\nDefinition 21.4 (Convergence in probability) A sequence of random variables \\{ X_{i} \\}_{1}^{n} converge in probability to the random variable X if\n\n\\lim_{n \\to \\infty} \\mathbb{P} (\\lvert X_{n} - X \\rvert &gt; \\epsilon) = 0.\n\n\n\nCorollary 21.1 The convergence in probability implies the convergence in distribution, which means convergence in probability is a stronger version of the convergence than the convergence in distribution.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will first prove a lemma that is useful in the proof later. Let X, Y be random variables and a be a real number. Then\n\n\\begin{aligned}\n\\mathbb{P} (Y \\leq a)\n& \\stackrel{(1)}{=} \\mathbb{P} (Y \\leq a, X \\leq a + \\varepsilon)\n+ \\mathbb{P} (Y \\leq a, X &gt; a + \\varepsilon)\n\\\\\n& \\stackrel{(2)}{\\leq} \\mathbb{P} (X \\leq a + \\varepsilon)\n+ \\mathbb{P} (Y \\leq a, X &gt; a + \\varepsilon)\n\\\\\n& \\leq \\mathbb{P} (X \\leq a + \\varepsilon)\n+ \\mathbb{P} (Y - X \\leq a - X, a - X &lt; -\\varepsilon)\n\\\\\n& \\leq \\mathbb{P} (X \\leq a + \\varepsilon)\n+ \\mathbb{P} (Y - X &lt; -\\varepsilon)\n\\\\\n& \\stackrel{(3)}{\\leq} \\mathbb{P} (X \\leq a + \\varepsilon)\n+ \\mathbb{P} (Y - X &lt; -\\varepsilon)\n+ \\mathbb{P} (Y - X &gt; \\varepsilon)\n\\\\\n& = \\mathbb{P} (X \\leq a + \\varepsilon)\n+ \\mathbb{P}(|Y - X| &gt; \\varepsilon)\n\\end{aligned}\n\nwhere\n\n\nfollows because of the law of total probability,\n\n\nfollows because the joint probability is less than the marginal probability: \\mathbb{P} (A) \\leq \\mathbb{P} (A, B),\n\nand (3) follows because any probability is non-negative: \\mathbb{P} (Y - X &gt; \\epsilon) \\geq 0.\n\nThen we can prove the theorem. Let a be a point at which F_{X} is continuous. For any \\epsilon &gt; 0, we have the followings because of the above lemma\n\n\\mathbb{P} (X_{n} \\leq a) \\leq \\mathbb{P} (X \\leq a + \\epsilon)\n+ \\mathbb{P} (\\lvert X_{n} - X \\rvert &gt; \\epsilon), \\\\\n\\mathbb{P} (X \\leq a - \\epsilon) \\leq \\mathbb{P} (X_{n} \\leq a)\n+ \\mathbb{P} (\\lvert X_{n} - X \\rvert &gt; \\epsilon).\n\nTherefore, we have\n\n\\mathbb{P} (X \\leq a - \\epsilon)\n- \\mathbb{P} (\\lvert X_{n} - X \\rvert &gt; \\epsilon)\n\\leq \\mathbb{P} (X_{n} \\leq a) \\leq \\mathbb{P} (X \\leq a + \\epsilon)\n+ \\mathbb{P} (\\lvert X_{n} - X \\rvert &gt; \\epsilon).\n\nBy taking n \\to \\infty, we have the following if \\{ X_{i} \\}_{1}^{n} converges to X in probability,\n\n\\mathbb{P} (X \\leq a - \\epsilon) \\leq \\mathbb{P} (X_{n} \\leq a) \\leq \\mathbb{P} (X \\leq a + \\epsilon) \\\\\nF_{X} (a - \\epsilon) \\leq F_{X_{n}} (a) \\leq F_{X} (a + \\epsilon) \\\\\n\nSince we have assumed that F_{X} is continuous at a, we have the following as \\epsilon \\to 0,\n\nF_{X} (a - \\epsilon) = F_{X_{n}} (a) = F_{X} (a + \\epsilon).\n\nTherefore, we have\n\nF_{X_{n}} (a) = F_{X} (a).\n\nby taking \\epsilon = 0.\nTherefore we have proved that the convergence in probability implies the convergence in distribution for all a where F_{X} (a) is continuous.\n\n\n\n\n\nAlmost sure convergence\n\nA sequence of random variables \\{ X_{i} \\}_{1}^{n} converge almost surely to the random variable X if the values of X_{n} approach the value of X in the sense that events for which X_{n} does not converge to X have probability 0,\n\n\\mathbb{P} (\\omega \\in \\Omega: \\lim_{n \\to \\infty} X_{n} (\\omega) = X (\\omega)) = 1.\n\n\n\nCorollary 21.2 The almost sure convergence implies the convergence in probability, which means almost sure convergence is a even stronger version of the convergence than the convergence in probability and the convergence in distribution.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "Probability and Statistics/08_Limit_Theorems.html#sample-mean",
    "href": "Probability and Statistics/08_Limit_Theorems.html#sample-mean",
    "title": "22  Limit Theorems",
    "section": "Sample mean",
    "text": "Sample mean\n\nDefinition 22.1 (Sample mean) Let X_{1}, \\dots , X_{n} be a sequence of i.i.d random variables with mean \\mu and variance \\sigma^{2}. Then the sample mean \\bar{X}_{n} is defined as\n\n\\bar{X}_{n} = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i}.\n\n\nThe sample mean is a random variable since it is a function of random variables. The expectation and variance of the sample mean can be calculated\n\n\\mathbb{E}_{\\bar{X}_{n}} [\\bar{X}_{n}] = \\mathbb{E}_{X_{1}, \\dots, X_{n}} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i}\n\\right]\n= \\frac{ 1 }{ n } \\mathbb{E}_{X_{i}} [X_{i}]\n= \\frac{ 1 }{ n } n \\mu\n= \\mu,\n\n\n\\mathrm{Var} [\\bar{X}_{n}] = \\mathrm{Var} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i}\n\\right]\n= \\frac{ 1 }{ n^{2} } \\sum_{i = 1}^{n} \\mathrm{Var} [X_{i}]\n= \\frac{ 1 }{ n^{2} } n \\sigma^{2}\n= \\frac{ \\sigma^{2} }{ n }."
  },
  {
    "objectID": "Probability and Statistics/08_Limit_Theorems.html#law-of-large-numbers-lln",
    "href": "Probability and Statistics/08_Limit_Theorems.html#law-of-large-numbers-lln",
    "title": "22  Limit Theorems",
    "section": "Law of large numbers (LLN)",
    "text": "Law of large numbers (LLN)\nThere are two versions laws of large numbers, both of which state that the the sample mean of n i.i.d random variables converges to their mean \\mu, that is, as n get larger, the sample mean is getting closer to \\mu.\n\nTheorem 22.1 (Weak law of large number (WLLN)) Let \\bar{X}_{n} be the sample mean of n i.i.d random variables X_{1}, \\dots , X_{n} with mean \\mu. Then \\bar{X}_{n} converges in probability to \\mu\n\n\\lim_{n \\to \\infty} \\mathbb{P} (\\lvert \\bar{X}_{n} - \\mu \\rvert &gt; \\epsilon) = 0, \\quad \\epsilon &gt; 0.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet X_{1}, \\dots, X_{n} be i.i.d random variables with finite mean \\mu and finite variance \\sigma^{2}. Then for any a &gt; 0\n\n\\mathbb{P}_{X_{1}, \\dots, X_{n}} \\left(\n    \\left\\lvert\n        \\frac{\\sum_{i = 1}^{n} x_{i}}{n} - \\mu\n        \\right\\rvert \\geq \\epsilon\n\\right) \\leq \\frac{\\sigma^{2}}{n \\epsilon^{2}}\n\nApplying the Chebyshev’s inequality Theorem 20.2 over multiple random variables, we get the following for any t &gt; 0,\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\sum_{i = 1}^{n} X_{i} - \\sum_{i = 1}^{n} \\mu_{i}\n    \\right\\rvert \\geq t\n\\right)\n& \\leq \\frac{ \\sum_{i = 1}^{n} \\sigma_{i}^{2} }{ t^{2} }\n\\\\\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\sum_{i = 1}^{n} X_{i} - n \\mu\n    \\right\\rvert \\geq t\n\\right)\n& \\leq \\frac{ n \\sigma^{2} }{ t^{2} }.\n\\\\\n\\end{aligned}\n\nSetting t = n \\epsilon,\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\sum_{i = 1}^{n} X_{i} - n \\mu\n    \\right\\rvert \\geq n \\epsilon\n\\right)\n& \\leq \\frac{n \\sigma^{2}}{n^{2} \\epsilon^{2}}\n\\\\\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\frac{ \\sum_{i = 1}^{n} X_{i} }{ n } - \\mu\n    \\right\\rvert \\geq \\epsilon\n\\right)\n& \\leq \\frac{\\sigma^{2}}{n \\epsilon^{2}}.\n\\\\\n\\mathbb{P} \\left(\n    \\lvert \\bar{X}_{n} - \\mu \\rvert \\geq \\epsilon\n\\right)\n& \\leq \\frac{\\sigma^{2}}{n \\epsilon^{2}}.\n\\end{aligned}\n\nWe can get WLLN by taking the limit n \\to \\infty\n\n\\lim_{n \\to \\infty} \\mathbb{P} \\left(\n    \\lvert \\bar{X}_{n} - \\mu \\rvert \\geq \\epsilon\n\\right) = 0.\n\n\n\n\n\nTheorem 22.2 (Strong law of large number (SLLN)) Let \\bar{X}_{n} be the sample mean of n i.i.d random variables X_{1}, \\dots , X_{n} with mean \\mu. Then \\bar{X}_{n} converges almost surely to \\mu\n\n\\mathbb{P} (\\lim_{n \\to \\infty} \\bar{X}_{n} = \\mu) = 1.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\nWLLN is form of convergence in probability, while SLLN is form of almost sure convergence. Therefore, SLLN is a stronger version than the WLLN."
  },
  {
    "objectID": "Probability and Statistics/08_Limit_Theorems.html#central-limit-theorems",
    "href": "Probability and Statistics/08_Limit_Theorems.html#central-limit-theorems",
    "title": "22  Limit Theorems",
    "section": "Central limit theorems",
    "text": "Central limit theorems\n\nTheorem 22.3 (Central limit theorem (CLT)) Let \\bar{X}_{n} be the sample mean of n i.i.d random variables X_{1}, \\dots , X_{n} with mean \\mu and variance \\sigma^{2}. If n goes to infinite, then \\bar{X}_{n} follows a Gaussian distribution with mean \\mu and \\frac{ \\sigma^{2} }{ n },\n\n\\bar{X}_{n} \\sim \\mathcal{N} \\left(\n    \\mu, \\frac{ \\sigma^{2} }{ n }\n\\right).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\nAlthough CLT is a form of convergence in distribution, which is known to be a weaker version of convergence than convergence in probability and almost sure convergence, it doesn’t mean that CLT is a weaker version of SLLN or WLLN."
  },
  {
    "objectID": "Probability and Statistics/09_Maximum_Likelihood_Estimation.html#maximum-likelihood",
    "href": "Probability and Statistics/09_Maximum_Likelihood_Estimation.html#maximum-likelihood",
    "title": "23  Maximum Likelihood Estimation",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\n\nLikelihood function\nGiven the dataset \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}, the term likelihood function simply refers to the probability function of \\mathcal{X} with the distribution parameters for \\mathbf{X} being \\boldsymbol{\\theta}\n\nf(\\boldsymbol{\\theta}) = \\mathbb{P}_{\\mathbf{X}}(\\mathcal{X} ; \\boldsymbol{\\theta}),\n\nwhich measures how likely are the parameters \\boldsymbol{\\theta} given the data \\mathcal{X}.\nNotes:\n\nSince the dataset are known, the probability function is only dependent on the parameters \\boldsymbol{\\theta}, and thus doesn’t have the same shape as the probability density of the variable \\mathbf{X}.\nThe semicolon indicate that \\boldsymbol{\\theta} is a parameter (fixed value) instead of a random variable.\n\n\n\nMLE Procedures\n\nWe collect a set of instances \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\} that we believe should be from the same distribution.\nWe select a parametric model \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta}) that we think can best explains the data.\nWe select the parameters \\boldsymbol{\\theta}^{*} to be the ones that maximize the probability of the data:\n\n\n\\begin{aligned}\n\\boldsymbol{\\theta}^{*}\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{\\mathbf{X}}(\\mathcal{X}; \\boldsymbol{\\theta})\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}_{i} ; \\boldsymbol{\\theta}) & [\\mathcal{X} \\text{ are i.i.d samples }]\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\ln \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}_{i} ; \\boldsymbol{\\theta}) & [\\text{the log trick}]\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i = 1}^{n} \\log \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}_{i} ; \\boldsymbol{\\theta}).\n\\end{aligned}\n\n\n\nOptimization methods\nIf the likelihood function is concave, the best parameters \\boldsymbol{\\theta}^{*} that maximize the likelihood function are the ones that make the gradient of f(\\boldsymbol{\\theta}) with respect to \\boldsymbol{\\theta} 0 and at the same time has negative semidefinite hessian matrix.\n\n\\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} \\mathbb{P}_{\\mathbf{X}}(\\mathcal{X} ; \\boldsymbol{\\theta}) = 0\n\n\n\\nabla_{\\boldsymbol{\\theta}}^{2} f(\\boldsymbol{\\theta}) \\preceq 0\n\nOtherwise, other numerical methods or algorithms might need to employed to solve the optimization problem."
  },
  {
    "objectID": "Probability and Statistics/09_Maximum_Likelihood_Estimation.html#example-linear-regression",
    "href": "Probability and Statistics/09_Maximum_Likelihood_Estimation.html#example-linear-regression",
    "title": "23  Maximum Likelihood Estimation",
    "section": "Example: linear regression",
    "text": "Example: linear regression\nGiven an instance \\mathbf{x} \\in \\mathbb{R}^{d}, the function f (\\cdot) is a linear function if it has the form\n\n\\begin{aligned}\nf (\\mathbf{x})\n& = \\mathbf{w}^{T} \\mathbf{x} + b\n\\\\\n& = \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}},\n\\\\\n\\end{aligned}\n\nwhere\n\n\\mathbf{w} is weight vector and b is the bias term,\n\\boldsymbol{\\theta} is the parameter vector that includes both weights and bias\n\n  \\boldsymbol{\\theta} = [ \\mathbf{w}, b ]^{T},\n  \n\\hat{\\mathbf{x}} is the instance vector appended with a constant 1\n\n  \\hat{\\mathbf{x}} = [ \\mathbf{x}, 1 ]^{T}.\n  \n\nGiven a dataset with instances \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\} and labels \\mathcal{Y} = \\{ y_{1}, \\dots, y_{n} \\}, linear regression is often formulated as a problem of finding parameters \\boldsymbol{\\theta} that minimize the mean squared error (MSE) loss function\n\n\\begin{aligned}\n& \\arg\\min_{\\boldsymbol{\\theta}} L_{\\text{MSE}}\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\frac{1}{n} \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}\n\\right)^{2}\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}\n\\right)^{2}\n\\\\\n\\end{aligned}\n\nwhich can be written in matrix notations by treating \\mathcal{X} as a matrix \\mathbf{X} \\in \\mathbb{R}^{n \\times d} and \\mathcal{Y} \\in \\mathbb{R}^{n \\times 1} as a vector \\mathbf{y}\n\n\\arg\\min_{\\boldsymbol{\\theta}} \\lVert \\mathbf{y} - \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \\rVert^{2}_{2},\n\nwhere \\hat{\\mathbf{X}} is the instance matrix append with a column of 1 in the last column.\n\nLinear regression as MLE\nSolving the optimization problem of linear regression with MSE loss can be formulated as solving MLE of parameters for a univariate normal distribution.\nFirst we can consider the difference \\epsilon between y and \\boldsymbol{\\theta}^{T} \\mathbf{x} for any instance \\mathbf{x} as a random variable that follows the a univariate normal distribution with zero mean and known variance\n\ny - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}} = \\epsilon \\sim \\mathcal{G} \\left(\n    \\epsilon, 0, \\sigma^{2}\n\\right).\n\nSince \\boldsymbol{\\theta}^{T} \\mathbf{x} is a not a random variable (constant), the label y for any instance \\mathbf{x} is thus a univariate normal random variable with its mean being the predicted label of the linear function\n\n\\begin{aligned}\ny\n& = \\epsilon + \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}\n\\\\\n& \\sim \\mathcal{G} \\left(\n    y, \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}, \\sigma^{2}\n\\right),\n\\end{aligned}\n\nIn other words, the conditional probability of any label given the instance \\mathbf{x} follows a univariate normal distribution\n\n\\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y \\mid \\mathbf{x}\n\\right) = \\mathcal{G} \\left(\n    y, \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}, \\sigma^{2}\n\\right).\n\nGiven a set of instances \\mathcal{X} and their labels \\mathcal{Y}, \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left( y \\mid \\mathbf{x} \\right) is a likelihood function of parameters \\boldsymbol{\\theta} and thus MLE can be used to find the best parameters, which can be shown to have the optimization problem of fitting a linear function with MSE loss\n\n\\begin{aligned}\n\\boldsymbol{\\theta}^{*}\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y \\mid \\mathbf{x}\n\\right)\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n\\right)^{2}\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\lVert \\mathbf{y} - \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \\rVert^{2}_{2}.\n\\end{aligned}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\boldsymbol{\\theta}^{*}\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y \\mid \\mathbf{x}\n\\right)\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\log \\mathcal{G} \\left(\n    y, \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}, \\sigma^{2}\n\\right)\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\log - \\frac{\n    1\n}{\n    \\sqrt{2 \\pi \\sigma^{2}}\n} \\exp{ - \\frac{\n    \\left(\n        y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n    \\right)^{2}\n}{\n    2 \\sigma^{2}\n}}\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} - \\frac{1}{2} \\log \\left(\n    2 \\pi \\sigma^{2}\n\\right) - \\sum_{i=1}^{n} \\frac{\n    \\left(\n        y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n    \\right)^{2}\n}{\n    2 \\sigma^{2}\n}\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} - \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n\\right)^{2}\n& [\\text{only need term with } \\boldsymbol{\\theta}]\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n\\right)^{2}\n\\\\\n\\end{aligned}\n\n\n\n\n\n\nSolving linear regression\nSolving linear regression is the same in both formulations by setting the gradient w.r.t \\boldsymbol{\\theta} to 0 and solve for \\boldsymbol{\\theta}\n\n\\begin{aligned}\n\\nabla_{\\boldsymbol{\\theta}} \\lVert \\mathbf{y} - \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \\lVert^{2}_{2}\n& = 0\n\\\\\n2 \\left(\n    \\hat{\\mathbf{X}}^{T} \\hat{\\mathbf{X}} \\boldsymbol{\\theta} - \\hat{\\mathbf{X}}^{T} \\mathbf{y}\n\\right)\n& = 0\n\\\\\n\\hat{\\mathbf{X}}^{T} \\hat{\\mathbf{X}} \\boldsymbol{\\theta}\n& = \\hat{\\mathbf{X}}^{T} \\mathbf{y}\n\\\\\n\\boldsymbol{\\theta}\n& = \\left(\n    \\hat{\\mathbf{X}}^{T} \\hat{\\mathbf{X}}\n\\right)^{-1} \\hat{\\mathbf{X}}^{T} \\mathbf{y}.\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Probability and Statistics/10_Bayesian_Estimation.html#views-on-parameter-estimation",
    "href": "Probability and Statistics/10_Bayesian_Estimation.html#views-on-parameter-estimation",
    "title": "24  Bayesian Estimation",
    "section": "Views on parameter estimation",
    "text": "Views on parameter estimation\nThere are two different frameworks on statistical inferences: frequentist view and Bayesian view. Both views have the same definition of the probability, but they have different views on how probability of an event should be calculated or accessed. Thus, the way to do parameter estimation is different under the two frameworks.\n\nFrequentist view\nFrequentists believe that the probability of an event is a measure of relative frequency and should be calculated by observing how many times the event happens in a large number of trials.\n\nProbability: the probability of any event is objective and doesn’t change with different beliefs to the event.\nParameters: if the parameters of the distribution are unknown, the parameters must be fixed constants. That is, they must be certain determined values.\nEstimation: the single best estimation of the parameter can be derived using single dataset and its goodness (bias and variance) can be measured by sampling different datasets.\n\n\n\nBayesian view\nBayesian approach believes that a probability of an event includes not only the relative frequency, but also the subjective beliefs. That is, the degree of belief on the outcomes of the experiment.\n\nProbability: the subjective beliefs can be very different from person to person, and thus the probability of any event is very subjective.\nParameters: the unknown parameters of the distribution are viewed as random variables, and thus include subjective beliefs.\nEstimation: the subjective beliefs of the parameters are specified using a prior distribution, which is then updated using the single dataset observed. The result of the estimation is a posterior probabilities of a range of parameter values, which include both prior beliefs and relative frequency."
  },
  {
    "objectID": "Probability and Statistics/10_Bayesian_Estimation.html#bayesian-estimation",
    "href": "Probability and Statistics/10_Bayesian_Estimation.html#bayesian-estimation",
    "title": "24  Bayesian Estimation",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nIn Bayesian estimation, the unknown parameters are treated as random variables.\n\nThe subjective beliefs about the parameters that we want to estimate before observing any dataset are encoded using a distribution\n\n  \\mathbb{P}_{\\boldsymbol{\\Theta}} (\\boldsymbol{\\theta}),\n  \nwhich specifies the prior probabilities of all possible values of the parameters.\nThe likelihood of a dataset \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\} given parameters \\boldsymbol{\\theta} is a conditional probability\n\n  \\mathbb{P}_{\\mathbf{X} \\mid \\boldsymbol{\\Theta}} \\left(\n      \\mathcal{X} \\mid \\boldsymbol{\\theta}\n  \\right).\n  \n\nUnlike maximum likelihood estimation, which is under frequentist view and gives only the single best parameter, the result of Bayesian estimation is a posterior distribution that informs us how the observed data update the prior. According to Bayes Theorem, the posterior distribution can be calculated by:\n\n\\mathbb{P}_{\\boldsymbol{\\Theta} \\mid \\mathbf{X}} \\left(\n    \\boldsymbol{\\theta} \\mid \\mathcal{X}\n\\right) = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid \\boldsymbol{\\Theta}} \\left (\n        \\mathcal{X} \\mid \\boldsymbol{\\theta}\n    \\right) \\mathbb{P}_{\\boldsymbol{\\Theta}} \\left(\n        \\boldsymbol{\\theta}\n    \\right)\n}{\n    \\mathbb{P}_{\\mathbf{X}} \\left(\n        \\mathcal{X}\n    \\right)\n}.\n\n\nMaximum a posteriori (MAP) estimation\nIn the case where we want a single estimate for the parameter using Bayesian estimation, MAP estimation chooses the value of the parameter that has the largest probability in the posterior distribution\n\n\\boldsymbol{\\theta}_{MAP} = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{\\boldsymbol{\\Theta} \\mid \\mathbf{X}} \\left(\n    \\boldsymbol{\\theta} \\mid \\mathcal{X}\n\\right)."
  },
  {
    "objectID": "Probability and Statistics/10_Bayesian_Estimation.html#example-mean-of-the-univariate-gaussian",
    "href": "Probability and Statistics/10_Bayesian_Estimation.html#example-mean-of-the-univariate-gaussian",
    "title": "24  Bayesian Estimation",
    "section": "Example: mean of the univariate Gaussian",
    "text": "Example: mean of the univariate Gaussian\nHere we shows an example of estimating the posterior probability of the mean parameter of a univariate normal distribution using Bayesian estimation.\nConsider the univariate case where the probability of the instance x follows a normal distribution with unknown mean \\mu and known variance \\sigma^{2}:\n\n\\mathbb{P}_{X \\mid \\mu} (x \\mid \\mu) \\sim \\mathcal{N} (\\mu, \\sigma^{2}) = \\mathcal{G} (x, \\mu, \\sigma^{2}),\n\nand we assume whatever prior knowledge we have about \\mu can be expressed by another normal distribution with known mean \\mu_{0} and known variance \\sigma_{0}^{2}:\n\n\\mathbb{P}_{\\mu} (\\mu) \\sim \\mathcal{N} (\\mu_{0}, \\sigma_{0}^{2}) = \\mathcal{G} (\\mu, \\mu_{0}, \\sigma_{0}^{2}).\n\n\nPosterior distribution of mean parameter\nSuppose now that n samples \\mathcal{X} = \\{x_{1}, \\dots, x_{n}\\} are independently sampled. We can use Bayes formula to obtain the posterior probability:\n\n\\mathbb{P}_{\\mu \\mid \\mathcal{X}} (\\mu \\mid \\mathcal{X}) = \\frac{\n    \\mathbb{P}_{X \\mid \\mu} (\\mathcal{X} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu)\n}{\n    \\mathbb{P}_{X} (\\mathcal{X})\n}.\n\nSince \\mathbb{P}_{X}(\\mathcal{X}) is a normalization factor that doesn’t depend on \\mu, we now omit it for simplicity:\n\n\\begin{aligned}\n\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X})\n& \\propto \\mathbb{P}_{X \\mid \\mu} (\\mathcal{X} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu)\n\\\\\n& = \\prod_{i=1}^{n} \\mathbb{P}_{X \\mid \\mu} (x_{i} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu)\n& [\\text{independent assumption}].\n\\\\\n\\end{aligned}\n\nAfter expanding the normal distribution definition and some simplifications, we can see that \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) also follows a normal distribution:\n\n\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) \\sim \\mathcal{N} (\\mu_{n}, \\sigma_{n}^{2})\n\nwhere\n\n\\mu_{n} = \\frac{\n    \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n}{\n    \\sigma^{2} + n \\sigma_{0}^{2}\n},\n\n\n\\sigma_{n}^{2} = \\frac{\n    \\sigma_{0}^{2} \\sigma^{2}\n}{\n    n \\sigma_{0}^{2} + \\sigma^{2}\n}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\mathbb{P}_{\\mu \\mid \\mathcal{D}} (\\mu \\mid \\mathcal{D})\n& \\propto \\mathbb{P}_{\\mathcal{D} \\mid \\mu} (\\mathcal{D} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu)\n\\\\\n& = \\prod_{i=1}^{n} \\mathbb{P}_{X \\mid \\mu} (x_{i} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu)\n\\\\\n& = \\prod_{i=1}^{n} \\frac{ 1 }{ \\sqrt{2 \\pi \\sigma^{2}} } \\exp{- \\frac{\n    (x_{i} - \\mu)^{2}\n}{\n    2 \\sigma^{2}\n}} \\frac{ 1 }{ \\sqrt{2 \\pi \\sigma_{0}^{2}} } \\exp{- \\frac{\n    (\\mu - \\mu_{0})^{2}\n}{\n    2 \\sigma_{0}^{2}\n}}\n\\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\prod_{i=1}^{n} \\exp{\\left[\n    - \\frac{ (x_{i} - \\mu)^{2} }{ 2 \\sigma^{2} } - \\frac{ (\\mu - \\mu_{0})^{2} }{ 2 \\sigma_{0}^{2} }\n\\right] }\n& [\\text{merging constants and exponential}]\n\\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{\\left[\n    \\sum_{i=1}^{n} - \\frac{\n        (x_{i} - \\mu)^{2}\n    }{\n        2 \\sigma^{2}\n    } - \\frac{\n        (\\mu - \\mu_{0})^{2}\n    }{\n        2 \\sigma_{0}^{2}\n    }\n\\right]}\n\\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{\\left[\n    \\sum_{i=1}^{n} - \\frac{\n        x_{i}^{2} - 2 x_{i} \\mu + \\mu^{2}\n    }{\n        2 \\sigma^{2}\n    } - \\frac{\n        \\mu^{2} - 2 \\mu \\mu_{0} + \\mu_{0}^{2}\n    }{\n        2 \\sigma_{0}^{2}\n    }\n\\right]}\n& [\\text{expanding squares}]\n\\\\\n& = \\frac{ 1 }{\\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{\\left[\n    - \\frac{ \\sum_{i=1}^{n} \\left[\n        x_{i}^{2} - 2 x_{i} \\mu + \\mu^{2}\n    \\right] }{ 2 \\sigma^{2} } - \\frac{\n        \\mu^{2} - 2 \\mu \\mu_{0} + \\mu_{0}^{2}\n    }{\n        2 \\sigma_{0}^{2}\n    }\n\\right]}\n\\\\\n& = \\frac{ 1 }{\\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[\n    - \\frac{\n        \\sum_{i=1}^{n} x_{i}^{2} - \\sum_{i=1}^{n} 2 x_{i} \\mu + n \\mu^{2}\n    }{\n        2 \\sigma^{2}\n    } - \\frac{\n        \\mu^{2} - 2 \\mu \\mu_{0} + \\mu_{0}^{2}\n    }{\n        2 \\sigma_{0}^{2}\n    }\n\\right] }\n& [\\text{reordering sums}]\n\\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{\\left[\n        - \\frac{ \\sum_{i=1}^{n} x_{i}^{2}\n    }{\n        2 \\sigma^{2}\n    } + \\frac{\n        \\sum_{i=1}^{n} 2 x_{i} \\mu\n    }{\n        2 \\sigma^{2}\n    } - \\frac{\n        n \\mu^{2}\n    }{\n        2 \\sigma^{2}\n    } - \\frac{ \\mu^{2} }{ 2 \\sigma_{0}^{2} } + \\frac{\n        2 \\mu \\mu_{0}\n    }{\n        2 \\sigma_{0}^{2}\n    } - \\frac{ \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} }\n\\right]}\n\\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{\\left[\n    - \\left( \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \\right) \\mu^{2} + 2 \\left(\n        \\frac{\n            \\sum_{i=1}^{n} x_{i}\n        }{\n            2 \\sigma^{2}\n        } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} }\n    \\right) \\mu - \\frac{\n        \\sum_{i=1}^{n} x_{i}^{2}\n    }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} }\n\\right] }\n& [\\text{grouping } \\mu]\n\\\\\n& = \\frac{ \\exp{ \\left[\n    - \\frac{ \\sum_{i=1}^{n} x_{i}^{2} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} }\n\\right] } }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{\\left[\n    - \\left(\n        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n    \\right) \\mu^{2} + 2 \\left(\n        \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} }\n    \\right) \\mu\n\\right] }\n& [\\text{extracting out terms without } \\mu]\n\\\\\n& \\propto \\exp{ \\left[\n    - \\left(\n        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n    \\right) \\mu^{2} + 2 \\left(\n        \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} }\n    \\right) \\mu\n\\right] }\n& [\\text{removing terms without } \\mu]\n\\\\\n\\end{aligned}\n\nUsing the completing the squares trick\n\n\\begin{aligned}\nax^{2} + 2bx + c  \n& = a \\left(\n    x^{2} + 2 \\frac{ b }{ a } x + \\frac{ c }{ a }\n\\right)\n\\\\\n& = a \\left(\n    x^{2} + 2 \\frac{ b }{ a } x + \\left(\n        \\frac{ b }{ a }\n    \\right)^{2} - \\left(\n        \\frac{ b }{ a }\n    \\right)^{2} + \\frac{ c }{ a }\n\\right)\n\\\\\n& = a \\left(\n    x + \\frac{ b }{ a }\n\\right)^{2} + c - \\frac{ b^{2} }{ a },\n\\\\\n\\end{aligned}\n\nand treating\n\na = - \\left(\n    \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n\\right),\n\n\nb = \\left(\n    \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} }\n\\right),\n\nwe can have\n\n\\begin{aligned}\n\\mathbb{P}_{\\mu \\mid \\mathcal{D}} (\\mu \\mid \\mathcal{D})\n& \\propto \\exp{ \\left[\n    - \\left(\n        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n    \\right) \\mu^{2}\n    + 2 \\left(\n        \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} }\n    \\right) \\mu\n\\right] }\n\\\\\n& \\propto \\exp{ \\left[\n    - \\left(\n        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n    \\right)\n    \\left(\n        \\mu - \\frac{\n            \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} }\n        }{\n            \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n        }\n    \\right)^{2}\n\\right] }\n& [\\text{remove } \\frac{ b^{2} }{ a } \\text{ as it doesn't depend on } \\mu]\n\\\\\n& = \\exp{ \\left[\n    - \\left(\n        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n    \\right)\n    \\left(\n        \\mu - \\frac{\n            \\frac{\n                \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n            }{\n                2 \\sigma^{2} \\sigma_{0}^{2}\n            }\n        }{\n            \\frac{\n                \\sigma^{2} + n \\sigma_{0}^{2}\n            }{\n                2 \\sigma^{2} \\sigma_{0}^{2}\n            }\n        }\n    \\right)^{2}\n\\right] }\n\\\\\n& = \\exp{ \\left[\n    - \\left(\n        \\frac{ 2 \\sigma^{2} \\sigma_{0}^{2} }{ \\sigma^{2} + n \\sigma_{0}^{2} }\n    \\right)^{-1}\n    \\left(\n        \\mu - \\frac{\n            \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n        }{\n            \\sigma^{2} + n \\sigma_{0}^{2}\n        }\n    \\right)^{2}\n\\right] }\n.\n\\\\\n\\end{aligned}\n\n\n\n\nThis means that the unknown parameter \\mu estimated by a set of instances \\mathcal{X} using Bayesian estimation has a probability \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) that follows a normal distribution that has mean \\mu_{n} and variance \\sigma_{n}^{2}.\n\nSince \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) is a normal distribution, \\mathbb{P}_{\\mu \\mid X} (\\mu_{n} \\mid \\mathcal{X}) is largest and thus we can view \\mu_{n} as our best guess for \\mu after observing \\mathcal{X}.\nThen we can view the variance \\sigma_{n}^{2} as the uncertainty about this best guess.\nSince \\sigma_{n}^{2} decreases monotonically with n, each additional observation decreases our uncertainty of the best guess.\n\n%%markdown\nSince \\mu_{n} can be further rewritten as\n\n\\begin{aligned}\n\\mu_{n}\n& = \\frac{\n    \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n}{\n    \\sigma^{2} + n \\sigma_{0}^{2}\n}\n\\\\\n& = \\frac{\n    \\sigma_{0}^{2}\n}{\n    \\sigma^{2} + n \\sigma_{0}^{2}\n} \\sum_{i=1}^{n} x_{i} + \\frac{\n    \\sigma^{2}\n}{\n    \\sigma^{2} + n \\sigma_{0}^{2}\n} \\mu_{0}\n\\\\\n& = \\frac{\n    n \\sigma_{0}^{2}\n}{\n    n \\sigma_{0}^{2} + \\sigma^{2}\n} \\bar{x}_{n} + \\left(\n    1 - \\frac{\n        n \\sigma_{0}^{2}\n    }{\n        n \\sigma_{0}^{2} + \\sigma^{2}\n    }\n\\right) \\mu_{0}\n& [\\bar{x}_{n} = \\frac{ 1 }{ n } \\sum_{i=1}^{n} x_{i}]\n\\\\\n& = \\alpha_{n} \\bar{x}_{n} + (1 - \\alpha_{n}) \\mu_{0}\n& [\\alpha_{n} = \\frac{\n    n \\sigma_{0}^{2}\n}{\n    n \\sigma_{0}^{2} + \\sigma^{2}\n}],\n\\\\\n\\end{aligned}\n\nthe final equation shows that \\mu_{n} is a combination of the maximum likelihood estimate \\bar{x}_{n} and the prior information \\mu_{0}.\nSince\n  \n\\begin{aligned}\n\\lim_{n \\to \\infty} \\mu_{n}\n& = \\bar{x}_{n}\n& [\\lim_{n \\to \\infty} \\frac{ n \\sigma_{0}^{2} }{ n \\sigma_{0}^{2} + \\sigma^{2} } = 1]\n\\\\\n\\lim_{n \\to 0} \\mu_{n}\n& = \\mu_{0}\n& [\\lim_{n \\to 0} \\frac{ n \\sigma_{0}^{2} }{ n \\sigma_{0}^{2} + \\sigma^{2} } = 0],\n\\end{aligned}\n\n\nIf there is large amount of data, \\mu_{n} converges to maximum likelihood estimate.\nIf there is no observed data, \\mu_{n} converges to the mean of the prior knowledge.\n\nIf the number of sampled data n is fixed,\n  \n\\begin{aligned}\n\\lim_{\\sigma_{0} \\to \\infty} \\mu_{n}\n& = \\bar{x}_{n}\n& [\\lim_{\\sigma_{0} \\to \\infty} \\frac{ n \\sigma_{0}^{2} }{ n \\sigma_{0}^{2} + \\sigma^{2} } = 1]\n\\\\\n\\lim_{\\sigma_{0} \\to 0} \\mu_{n}\n& = \\mu_{0}\n& [\\lim_{\\sigma \\to \\infty} \\frac{ n \\sigma_{0}^{2} }{ n \\sigma_{0}^{2} + \\sigma^{2} } = 0],\n\\end{aligned}\n\nwhich means\n\n\\mu_{n} will converge to the maximum likelihood estimate \\bar{x}_{n} if our prior knowledge of the \\mu indicates that we have no certainty about \\mu_{n} (infinite variance).\n\\mu_{n} will converge to the mean of our prior knowledge \\mu_{0} if our prior knowledge of the \\mu indicates that we are very certain about \\mu_{0} (zero variance).\n\n\n\nPredictive distribution function\n\n\\begin{aligned}\n\\mathbb{P}_{X \\mid X}(x \\mid \\mathcal{X})\n& = \\int \\mathbb{P}_{X \\mid \\mu}(x \\mid \\mu) \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) \\mathop{d \\mu}\n\\\\\n& \\sim \\mathcal{N}(\\mu_{n}, \\sigma^{2} + \\sigma_{n}^{2})\n\\end{aligned}\n\n\n\nSelecting parameter priors\nWhen the number of samples n is large, the predictive distribution will not change much if we select different mean parameter priors. Consider the 2 extreme cases of the mean parameter priors.\n\nUniform prior (normal distribution with infinite variance).\nSince\n\n\\begin{aligned}\n\\lim_{\\sigma_{0}^{2} \\to \\infty} \\mu_{n}\n& = \\bar{x}_{n},\n\\\\\n\\lim_{\\sigma_{0}^{2} \\to \\infty} \\sigma_{n}\n& = \\frac{ \\sigma^{2} }{ n },\n\\end{aligned}\n\nthe predictive distribution is\n\n\\mathbb{P}_{X \\mid X}(x \\mid \\mathcal{X}) \\sim \\mathcal{N} \\left(\n     \\bar{x}_{n}, \\sigma^{2} + \\frac{ \\sigma^{2} }{ n }\n\\right).\n\nDirac delta prior (normal distribution with zero variance).\nSince\n\n\\begin{aligned}\n\\lim_{\\sigma_{0} \\to 0} \\mu_{n}\n& = \\mu_{0},\n\\\\\n\\lim_{\\sigma_{0} \\to 0} \\sigma_{n}\n& = 0,\n\\end{aligned}\n\nthe predictive distribution is\n\n\\mathbb{P}_{X \\mid X}(x \\mid \\mathcal{X}) \\sim \\mathcal{N} \\left(\n     \\mu_{0}, \\sigma^{2} \\right\n).\n\nSince \\mu_{0} is \\bar{x}_{n} with extra points, \\mu_{0} = \\bar{x}_{n} when n is large."
  },
  {
    "objectID": "Probability and Statistics/10_Bayesian_Estimation.html#references",
    "href": "Probability and Statistics/10_Bayesian_Estimation.html#references",
    "title": "24  Bayesian Estimation",
    "section": "References",
    "text": "References\n\nhttps://www.bu.edu/sph/files/2014/05/Bayesian-Statistics_final_20140416.pdf"
  },
  {
    "objectID": "Probability and Statistics/11_Expectation_Maximization.html#the-em-algorithm",
    "href": "Probability and Statistics/11_Expectation_Maximization.html#the-em-algorithm",
    "title": "25  Expectation-maximization",
    "section": "The EM algorithm",
    "text": "The EM algorithm\n\nProblem statements\nIn EM problems, we have two types of variables:\n\nObserved variables: a set of random variables \\mathbf{X} from which we can draw samples.\nHidden variables: another set of random variables \\mathbf{Z} from which we cannot draw samples.\n\nAlthough we cannot draw samples from \\mathbf{Z}, we assume there exists a \\mathbf{z} for each \\mathbf{x} observed. Thus we define two types of datasets.\n\nIncomplete data: samples drawn from the observed variables\n\n  \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}.\n  \nComplete data: the sample pairs of the observed and hidden variables that we never be able to observe\n\n  \\mathcal{X}, \\mathcal{Z} = \\{ (\\mathbf{x}_{1}, \\mathbf{z}_{1}), \\dots, (\\mathbf{x}_{n}, \\mathbf{z}_{n}) \\}.\n  \n\nGiven the incomplete data \\mathcal{X}, the goal is to find the maximum likelihood estimate of parameters \\Psi for the log-likelihood of the incomplete data \\mathcal{X}\n\n\\Psi^{*} = \\arg\\max_{\\Psi} \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi).\n\n\nIf we knew what the model \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}) is without introducing other variables, the problem is a standard MLE problem and usually can be easily solved.\nIf instead the joint probability \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathbf{x}, \\mathbf{z}) is known or can be derived, we can still solve the problem by maximizing the equation that marginalizes out the latent variables from the joint probability\n\n  \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) = \\log \\int \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n      \\mathcal{X}, \\mathcal{Z}; \\Psi\n  ) \\partial \\mathbf{z},\n  \nwhich in most of the cases doesn’t have a closed-form solution and thus needs to be solved using iterative algorithms.\nEM is one of such algorithm with some unique properties that other algorithms don’t have.\n\n\n\nEM procedures\n\nE-Step\nGiven parameters \\hat{\\Psi} estimated at the current iteration and incomplete data \\mathcal{X}, compute the Q function, which is the expected value of the log likelihood of the complete data \\mathcal{X}, \\mathcal{Z} over the distribution of \\mathbf{Z}\n\nQ_{\\hat{\\Psi}} (\\Psi) = \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z} ; \\Psi) \\mid \\mathcal{X}\n\\right].\n\n\nSince \\mathcal{X} is given but we never be able to see \\mathcal{Z}, the log-likelihood is a function of both \\Psi and \\mathbf{Z},\n\n  L(\\mathcal{Z}, \\Psi) = \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \\mid \\mathcal{X}.\n  \nThe Q function only depend on \\Psi but not \\mathcal{Z} because taking the expected value of L(\\mathcal{Z}, \\Psi) over the distribution of \\mathbf{Z} eliminates \\mathcal{Z} in the expression.\nThe estimate of the parameter \\hat{\\Psi} in the current iteration is used in calculating the probability of \\mathcal{Z} given \\mathcal{X}, which is needed in the expectation calculation process.\nDepending the problem setups, the expressions for both \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) and \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi}) (used in \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}; \\hat{\\Psi}} [\\cdot]) should be already known or can be derived from the known probabilistic models between \\mathbf{X} and \\mathbf{Z}.\n\n\n\nM-Step\nFind the parameter \\hat{\\Psi} that maximizes the expected value derived in the E-step.\n\n\\hat{\\Psi} = \\arg\\max_{\\Psi} Q_{\\hat{\\Psi}} (\\Psi)\n\nRepeat the E-step and M-step until convergence."
  },
  {
    "objectID": "Probability and Statistics/11_Expectation_Maximization.html#derivation-of-em",
    "href": "Probability and Statistics/11_Expectation_Maximization.html#derivation-of-em",
    "title": "25  Expectation-maximization",
    "section": "Derivation of EM",
    "text": "Derivation of EM\nThe derivation of the EM algorithm mostly comes from the following observation: the log-likelihood of the incomplete data can be decomposed into 2 components\n\n\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) = G (\\Psi, q) +  D (\\Psi, q)\n\nwhere q is an arbitrary distribution of the latent variables.\n\nThe lower bound of EM\n\n  G (\\Psi, q) = \\mathbb{E}_{\\mathbf{Z}} \\left[\n      \\log \\frac{\n          \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n      }{\n          q (\\mathcal{Z})\n      }\n  \\right]\n  \nThe KL-divergence of between two distributions of \\mathbf{Z}, i.e. q(\\mathcal{Z}) and \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n\n  D (\\Psi, q) = \\mathrm{KL} \\left[\n      q \\mathrel{\\Vert} \\mathbb{P}_{\\mathbf{X} \\mid \\mathbf{Z}}\n  \\right] = \\mathbb{E}_{\\mathbf{Z}} \\left[\n      \\log \\frac{\n          q (\\mathcal{Z})\n      }{\n          \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n      }\n  \\right].\n  \n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\log \\mathbb{P}_\\mathbf{X} (\\mathbf{x}) is not dependent on \\mathbf{Z}, taking its expectation over any distribution of \\mathbf{Z} is equal to itself. Thus,\n\n\\begin{aligned}\n\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)\n\\right]\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        q (\\mathcal{Z})\n    }{\n        q (\\mathcal{Z})\n    } \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n    }{\n        q (\\mathcal{Z})\n    } \\frac{\n        q (\\mathcal{Z})\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n    }{\n        q (\\mathcal{Z})\n    }\n\\right] + \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        q (\\mathcal{Z})\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = G (\\Psi, q) + D (\\Psi, q)\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n\\right] - \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log q (\\mathcal{Z})\n\\right] + \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        q (\\mathcal{Z})\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = Q (\\Psi, q) + \\mathrm{H} (q) + D (\\Psi, q)\n\\end{aligned}\n\n\n\n\n\nLower bound\nSince KL-divergence D (\\Psi, q) is proved to be non-negative, G(\\Psi, q) can be seen as a lower bound of \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)\n\n\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) \\geq G(\\Psi, q),\n\nwhich can also be proven using Jensen’s inequality.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)\n& = \\log \\int \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n    \\mathcal{X}, \\mathcal{Z}; \\Psi\n) \\partial \\mathbf{z}\n\\\\\n& = \\log \\int q (\\mathcal{Z}) \\frac{\n    \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n        \\mathcal{X}, \\mathcal{Z}; \\Psi\n    )\n}{\n    q (\\mathcal{Z})\n} \\partial \\mathbf{z}\n\\\\\n& = \\log \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n            \\mathcal{X}, \\mathcal{Z}; \\Psi\n    )\n    }{\n        q (\\mathcal{Z})\n    }\n\\right]\n\\\\\n& \\geq \\mathbb{E}_{\\mathbf{Z}} \\left[ \\log\n    \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n            \\mathcal{X}, \\mathcal{Z}; \\Psi\n        )\n    }{\n        q (\\mathcal{Z})\n    }\n\\right]\n\\\\\n& \\geq G (\\Psi, q)\n\\\\\n\\end{aligned}\n\n\n\n\n\n\nEM as coordinate ascent on lower bound\nThe EM is essentially doing the coordinate ascent on G (\\Psi, q), which is believed to be easier to optimize than directly optimizing \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi), while guaranteeing the \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) is non-decreasing as G (\\Psi, q) is optimized.\nCoordinate ascent is an optimization method that optimize a single variable or one dimension of the variable at a time, while fixing the values of the rest of the variables from the last iteration unchanged. In the case of applying to G (\\Psi, q) function, \\Psi and q are separately maximized in different steps of each iteration.\n\nE-step: given the parameter \\hat{\\Psi} estimated in the last iteration, the choice of the q function is optimized to maximize the value of G (\\Psi, q).\nM-step: given the \\hat{q} function selected in E-step, \\Psi is optimized to maximize the value of G (\\Psi, \\hat{q}) and will be used in the E-step of the next iteration.\n\n\n\nE-step\nSince the value of \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) doesn’t depend on the choice of q, the choice of q only affect the balance between G (\\Psi, q) and D (\\Psi, q) when the \\Psi is fixed.\nThus, given the parameters \\hat{\\Psi} estimated in the last iteration, G (\\hat{\\Psi}, q) is maximized with respect to q when D (\\hat{\\Psi}, q) is minimized. Since the minimized value of D (\\Psi, q) is 0, we have\n\n\\begin{aligned}\nD (\\hat{\\Psi}, q)\n& = 0\n\\\\\n\\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        q (\\mathcal{Z})\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi})\n    }\n\\right]\n& = 0\n\\\\\nq (\\mathcal{Z})\n& = \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi}),\n\\end{aligned}\n\nwhich shows that G (\\hat{\\Psi}) is maximized when the distribution of latent variables is chosen to be the probability of the latent variables given the observed data and the current estimate of the parameters.\nA nice property of optimizing q, even though it doesn’t affect the value of \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) at all, is that the value of KL-divergence D (\\Psi, \\hat{q}) will also be non-decreasing no matter what \\Psi is selected in the M-step by maximizing G (\\Psi, \\hat{q}).\n\nThis can be seen from the fact that D (\\Psi, \\hat{q}) = 0 when \\hat{p} is selected to maximize G (\\hat{\\Psi}, q), and thus any \\Psi will guarantee that the value of D (\\Psi, \\hat{q}) is larger or equal to 0.\nThis property implicitly prove the convergence of the EM algorithm in that both G (\\Psi, q) and D (\\Psi, q) will be non-decreasing during each iteration, and therefore, the value of \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) is non-decreasing in each iteration.\n\n\n\nM-step\nG (\\Psi, q) can be further decomposed into two components\n\nG (\\Psi, q) = Q (\\Psi, q) + \\mathrm{H} (q).\n\n\nThe expected value of the complete data with respect to the distribution q\n\n  Q (\\Psi, q) = \\mathbb{E}_{\\mathbf{Z} \\sim q} \\left[\n      \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n  \\right].\n  \nThe entropy of the latent variables\n\n  \\mathrm{H} (q) = - \\mathbb{E}_{\\mathbf{Z}} \\left[\n      \\log q (\\mathcal{Z})\n  \\right].\n  \n\nSince \\mathrm{H} (q) doesn’t depend on \\Psi, it will stay as a constant in the process of maximizing G (\\Psi, q) with respect to \\Psi.\nGiven \\hat{q} = \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi}), maximizing G (\\Psi, \\hat{q}) is the same as maximizing Q function we defined above:\n\n\\begin{aligned}\n\\arg\\max_{\\Psi} G (\\Psi, \\hat{q})\n& = \\arg\\max_{\\Psi} Q_{\\hat{\\Psi}} (\\Psi)\n\\\\\n& = \\arg\\max_{\\Psi} \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z} ; \\Psi)\n\\right].\n\\end{aligned}"
  },
  {
    "objectID": "Probability and Statistics/11_Expectation_Maximization.html#example-mixture-model",
    "href": "Probability and Statistics/11_Expectation_Maximization.html#example-mixture-model",
    "title": "25  Expectation-maximization",
    "section": "Example: mixture model",
    "text": "Example: mixture model\nOne application of EM algorithm is to obtain MLE of the parameters in a mixture models.\n\nMixture model\nWe say random variable \\mathbf{X} follows a mixture model if its distribution is a weighted combination of multiple components, where each component has a simple parametric distributions. Thus mixture model can represent distributions that cannot be expressed using a single parametric distribution.\nEach sample \\mathbf{x} is associated with a latent random variable z that indicates which component (parametric distribution) that \\mathbf{x} should be drawn. Thus the sample \\mathbf{x} has the conditional probability in a parametric form with parameters \\boldsymbol{\\theta}\n\n\\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x} \\mid z ; \\boldsymbol{\\theta}),\n\nif we know the latent variable z for the sample.\nAssuming in total we have c latent variables for all samples and each latent variable has the probability \\mathbb{P}_{Z} (z), the probability of the sample is\n\n\\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}) = \\sum_{z=1}^{c} \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x} \\mid z) \\mathbb{P}_{Z} (z).\n\nThe Gaussian mixture model is simply a mixture model in which all components are Gaussian distributions.\n\n\nEM for mixture model\nIf we knew what z is for each \\mathbf{x}, the estimate of parameters for each component can be easily derived by sampling a dataset \\mathcal{X}_{z} from the conditional distribution and applying MLE.\nHowever, in practice, we never know which component each sample belongs to, and thus we apply EM by treating \\mathbf{X} as observed variables and Z as the hidden variable.\nThe goal of applying EM is to find the parameters \\Psi in the mixture model including:\n\nThe parameters for the parametric distribution of each component \\{ \\boldsymbol{\\theta}_{1}, \\dots, \\boldsymbol{\\theta}_{c} \\}.\nThe probability of each component \\{ \\pi_{1}, \\dots, \\pi_{c} \\}.\n\n\nE-step: complete data likelihood\nTo derive the EM procedure, we first need to write out the log-likelihood of the complete data in terms of the known parametric distributions\n\n\\begin{aligned}\nL(\\mathcal{Z}, \\Psi)\n& = \\log \\mathbb{P}_{\\mathbf{X}, Z} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n\\\\\n& = \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathcal{X} \\mid \\mathcal{Z} ; \\boldsymbol{\\theta}) \\mathbb{P}_{Z} (\\mathcal{Z})\n\\\\\n& = \\log \\prod_{i=1}^{n} \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid z_{i} ; \\boldsymbol{\\theta}_{z_{i}}) \\pi_{z_{i}}\n& [\\text{i.i.d assumption}],\n\\\\\n\\end{aligned}\n\nwhere x_{i} is a sample, z_{i} indicates the component that x_{i} belongs to, \\boldsymbol{\\theta}_{z_{i}} is the parameters of the distribution for the component z_{i}, and \\pi_{z_{i}} = \\mathbb{P}_{Z} (z_{i}) is the probability of the component z_{i}.\nSince z is discrete and range from 1 to c, any function of z can be written as\n\nf(z) = \\prod_{i=1}^{c} f(i)^{\\mathbb{1} (z = i)},\n\nwhere z is extracted out from the function to the power of the function.\nThus, the complete data likelihood can be further simplified to\n\n\\begin{aligned}\nL(\\mathcal{Z}, \\Psi)\n& = \\log \\prod_{i=1}^{n} \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid z_{i} ; \\boldsymbol{\\theta}_{z_{i}}) \\pi_{z_{i}}\n\\\\\n& = \\log \\prod_{i=1}^{n} \\prod_{j=1}^{c} \\left[\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}) \\pi_{j}\n\\right]^{\\mathbb{1}(z_{i} = j)}\n& [f(z_{i}) = \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid z_{i} ; \\boldsymbol{\\theta}_{z_{i}}) \\pi_{z_{i}}]\n\\\\\n& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\log \\left[\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}) \\pi_{j}\n\\right]^{\\mathbb{1}(z_{i} = j)}\n\\\\\n& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\mathbb{1}(z_{i} = j) \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}) \\pi_{j}.\n\\end{aligned}\n\n\n\nE-step: Q function\nTaking the expectation of complete data log-likelihood over Z gives us the Q function that doesn’t depend on Z\n\n\\begin{aligned}\nQ_{\\hat{\\Psi}} (\\Psi)\n& = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    L(\\mathcal{Z}, \\Psi)\n\\right]\n\\\\\n& = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\mathbb{1}(z_{i} = j) \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n    \\right) \\pi_{j}\n\\right]\n\\\\\n& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\mathbb{1}(z_{i} = j)\n\\right] \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n    \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n\\right) \\pi_{j}\n\\\\\n& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} h_{i, j} \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n    \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n\\right) \\pi_{j},\n\\\\\n\\end{aligned}\n\nwhere h_{i, j} = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[ \\mathbb{1}(z_{i} = j) \\right] is a constant value that can be computed given that we have parameter \\hat{\\Psi} estimated in the last iteration.\n\n\\begin{aligned}\nh_{i, j}\n& = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\mathbb{1} (z_{i} = j)\n\\right]\n\\\\\n& = \\sum_{k=1}^{c} \\mathbb{P}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left(\n    k \\mid \\mathbf{x}_{i}\n\\right)\n\\mathbb{1} (k = j)\n\\\\\n& = \\mathbb{P}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left(\n    j \\mid \\mathbf{x}_{i}\n\\right)\n& [\\mathbb{1} (k = j) = 0 \\text{ for } k \\neq j]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid j; \\hat{\\boldsymbol{\\theta}}_{j}\n    \\right) \\hat{\\pi}_{j}\n}{\n    \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}_{i})\n}\n& [\\text{Bayes' Theorem}]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid j ; \\hat{\\boldsymbol{\\theta}}_{j}\n    \\right) \\hat{\\pi}_{j}\n}{\n    \\sum_{k=1}^{c} \\mathbb{P}_{\\mathbf{X}, Z} \\left(\n        \\mathbf{x}_{i}, k\n    \\right)\n}\n& [\\text{marginalization}]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid j; \\hat{\\boldsymbol{\\theta}}_{j}\n    \\right) \\hat{\\pi}_{j}\n}{\n    \\sum_{k=1}^{c} \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid k; \\hat{\\boldsymbol{\\theta}}_{k}\n    \\right) \\hat{\\pi}_{k}\n}\n\\end{aligned}\n\n\n\nM-step: maximizes Q function\nComputing \\hat{\\Psi}_{\\text{new}} that maximizes Q function for the next iteration is an optimization problem\n\n\\hat{\\Psi}_{\\text{new}} = \\arg\\max_{\\Psi} \\sum_{i=1}^{n} \\sum_{j=1}^{c} h_{i, j} \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n    \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n\\right) \\pi_{j},\n\nwhich can usually be solved analytically depending on the mathematical form of the parametric distribution of the component \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x} \\mid z).\nAgain, \\hat{\\Psi}_{\\text{new}} is the estimated parameters that include\n\nparameters \\{ \\hat{\\boldsymbol{\\theta}}_{1}, \\dots, \\hat{\\boldsymbol{\\theta}}_{c} \\} in the c components of the mixture model.\nprobability parameters \\{ \\hat{\\pi}_{1}, \\dots, \\hat{\\pi}_{c} \\} of c components."
  },
  {
    "objectID": "Probability and Statistics/11_Expectation_Maximization.html#reference",
    "href": "Probability and Statistics/11_Expectation_Maximization.html#reference",
    "title": "25  Expectation-maximization",
    "section": "Reference",
    "text": "Reference\n\nhttp://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf\nhttps://gregorygundersen.com/blog/2019/11/10/em/\nhttps://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1268&context=gc_cs_tr\nhttps://mbernste.github.io/posts/em/"
  },
  {
    "objectID": "Probability and Statistics/12_Concentration_Inequalities2.html#sub-gaussian-random-variables",
    "href": "Probability and Statistics/12_Concentration_Inequalities2.html#sub-gaussian-random-variables",
    "title": "26  Concentration Inequalities II",
    "section": "Sub-Gaussian random variables",
    "text": "Sub-Gaussian random variables\nA random variable X with mean \\mu is called Sub-Gaussian with parameter \\sigma (X \\sim SubGau (\\sigma)) if\n\n\\mathbb{E}_{X} \\left[\n    e^{s (X - \\mu)}\n\\right] \\leq \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 }\n\\right].\n\nThe definition can equivalently be expressed in terms of bounds on the tail of X. Let X \\sim SubGau (\\sigma) and \\mu = \\mathbb{E}_{X} [X]. Then for any t &gt; 0,\n\n\\mathbb{P}_{X} \\left(\n    X - \\mu \\geq t\n\\right) \\leq \\exp \\left[\n    \\frac{ - t^{2} }{ 2 \\sigma^{2} }\n\\right].\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst use Chernoff bound to derive\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x \\geq t)\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{X} [e^{s x}]\n}{\n    e^{s t}\n}\n\\\\\n& \\leq \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t\n\\right].\n\\end{aligned}\n\nSince \\exp \\left[ \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t \\right] is a convex function\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t\n\\right]\n& = 0\n\\\\\n(\\sigma^{2} s - t) \\exp \\left[\n    \\frac{ \\sigma^{2} s^{2} }{ 2 } - t s\n\\right]\n& = 0\n\\\\\ns\n& = \\frac{ t }{ \\sigma^{2} }.\n\\end{aligned}\n\nPlug s = \\frac{ t }{ \\sigma^{2} } back and we get derive the result\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x \\geq t)\n& \\leq \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ t^{2} }{ 2 \\sigma^{2} } - \\frac{ t^{2} }{ \\sigma^{2} }\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ - t^{2} }{ 2 \\sigma^{2} }\n\\right].\n\\end{aligned}"
  },
  {
    "objectID": "Probability and Statistics/12_Concentration_Inequalities2.html#hoeffdings-inequality",
    "href": "Probability and Statistics/12_Concentration_Inequalities2.html#hoeffdings-inequality",
    "title": "26  Concentration Inequalities II",
    "section": "Hoeffding’s inequality",
    "text": "Hoeffding’s inequality\nHoeffding’s inequality provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value by more than a certain amount.\n\nHoeffding’s Lemma\nLet X be a bounded random variable with a \\leq X \\leq b and \\mu = \\mathbb{E}_{X} [x]. Then for all s &gt; 0,\n\n\\mathbb{E}_{X} [e^{s (x - \\mu)}] \\leq \\exp \\left[\n    \\frac{ s^{2} (b - a)^{2} }{ 8 }\n\\right].\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince f (x) = e^{a x} is a convex function, the definition of the convex function states that\n\ne^{s x} \\leq \\frac{ (x - a) e^{s b} }{ b - a } + \\frac{ (b - x) e^{s a} }{ b - a }.\n\nTaking the expectation on the both ends and the fact that \\mathbb{E}_{X} (x) = 0,\n\n\\begin{aligned}\n\\mathbb{E}_{X} [e^{s x}]\n& \\leq \\mathbb{E}_{X} \\left[\n    \\frac{ (x - a) e^{s b} }{ b - a } + \\frac{ (b - x) e^{s a} }{ b - a }\n\\right]\n\\\\\n& = \\frac{\n    e^{s b} \\mathbb{E}_{X} [x] - a e^{s b} + b e^{s a} - e^{s a} \\mathbb{E}_{X} [x]\n}{\n    b - a\n}\n\\\\\n& = \\frac{ b e^{s a} - a e^{s b} }{ b - a }.\n\\end{aligned}\n\nNow we will prove that\n\n\\frac{ b e^{s a} - a e^{s b} }{ b - a } \\leq \\exp \\left[\n    \\frac{ s^{2} (b - a)^{2} }{ 8 }\n\\right]\n\nwhich is same as proving\n\n\\log \\left[\n    \\frac{ b e^{s a} - a e^{s b} }{ b - a }\n\\right] \\leq \\frac{ s^{2} (b - a)^{2} }{ 8 }.\n\nBy taking p = \\frac{ b }{ b - a } and 1 - p = -\\frac{ a }{ b - a }\n\n\\begin{aligned}\n\\log \\left[\n    \\frac{ b e^{s a} - a e^{s b} }{ b - a }\n\\right]\n& = \\log \\left[\n    \\frac{ b e^{s a} }{ b - a } - \\frac{ a e^{s b} }{ b - a }\n\\right]\n\\\\\n& = \\log \\left[\n    p e^{s a} + (1 - p) e^{s b}\n\\right]\n\\\\\n& = \\log \\left[\n    e^{s a} e^{-s a} (p e^{s a} + (1 - p) e^{s b})\n\\right]\n\\\\\n& = s a + \\log \\left[\n    (p + (1 - p) e^{s b - s a})\n\\right]\n\\end{aligned}\n\nand taking u = (b - a) s, we can get a function\n\n\\begin{aligned}\n\\phi (u)\n& = s a + \\log \\left[\n    (p + (1 - p) e^{s b - s a})\n\\right]\n\\\\\n& = (p - 1) u + \\log \\left[\n    (p + (1 - p) e^{u})\n\\right].\n\\end{aligned}\n\nThis function can be approximated using Taylor series until the second order term at point a = 0,\n\n\\phi (u) = \\phi (0) + \\phi' (0) x + \\frac{ \\phi' (0) }{ 2 } x^{2},\n\nwhere\n\n\\phi' (u) = (p - 1) + \\frac{ (1 - p) e^{u} }{ p + (1 - p) e^{u} } = 0,\n\nand\n\n\\phi'' (u) = \\frac{ p (1 - p) e^{u} }{ (p + (1 - p) e^{u})^{2} } = 0.\n\nSince \\phi (0) = 0, \\phi' (0) = 0 and\n\n\\phi'' (0) = \\frac{ p (1 - p) }{ (p + (1 - p))^{2} }\n\nreaches its maximum of 0.25 at p = 0.5, we can derive\n\n\\log \\left[\n    \\frac{ b e^{s a} - a e^{s b} }{ b - a }\n\\right] = \\phi (u) \\leq \\frac{ u^{2} }{ 8 } = \\frac{ s^{2} (b - a)^{2} }{ 8 }.\n\nTherefore, we can prove the lemma\n\n\\mathbb{E}_{X} [e^{s x}] \\leq \\frac{ b e^{s a} - a e^{s b} }{ b - a } \\leq \\exp \\left[\n    \\frac{ s^{2} (b - a)^{2} }{ 8 }\n\\right].\n\n\n\n\nEquivalently, Hoeffding’s lemma states that any random variable X \\in [a, b] is a subGaussian variable\n\nX \\in SubGau \\left(\n    \\frac{ (b - a)^{2} }{ 4 }\n\\right).\n\n\n\nHoeffding’s inequality\nLet X_{1}, \\dots, X_{n} be bounded independent random variables such that X_{i} \\in [a_{i}, b_{i}], \\forall i. Let X = \\sum_{i=1}^{n} X_{i} and \\mu = \\mathbb{E}_{X} (x). Then for t &gt; 0\n\n\\mathbb{P}_{X} \\left(\n    x - \\mu \\geq t\n\\right) \\leq \\exp \\left[\n    \\frac{\n        - 2 t^{2}\n    }{\n        \\sum_{i=1}^{n} (b_{i} - a_{i})^{2}\n    }\n\\right].\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe can first apply Chernoff’s bounding method\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x - \\mu \\geq t)\n& \\leq \\inf_{s &gt; 0} \\frac{ M_{X} (s) }{ e^{s t} }\n\\\\\n& = \\inf_{s &gt; 0} \\frac{\n    \\prod_{i = 1}^{n} M_{X_{i}} (s)\n}{\n    e^{s t}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\frac{\n    \\prod_{i = 1}^{n} \\mathbb{E}_{X} \\left[\n        e^{s (X_{i} - \\mathbb{E}_{X_{i}} [x_{i}])}\n    \\right]\n}{\n    e^{s t}\n}.\n\\end{aligned}\n\nThen by applying the Hoeffding’s Lemma,\n\n\\mathbb{E}_{X_{i}} \\left[\n    e^{s (X_{i} - \\mathbb{E}_{X_{i}} [x_{i}])}\n\\right] \\leq \\exp \\left[\n    \\frac{ s^{2} (b_{i} - a_{i})^{2} }{ 8 }\n\\right]\n\nwe can have\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x - \\mu \\geq t)\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\prod_{i = 1}^{n} \\mathbb{E}_{X_{i}} \\left[\n        e^{s (X_{i} - \\mathbb{E}_{X_{i}} [x_{i}])}\n    \\right]\n}{\n    e^{s t}\n}\n\\\\\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\prod_{i = 1}^{n} \\exp\\left[\n        \\frac{ s^{2} (b_{i} - a_{i})^{2} }{ 8 }\n    \\right]\n}{e^{s t}}\n\\\\\n& = \\inf_{s &gt; 0} \\exp\\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 8 } - s t\n\\right].\n\\end{aligned}\n\nSince \\exp \\left[ \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t \\right] is a convex function,\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t\n\\right]\n& = 0\n\\\\\n\\left(\n    \\frac{ s \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 4 } - t\n\\right)\n\\exp\\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 8 }  - s t\n\\right]\n& = 0\n\\\\\n\\frac{ s \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 4 } - t\n& = 0\n\\\\\ns\n& = \\frac{ 4 t }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\end{aligned}\n\nTherefore, we have proved Hoeffding’s inequality\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x - \\mu \\geq t)\n& \\leq \\inf_{s &gt; 0} \\exp\\left[\n    \\frac{ s^{2} }{ 8 } \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} - s t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{\n        \\left(\n            \\frac{ 4 t }{ \\sum_{i}^{n} (b_{i} - a_{i})^{2} }\n        \\right)^{2}\n    }{\n        8\n    } \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} - \\frac{ 4 t }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} } t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\right].\n\\end{aligned}\n\n\n\n\nAnother version is to bound the estimated mean.\nLet X_{1}, \\dots, X_{n} be bounded independent random variables such that X_{i} \\in [a_{i}, b_{i}], \\forall i. Let \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} and \\mu = \\mathbb{E}_{\\bar{X}} (x). Then for t &gt; 0\n\n\\mathbb{P}_{\\bar{X}} \\left(\n    x - \\mu \\geq t\n\\right) \\leq \\exp \\left[\n    -\\frac{ 2 n^{2} t^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]"
  },
  {
    "objectID": "Probability and Statistics/12_Concentration_Inequalities2.html#martingales",
    "href": "Probability and Statistics/12_Concentration_Inequalities2.html#martingales",
    "title": "26  Concentration Inequalities II",
    "section": "Martingales",
    "text": "Martingales\n\nMartingale sequence\nGiven a sequence of random variables X_{1}, \\dots, X_{\\infty}, define another sequence of random variables Y_{1}, \\dots, Y_{\\infty} where Y_{n} is a measurable function of X_{1}, \\dots X_{n}\n\nY_{n} = f (X_{1}, \\dots, X_{n}).\n\nFor all Y_{n}, if \\mathbb{E}_{Y_{n}} [y_{n}] is finite and\n\n\\mathbb{E}_{Y_{n + 1} \\mid X_{1}, \\dots, X_{n}} [Y_{n + 1} \\mid X_{1}, \\dots, X_{n}] = Y_{n},\n\nthen Y_{1}, \\dots, Y_{\\infty} is a martingale sequence with respect to X_{1}, \\dots, X_{\\infty}.\n\n\nMartingale difference sequence\nGiven a sequence of random variables X_{1}, \\dots, X_{\\infty}, define another sequence of random variables Z_{1}, \\dots, Z_{\\infty} where Z_{n} is a measurable function of X_{1}, \\dots X_{n}\n\nZ_{n} = f (X_{1}, \\dots, X_{n}).\n\nFor all Z_{n}, if E_{Z_{n}} [z_{n}] is finite and\n\n\\mathbb{E}_{Z_{n + 1} \\mid X_{1}, \\dots, X_{n}} [Z_{n + 1} \\mid X_{1}, \\dots, X_{n}] = 0\n\nthen Z_{1}, \\dots, Z_{\\infty} is a martingale difference sequence with respect to X_{1}, \\dots, X_{\\infty}.\n\n\nA common Martingale sequence\nIf X_{1}, \\dots, X_{\\infty} is a sequence of independent random variables where \\mathbb{E}_{X_{i}} [x_{i}] = 0, \\forall X_{i}, then Y_{1}, \\dots, X_{\\infty} with Y_{n} = \\sum_{i = 1}^{n} X_{i} is martingale sequence.\n\n\\begin{aligned}\n\\mathbb{E}_{Y_{n + 1} \\mid X_{1}, \\dots, X_{n}}\n& = \\mathbb{E}_{Y_{n} + X_{n + 1} \\mid X_{1}, \\dots, X_{n}}\n\\\\\n& = \\mathbb{E}_{Y_{n} \\mid X_{1}, \\dots, X_{n}} + \\mathbb{E}_{X_{n + 1} \\mid X_{1}, \\dots, X_{n}}\n\\\\\n& = Y_{n} + \\mathbb{E}_{X_{n + 1}}\n\\\\\n& = Y_{n}\n\\end{aligned}\n\nSuppose Y_{1}, \\dots, Y_{n} is a martingale sequence with respect to X_{1}, \\dots, X_{n}. then the sequence Z_{1}, \\dots, Z_{\\infty} with Z_{n} = Y_{n} - Y_{n - 1} is a martingale difference sequence.\n\n\\begin{aligned}\n\\mathbb{E}_{Z_{n + 1} \\mid X_{1}, \\dots, X_{n}}\n& = \\mathbb{E}_{Y_{n + 1} - Y_{n} \\mid X_{1}, \\dots, X_{n}}\n\\\\\n& = \\mathbb{E}_{Y_{n + 1} \\mid X_{1}, \\dots, X_{n}} - \\mathbb{E}_{Y_{n} \\mid X_{1}, \\dots, X_{n}}\n\\\\\n& = Y_{n} - Y_{n}\n\\\\\n& = 0\n\\end{aligned}"
  },
  {
    "objectID": "Probability and Statistics/12_Concentration_Inequalities2.html#azuma-inequality",
    "href": "Probability and Statistics/12_Concentration_Inequalities2.html#azuma-inequality",
    "title": "26  Concentration Inequalities II",
    "section": "Azuma Inequality",
    "text": "Azuma Inequality\nFor a sequence of Martingale difference sequence of bounded random variable Z_{1}, \\dots, Z_{\\infty} with Z_{i} \\in [a_{i}, b_{i}], \\forall i, then:\n\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right] \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\right].\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst we can apply Chernoff bound to derive\n\n\\begin{aligned}\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right]\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n        e^{s \\sum_{i = 1}^{n} z_{i}}\n    \\right]\n}{\n    e^{s t}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n        \\prod_{i = 1}^{n} e^{s z_{i}}\n    \\right]\n}{\n    e^{s t}\n}\n\\end{aligned}\n\nThen we can use the law of total expectation to derive\n\n\\begin{aligned}\n\\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\prod_{i = 1}^{n} e^{s z_{i}}\n\\right]\n& = \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} \\left[\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n} \\mid Z_{1}, \\dots Z_{n - 1}} \\left[\n        \\prod_{i = 1}^{n} e^{s z_{i}} \\mid z_{1}, \\dots, z_{n - 1}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} \\left[\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n} \\mid Z_{1}, \\dots Z_{n - 1}} \\left[\n        e^{s z_{n}} \\prod_{i = 1}^{n - 1} e^{s z_{i}} \\mid z_{1}, \\dots, z_{n - 1}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} \\left[\n    \\prod_{i = 1}^{n - 1} e^{s z_{i}} \\mathbb{E}_{Z_{n} \\mid Z_{1}, \\dots, Z_{n - 1}} \\left[\n        e^{s z_{n}} \\mid z_{1}, \\dots, z_{n - 1}\n    \\right]\n\\right],\n\\end{aligned}\n\nwhere the last equality is derived because \\prod_{i = 1}^{n - 1} e^{s z_{i}} is a constant given z_{1}, \\dots, z_{n - 1}.\nWe can derive a upper bound by applying the Hoeffding’s lemma\n\n\\begin{aligned}\n\\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\prod_{i = 1}^{n} e^{s z_{i}}\n\\right]\n& \\leq \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}}\\ \\left[\n    \\prod_{i = 1}^{n - 1} e^{s z_{i}} \\exp \\left[\n        \\frac{ s^{2} (a_{n} - b_{n})^{2} }{ 8 }\n    \\right]\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ s^{2} (a_{n} - b_{n})^{2} }{ 8 }\n\\right] \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}}\\ \\left[\n    \\prod_{i = 1}^{n - 1} e^{s z_{i}}\n\\right]\n\\end{aligned}\n\nWe can apply the same procedure for \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} [ \\prod_{i = 1}^{n - 1} e^{s z_{i}} ]\n\n\\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} \\left[\n    \\prod_{i = 1}^{n - 1} e^{s z_{i}}\n\\right] = \\exp \\left[\n    \\frac{ s^{2} (a_{n - 1} - b_{n - 1})^{2} }{ 8 }\n\\right] \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 2}}\\ \\left[\n    \\prod_{i = 1}^{n - 2} e^{s z_{i}}\n\\right]\n\nand do this iteratively to derive\n\n\\begin{aligned}\n\\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\prod_{i = 1}^{n} e^{s z_{i}}\n\\right]\n& = \\exp \\left[\n    \\frac{ s^{2} (a_{n} - b_{n})^{2} }{ 8 }\n\\right] \\times \\dots \\times \\exp \\left[\n    \\frac{ s^{2} (a_{1} - b_{1})^{2} }{ 8 }\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nTherefore,\n\n\\begin{aligned}\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right]\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n        e^{s \\sum_{i = 1}^{n} z_{i}}\n    \\right]\n}{\n    e^{s t}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t\n\\right]\n\\end{aligned}\n\nSince \\exp \\left[ \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t \\right] is a convex function,\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t\n\\right]\n& = 0\n\\\\\n\\left(\n    \\frac{ s \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 4 } - t\n\\right)\n\\exp\\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 8 }  - s t\n\\right]\n& = 0\n\\\\\n\\frac{ s \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 4 } - t\n& = 0\n\\\\\ns\n& = \\frac{ 4 t }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\end{aligned}\n\nand therefore\n\n\\begin{aligned}\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right]\n& \\leq \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{\n        \\left(\n            \\frac{ 4 t }{ \\sum_{i}^{n} (b_{i} - a_{i})^{2} }\n        \\right)^{2}\n    }{\n        8\n    } \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} - \\frac{\n        4 t\n    }{\n        \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2}\n    } t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\right].\n\\end{aligned}"
  },
  {
    "objectID": "Probability and Statistics/12_Concentration_Inequalities2.html#mcdiarmids-inequality",
    "href": "Probability and Statistics/12_Concentration_Inequalities2.html#mcdiarmids-inequality",
    "title": "26  Concentration Inequalities II",
    "section": "McDiarmid’s Inequality",
    "text": "McDiarmid’s Inequality\nGiven a function f: \\mathbb{R}^{n} \\to \\mathbb{R} with the bounded difference property, that is, the maximum change of the function output induced by replacing any x_{i} with x'_{i} is bounded by c_{i},\n\n\\lvert f (x_{1}, \\dots, x_{i}, \\dots, x_{n}) - f (x_{1}, \\dots, x'_{i}, \\dots, x_{n}) \\rvert \\leq c_{i},\n\nthen the function f of independent random variables X_{1}, \\dots, X_{n} satisfies\n\n\\mathbb{P}_{X_{1}, \\dots, X_{n}} \\left[\n    f (X_{1}, \\dots, X_{n}) - \\mathbb{E}_{X_{1}, \\dots, X_{n}} \\left[\n        f (X_{1}, \\dots, X_{n})\n    \\right] \\geq t\n\\right] \\leq \\exp \\left[\n    \\frac{\n        - 2 t^{2}\n    }{\n        \\sum_{i=1}^{n} c_{i}^{2}\n    }\n\\right].\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet X_{1}, \\dots, X_{n} be a sequence of independent random variable, and Y_{1}, \\dots, Y_{n} be a Martingale sequence with respect to X_{1}, \\dots, X_{n} where\n\nY_{i} = g (X_{1}, \\dots, X_{i}) = \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i}\n\\right].\n\nNote that we have\n\n\\begin{aligned}\nY_{0}\n& = \\mathbb{E}_{X_{1}, \\dots, X_{n}} \\left[\n    f (X_{1}, \\dots, X_{n})\n\\right],\n\\\\\nY_{n}\n& = \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{n}} \\left[\n    f (X_{1}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{n}\n\\right] = f (X_{1}, \\dots, X_{n}).\n\\end{aligned}\n\nLet Z_{1}, \\dots, Z_{n} be an independent random variable with Z_{i} = Y_{i} - Y_{i - 1}, which is proved to be a Martingale difference sequence.\nNote that\n\n\\sum_{i = 1}^{n} Z_{i} = Y_{n} - Y_{0} = f (X_{1}, \\dots, X_{n}) - \\mathbb{E}_{X_{1}, \\dots, X_{n}} \\left[\n    X_{1}, \\dots, X_{n}\n\\right]\n\nFor each Z_{i}, its upper bound U_{i} and lower bound L_{i} can be written as\n\n\\begin{aligned}\nU_{i}\n& = \\sup_{x_{i} \\in X_{i}} \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, x_{i}, \\dots X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right] - \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i - 1}} \\left[\n    f (X_{1}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\\\\nL_{i}\n& = \\inf_{x_{i} \\in X_{i}} \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, x_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right] - \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i - 1}} \\left[\n    f (X_{1}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\end{aligned}\n\nThe difference between U_{i} and L_{i} is less than c_{i} because of the bounded property property of f,\n\n\\begin{aligned}\nU_{i} - L_{i}\n& = \\sup_{x_{i} \\in X_{i}} \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, x_{i}, \\dots X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right] - \\inf_{x_{i} \\in X_{i}} \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, x_{i}, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\\\\n& = \\sup_{x_{i}, x'_{i} \\in X_{i}} \\left[\n    \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n        f (X_{1}, \\dots, x_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n    \\right] - \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n        f (X_{1}, \\dots, x'_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n    \\right]\n\\right]\n\\\\\n& = \\sup_{x_{i}, x'_{i} \\in X_{i}} \\left[\n    \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n        f (X_{1}, \\dots, x_{i}, \\dots, X_{n}) -\n        f (X_{1}, \\dots, x'_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    \\sup_{x_{i}, x'_{i} \\in X_{i}} f (X_{1}, \\dots, x_{i}, \\dots, X_{n}) -\n    f (X_{1}, \\dots, x'_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\\\\n& \\leq \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    c_{i}\n    \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\\\\n& \\leq c_{i}.\n\\end{aligned}\n\nTherefore, we have a Martingale difference sequence of bounded random variable Z_{1}, \\dots, Z_{n}, where Z_{i} \\in [a_{i}, b_{i}], \\forall i and b_{i} - a_{i} \\leq c_{i}. We can apply Azuma inequality to obtain McDiarmid’s inequality.\n\n\\begin{aligned}\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right]\n& = \\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\mathbb{E}_{X_{1}, \\dots, X_{n}} \\left[\n        X_{1}, \\dots, X_{n}\n    \\right]\n\\right]\n\\\\\n& \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n& \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} c_{i}^{2} }\n\\right].\n\\end{aligned}"
  },
  {
    "objectID": "Learning Theory/1_Statistical_Learning.html#functions",
    "href": "Learning Theory/1_Statistical_Learning.html#functions",
    "title": "27  Statistical Learning",
    "section": "Functions",
    "text": "Functions\n\nDecision function\nA decision function is a function f: \\mathcal{X} \\to \\mathcal{Y} whose domain is \\mathcal{X} and the range is \\mathcal{Y}\n\n\\hat{y} = f (x)\n\nthat maps each input instance x \\in \\mathcal{X} to a label y \\in \\mathcal{Y}.\nHere we have two types of decision functions that have slightly different meanings in the context of machine learning\n\nConcept c and concept class C: a concept from a concept class c \\in \\mathcal{C} is the decision function that the algorithm wants to learn, which assigns all correct labels for given instances.\nHypothesis h and hypothesis class H: a hypothesis from a hypothesis class h \\in \\mathcal{H} is the decision function that the algorithm actually learns from the hypothesis class.\n\n\n\nLoss function\nThe way we evaluate a function f on a labeled instance (x, y) is determined by a loss function L: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}^{+}\n\nL (z) = L (f (x), y)\n\nwhich calculates some notion of discrepancy between the true label y and the predicated label \\hat{y} = f (x).\nAll the loss functions used in this note are 0-1 loss\n\nL (z) = L (f (x), y) = \\mathbb{1} \\left[\n    f (x) \\neq y\n\\right],\n\nwhich incurs a loss of 1 if the predicated label is the same as the true label and 0 if they are the same."
  },
  {
    "objectID": "Learning Theory/1_Statistical_Learning.html#learning-in-a-probability-setting",
    "href": "Learning Theory/1_Statistical_Learning.html#learning-in-a-probability-setting",
    "title": "27  Statistical Learning",
    "section": "Learning in a probability setting",
    "text": "Learning in a probability setting\nIn a statistical learning problem, each labeled instance is an independent and identically distributed (i.i.d.) draw from some fixed but unknown joint distribution \\mathbb{P}_{X, Y} over \\mathcal{X} \\times \\mathcal{Y} that describes the probability that both x and y happens in the real world.\nThis means that there is always a probability associated with each term:\n\nthe distribution \\mathbb{P}_{X} for a multivariate random variable X that describes the probability of an instance x\nthe distribution \\mathbb{P}_{Y} for a random variable Y that describes the probability of a label y.\n\nWe can decompose the joint probability according to the chain rule:\n\n\\mathbb{P}_{X, Y}(x, y) = \\mathbb{P}_{X \\mid Y}(x \\mid y) \\mathbb{P}_{Y}(y),\n\nwhere \\mathbb{P}_{X \\mid Y}(x \\mid y) is called class conditional probability, which gives the probability of the instance if we know the label is y.\nFor simplicity, sometimes we will write \\mathbb{P}_{Z} \\coloneqq \\mathbb{P}_{X, Y} to denote the probability of the labeled instance.\n\nTrue risk\nThe true risk of the hypothesis h is defined as the expectation of the loss function over the joint probability\n\nR (h) =  \\mathbb{E}_{X, Y} [L (h (X), Y)] = \\mathbb{E}_{Z} [L (Z)]\n\nwhich is the probability that h makes a mistake if the loss function is 0-1 loss\n\nR (h) = \\mathbb{P}_{X, Y} \\left[\n    \\mathbb{1} \\left[\n        h (x) \\neq y\n    \\right]\n\\right].\n\n\nLemma 27.1 Apart from the expectation with respect to the join probability of \\mathbb{P}_{X, Y}, the true risk of any hypothesis h can also be written as the expectation of the conditional expectation of the loss function\n\nR(h) = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof is based on the definition of the expectation and the chain rules of the probability\n\n\\begin{aligned}\nR(h)\n& = \\mathbb{E}_{X, Y} [L (h (X), Y)]\n\\\\\n& = \\int \\int \\mathbb{P}_{X, Y} (x, y) L (h (x), y) \\mathop{d x} \\mathop{dy}\n& [\\text{definition of } \\mathbb{E} [\\cdot]]\n\\\\\n& = \\int \\int \\mathbb{P}_{Y \\mid X} (y \\mid x) \\mathbb{P}_{X} (x) L (g(x), y) \\mathop{d x} \\mathop{dy}\n& [\\text{probability chain rule}]\n\\\\\n& = \\int \\mathbb{P}_{X} (x) \\int \\mathbb{P}_{Y \\mid X} (y \\mid x) L (g(x), y) \\mathop{dy} \\mathop{d x}\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\\end{aligned}\n\n\n\n\n\n\nEmpirical risk\nThe empirical risk function is used with the past data of n labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} as a surrogate function for the risk function\n\nR_{\\mathcal{S}} (h) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (h (x_{i}), y) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (z_{i}),\n\nwhich is the average number of mistakes h made in \\mathcal{D}^{n} if the loss is 0-1 loss\n\nR_{\\mathcal{S}} (h) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{1} \\left[\n    h (x_{i}) \\neq y_{i}\n\\right].\n\nThe idea is that if the past data we have is representative of the actual distribution, then it will be the case that the empirical risk will be close to the true risk.\n\n\nEmpirical risk as a unbiased estimator\nThe empirical risk is an unbiased estimator of the true risk. That is, the expectation of the empirical risk over all samples is the true risk\n\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    R_{\\mathcal{S}} (h)\n\\right] = R (h).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    R_{\\mathcal{S}} (h)\n\\right]\n& = \\mathbb{E}_{Z} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (z_{i})\n\\right]\n\\\\\n& = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{E}_{Z} [L (z_{i})]\n\\\\\n& = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} R (h)\n\\\\\n& = R (h).\n\\end{aligned}\n\n\n\n\nAlso, by the law of large numbers, we have R_{n} (h) \\to R(h) as n \\to \\infty, almost surely, which means the empirical risk is closed to the true risk if the sample size is large enough."
  },
  {
    "objectID": "Learning Theory/1_Statistical_Learning.html#some-probability-facts",
    "href": "Learning Theory/1_Statistical_Learning.html#some-probability-facts",
    "title": "27  Statistical Learning",
    "section": "Some probability facts",
    "text": "Some probability facts\n\nBasics\n\nComplement rule\n\n\n\\mathbb{P} (A &gt; t) &lt; \\delta \\implies \\mathbb{P} (A \\leq t) \\geq 1 - \\delta"
  },
  {
    "objectID": "Learning Theory/2_Bayesian_Classifier.html#map-rule",
    "href": "Learning Theory/2_Bayesian_Classifier.html#map-rule",
    "title": "28  Bayesian Classifier",
    "section": "MAP rule",
    "text": "MAP rule\nSince the true risk R (h) does not depend on X and Lemma 27.1 shows that R(h) = \\mathbb{E}_{X} \\left[ \\mathbb{E}_{Y \\mid X} \\left[ L (h (X), Y) \\right] \\right], the Bayes classifier can also be written as the hypothesis that minimizes the conditional expectation\n\nh^{*} = \\argmin_{h} \\mathbb{E}_{Y \\mid X} \\left[\n    L (h (X), Y)\n\\right],\n\nwhich can be further simplied to maximum a-posteriori probability (MAP) rule if the loss function is 0-1 loss and there are m labels y \\in [1, m]\n\nh^{*} (x) = \\argmax_{y \\in [1, m]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of Bayes classifier,\n\n\\begin{aligned}\nh^{*}\n& = \\argmin_{h} \\mathbb{E}_{Y \\mid X} \\left[\n    L (h (X), Y)\n\\right]\n\\\\\n& = \\argmin_{h} \\sum_{y=1}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x) L (h, y)\n& [\\text{def of } \\mathbb{E}_{Y \\mid X}]\n\\\\\n& = \\argmin_{h} \\sum_{y = h (x)}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x) \\times 0\n+\n\\sum_{y \\neq h (x)}^{m} \\mathbb{P}_{Y \\mid X}(y \\mid x) \\times 1\n& [\\text{def of 0-1 loss}]\n\\\\\n& = \\argmin_{h} \\sum_{y \\neq h (x)}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\argmin_{h} \\left[\n    1 - \\mathbb{P}_{Y \\mid X} (h (x) \\mid x)\n\\right]\n& [\\sum_{x \\neq \\alpha} \\mathbb{P}_{X} (x) = 1 - \\mathbb{P}_{X} (\\alpha) ]\n\\\\\n& = \\argmax_{h} \\mathbb{P}_{Y \\mid X} (h (x) \\mid x)\n& [\\argmin_{x} (1 - f(x)) = \\arg\\max_{x} (f(x))].\n\\end{aligned}\n\nwhere the last line can be simplied to\n\nh^{*} (x) = \\argmax_{y \\in [1, m]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\nsince h (x) \\in [1, m].\n\n\n\nAccording to Bayes Theorem,\n\n\\begin{aligned}\n\\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\frac{\\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)}{\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x})}\n\\\\\n& = \\arg\\max_{y} \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y) & [\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}) \\text{ doesn't depend on } y],\n\\\\\n\\end{aligned}\n\nMAP rule can thus be computed using the class conditional probability (likelihood) and the class probability (prior), which is more practical since the class conditional probability and class probability can be more easily obtained from the data than the posterior probability.\nUsing the log trick, the BDR for 0-1 loss is often calculated using:\n\n\\begin{aligned}\n\\arg\\max_{y} \\ln \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)\n\\\\\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) + \\ln \\mathbb{P}_{Y}(y).\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Learning Theory/2_Bayesian_Classifier.html#map-rule-for-binary-classification",
    "href": "Learning Theory/2_Bayesian_Classifier.html#map-rule-for-binary-classification",
    "title": "28  Bayesian Classifier",
    "section": "MAP rule for binary classification",
    "text": "MAP rule for binary classification\nSince there are only 2 labels in the binary classification problem, the MAP rule for binary classification is simplied to\n\n\\begin{aligned}\nh^{*} (x)\n& = \\argmax_{y \\in [0, 1]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\mathbb{1} \\left[\n    \\mathbb{P}_{Y \\mid X} (1 \\mid x) &gt; \\mathbb{P}_{Y \\mid X} (0 \\mid x)\n\\right]\n\\\\\n& = \\begin{cases}\n    1, \\quad \\mathbb{P}_{Y \\mid X} (1 \\mid x) &gt; \\frac{ 1 }{ 2 }  \\\\\n    0, \\quad \\mathbb{P}_{Y \\mid X} (1 \\mid x) &lt; \\frac{ 1 }{ 2 }.\n\\end{cases}\n\\end{aligned}\n\n\nRegression function\nThe conditional distribution \\mathbb{P}_{Y \\mid X} can be modeled with a Bernoulli distribution \\mathbb{P}_{Y \\mid X} (y \\mid x) = \\mathrm{Ber} (\\eta (x)), where \\eta (x) is the regression function\n\n\\eta (x) = \\mathbb{P}_{Y \\mid X} (1 \\mid x) = \\mathbb{E}_{Y \\mid X} (Y).\n\n\nLemma 28.1 For any hypothesis h, we can write its risk function with 0-1 loss for binary classification as\n\nR (h) = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h (X))\n    +\n    (1 - \\eta (X)) h (X)\n\\right].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of the risk function\n\nR (h) = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\nSince the 0-1 loss for binary classification problem can be written as\n\nL (h (x), y) = \\mathbb{1} \\left[\n    h (x) \\neq y\n\\right] = y (1 - h (x)) + (1 - y) h (x)\n\nwe have\n\n\\begin{aligned}\nR (h)\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        y (1 - h (x)) + (1 - y) h (x)\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} [y](1 - h (x))\n    +\n    \\mathbb{E}_{Y \\mid X} [1 - y] h (x)\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h (X))\n    +\n    (1 - \\eta (X)) h (X)\n\\right].\n\\end{aligned}\n\n\n\n\n\nTheorem 28.1 The risk of the Bayes classifier for binary classification with 0-1 loss is the expectation of the minimum of \\eta (X) and 1 - \\eta (X)\n\nR (h^{*}) = \\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right]\n\nand is less than \\frac{ 1 }{ 2 }\n\n\\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right] \\leq \\frac{ 1 }{ 2 }.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Lemma 28.1 and replacing h with the Bayes classifier h^{*}, we have\n\n\\begin{aligned}\nR (h^{*})\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h^{*} (X))\n    +\n    (1 - \\eta (X)) h^{*} (X)\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) \\mathbb{1} \\left[\n        \\eta (X) &lt; \\frac{ 1 }{ 2 }\n    \\right]\n    +\n    (1 - \\eta (X)) \\mathbb{1} \\left[\n        \\eta (X) &gt; \\frac{ 1 }{ 2 }\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right]\n\\end{aligned}\n\nwhere the last inequality follows because\n\nR (h^{*}) = \\mathbb{E}_{X} [\\eta (X)] = \\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]], \\quad \\text{ if } \\eta (X) &lt; \\frac{ 1 }{ 2 } \\\\\nR (h^{*}) = \\mathbb{E}_{X} [1 - \\eta (X)] = \\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]], \\quad \\text{ if } \\eta (X) &gt; \\frac{ 1 }{ 2 }.\n\nSince \\min [\\eta (X), 1 - \\eta (X)] &lt; \\frac{ 1 }{ 2 }, its expectation is also less than \\frac{ 1 }{ 2 }\n\n\\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]] &lt; \\frac{ 1 }{ 2 }.\n\n\n\n\n\n\nExcess risk\nFor any hypothesis h, we are interested in the difference between its risk R (h) and Bayes risk R (h^{*}), which is called excess risk of h\n\n\\mathcal{E} (h) = R (h) - R (h^{*}).\n\n\nTheorem 28.2 For any hypothesis h, the excess risk satisfies\n\n\\mathcal{E} (h) = \\mathbb{E}_{X} \\left[\n    \\lvert 2 \\eta (X) - 1 \\rvert \\times \\mathbb{1} \\left[\n        h (X) \\neq h^{*} (X)\n    \\right]\n\\right]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Lemma 28.1 for R (h) and R (h^{*}) and linearity of expectation, we have\n\n\\begin{aligned}\nR (h) - R(h^{*})\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (h^{*} (X) - h (X))\n    +\n    (1 - \\eta (X)) (h (X) - h^{*} (X))\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    (2 \\eta (X) - 1) (h^{*} (X) - h (X))\n\\right].\n\\end{aligned}\n\nNote that\n\nh^{*} (X) - h (X) =  \\mathrm{sgn} [2 \\eta (X) - 1] \\times \\mathbb{1} [h^{*} \\neq h (X)],\n\nbecause it combines all 3 cases for the results of h^{*} (X) - h (X).\n\nIf h^{*} (X) = h (X),\n\nh^{*} (X) - h (X) = 0.\n\nSince \\eta (X) &gt; \\frac{ 1 }{ 2 } \\implies h^{*} (X) = 1, if h^{*} (X) = 1, h (X) = 0,\n\nh^{*} (X) - h (X) = 1 = \\mathrm{sgn} [2 \\eta (X) - 1].\n\nSince \\eta (X) &lt; \\frac{ 1 }{ 2 } \\implies h^{*} (X) = 0, if h^{*} (X) = 0, h (X) = 1,\n\nh^{*} (X) - h (X) = -1 = \\mathrm{sgn} [2 \\eta (X) - 1].\n\n\nTherefore,\n\n\\begin{aligned}\nR (h) - R(h^{*})\n& = \\mathbb{E}_{X} \\left[\n    (2 \\eta (X) - 1) \\mathrm{sgn} [2 \\eta (X) - 1] \\times \\mathbb{1} [h^{*} \\neq h (X)],\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\lvert 2 \\eta (X) - 1 \\rvert \\times \\mathbb{1} [h^{*} \\neq h (X)]\n\\right].\n\\end{aligned}\n\nwhere the last equality holds since x \\times \\mathrm{sgn} [x] = \\lvert x \\rvert."
  },
  {
    "objectID": "Learning Theory/3_Effective_Class_Size.html#growth-function-shattering-coefficient",
    "href": "Learning Theory/3_Effective_Class_Size.html#growth-function-shattering-coefficient",
    "title": "29  Effective Class Size",
    "section": "Growth function (shattering coefficient)",
    "text": "Growth function (shattering coefficient)\nGiven a set of instances \\mathcal{S} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}, the projection of a hypothesis class \\mathcal{H} onto \\mathcal{S} is the set of all distinct labels that \\mathcal{H} can produce onto \\mathcal{S}\n\n\\mathcal{H} (\\mathcal{S}) = \\{ \\{ h (x_{1}), \\dots, h (x_{n}) \\}, \\forall h \\in \\mathcal{H} \\}.\n\nThe concept of the growth function (shattering coefficient) is to measure the richness of the decisions that a hypothesis class can make with respect to a dataset of size n.\n\nDefinition 29.1 The growth function of a hypothesis class \\mathcal{H} is defined as the maximum number of unique ways that the hypotheses in \\mathcal{H} can label any set of n instances\n\n\\Pi_{\\mathcal{H}} (n) = \\sup_{\\mathcal{S}: \\lvert \\mathcal{S} \\rvert = n} \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert.\n\n\nNote that \\Pi_{\\mathcal{H}} (n) \\leq 2^{n} for any \\mathcal{H} with binary labels."
  },
  {
    "objectID": "Learning Theory/3_Effective_Class_Size.html#vapnik-chervonenkis-vc-dimension",
    "href": "Learning Theory/3_Effective_Class_Size.html#vapnik-chervonenkis-vc-dimension",
    "title": "29  Effective Class Size",
    "section": "Vapnik-Chervonenkis (VC) dimension",
    "text": "Vapnik-Chervonenkis (VC) dimension\nWe say that a sample \\mathcal{S} is shattered by the hypothesis class \\mathcal{H} if a \\mathcal{H} can label \\mathcal{S} in every possible way. That is, \\mathcal{S} is shattered by \\mathcal{H} if\n\n\\mathcal{H} (\\mathcal{S}) = 2^{\\lvert \\mathcal{S} \\rvert}.\n\n\nThe Vapnik-Chervonenkis (VC) dimension of \\mathcal{H} is the size of the largest set that is shattered by \\mathcal{H}\n\n\\mathrm{VC} (\\mathcal{H}) = \\max_{n: \\mathcal{H} (\\mathcal{S}) = 2^{n}} n."
  },
  {
    "objectID": "Learning Theory/3_Effective_Class_Size.html#sauers-lemma",
    "href": "Learning Theory/3_Effective_Class_Size.html#sauers-lemma",
    "title": "29  Effective Class Size",
    "section": "Sauer’s lemma",
    "text": "Sauer’s lemma\nSauer’s lemma shows that the growth function of any hypothesis class \\mathcal{H} is upper-bounded by a function of its VC dimension.\n\nTheorem 29.1 For any hypothesis class \\mathcal{H} and any dataset size n, we have\n\n\\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove the lemma for any set of n instances and any hypothesis class \\mathcal{H} with \\mathrm{VC} (\\mathcal{H}) = d using induction on n and d.\nThat is, we will prove\n\nthe lemma is correct under the base cases where (n, d) = (0, d) and (n, d) = (n, 0),\nthe lemma is correct under general case for (n ,d) by assuming the lemma is true under the previous cases (m, c) where m &lt; n and c &lt; d.\n\nFirst the base case (n, d) = (0, d) implies that\n\n\\Pi_{\\mathcal{H}} (0) = 1 = {0 \\choose 0} = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {0 \\choose i}\n\nbecause any hypothesis class can have at most 1 distinct label on an empty set (n = 0).\nThen the base case (n, d) = (n, 0) means that \\mathcal{H} can only shatter the empty set \\emptyset and therefore according to the definition of shattering\n\n\\Pi_{\\mathcal{H}} (n) = \\mathcal{H} (\\emptyset) = 2^{0} = 1 = {n \\choose 0} = \\sum_{i = 0}^{0} {n \\choose i}.\n\nTherefore we have proven the lemma is true in the base case, that is,\n\n\\Pi_{\\mathcal{H}} (n) = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}\n\nis true when (n, d) = (0, d) and (n, d) = (n, 0).\nTo prove the general case of the lemma, take any set \\mathcal{S} = \\{\\mathbf{x}_{1} \\dots, \\mathbf{x}_{n} \\} with n instances and any hypothesis class \\mathcal{H}, we can classify each unique way of the label assignment \\pi \\in \\mathcal{H} (\\mathcal{S}) into 2 groups \\mathcal{G}_{1} and \\mathcal{G}_{2} based on whether \\pi can form a pair in \\mathcal{H} (\\mathcal{S}).\n\n(\\pi, \\pi') form a pair if \\pi (x_{i}) = \\pi' (x_{i}), \\forall i \\in [1, n - 1] and \\pi (x_{n}) = \\pi' (x_{n}) (\\pi agree with \\pi' for all x_{1}, \\dots, x_{n - 1} but not for x_{n}). We add the both \\pi and \\pi' to \\mathcal{G}_{1}.\n\\pi belongs to \\mathcal{G}_{2} if doesn’t belong to \\mathcal{G}_{1}.\n\nSince the pairs (\\pi, \\pi') \\in \\mathcal{G}_{1} have the same labels for \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n - 1} if we create \\mathcal{G}'_{1} by removing the labels for \\mathbf{x}_{n} in each \\pi \\in \\mathcal{G}_{1}\n\n\\mathcal{G}'_{1} = \\{ (\\pi (\\mathbf{x}_{1}), \\dots, \\pi (\\mathbf{x}_{n - 1})), \\forall \\pi \\in \\mathcal{G}_{1} \\} \\\\\n\nthen all of the pairs in \\mathcal{G}_{1} become the same label assignment, but if we create \\mathcal{G}_{2}' using the same procedure above, the resulting label assignments in \\mathcal{G}_{2}' are still unique.\nAlso, by the definition of \\mathcal{G}_{1}, \\mathcal{G}_{2}, \\mathcal{G}_{1}', and \\mathcal{G}_{2}', the label assignments in \\mathcal{G}_{1}' and \\mathcal{G}_{2}' do not overlap, and therefore\n\n\\mathcal{H} (\\mathcal{S}) = \\lvert \\mathcal{G}_{1} \\rvert + \\lvert \\mathcal{G}_{2} \\rvert = 2 \\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert.\n\nThen we will define 2 new hypotheses classes \\mathcal{H}_{1}, \\mathcal{H}_{2} whose domain is a set \\mathcal{S}' that is constructed by removing x_{n} from \\mathcal{S}\n\n\\mathcal{S}' = \\{\\mathbf{x}_{1} \\dots, \\mathbf{x}_{n - 1} \\}.\n\nand whose projections on \\mathcal{S}' are defined as\n\n\\mathcal{H}_{1} (\\mathcal{S}') = \\mathcal{G}_{1}' \\cup \\mathcal{G}_{2}' \\\\\n\\mathcal{H}_{2} (\\mathcal{S}') = \\mathcal{G}_{1}',\n\nand therefore,\n\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert = 2 \\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert = (\\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert) + \\lvert \\mathcal{G}_{1}' \\rvert = \\lvert \\mathcal{H}_{1} (\\mathcal{S}') \\rvert + \\lvert \\mathcal{H}_{2} (\\mathcal{S}') \\rvert.\n\nAlthough we never exactly defined what \\mathcal{H}_{1} and \\mathcal{H}_{2} are, we have completely specified their projections on the entire domain \\mathcal{S}', using which we can derive\n\n\\mathrm{VC} (\\mathcal{H}_{1}) \\leq \\mathrm{VC} (\\mathcal{H}),\n\nsince the \\mathcal{H} has all the same label assignments for \\mathcal{S}' = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n - 1} \\} that \\mathcal{H}_{1} has and any subset of \\mathcal{S}' that is shattered by \\mathcal{H}_{1} is shattered by \\mathcal{H}.\nFurthermore, since \\mathcal{H}_{2} (\\mathcal{S}') = \\mathcal{G}_{1}, if a subset \\mathcal{T} \\subseteq \\mathcal{S}' is shattered by \\mathcal{H}_{2}, then the set \\mathcal{T} \\cup \\{ x_{n} \\} must be shattered by \\mathcal{H}, which means\n\n\\mathrm{VC} (\\mathcal{H}_{2}) + 1 \\leq \\mathrm{VC} (\\mathcal{H}).\n\nNow we are ready to prove the general case of the lemma using the all results we proved above with \\mathcal{H}_{1} and \\mathcal{H}_{2}.\nAccording to the definition of growth function, for any hypothesis class \\mathcal{H} and any set \\mathcal{S} of size n\n\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert \\leq \\Pi_{\\mathcal{H}} (n)\n\nand since we have assumed that the lemma is correct for the case (m, c) where m &lt; n, c &lt; d,\n\n\\Pi_{\\mathcal{H}} (m) \\leq \\sum_{i = 0}^{c} {m \\choose i},\n\nwe have\n\n\\begin{aligned}\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert\n& = \\lvert \\mathcal{H}_{1} (\\mathcal{S}') \\rvert + \\lvert \\mathcal{H}_{2} (\\mathcal{S}') \\rvert\n\\\\\n& \\leq \\Pi_{\\mathcal{H}_{1}} (n - 1) + \\Pi_{\\mathcal{H}_{2}} (n - 1).\n\\\\\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{1})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{2})} {n - 1 \\choose i}\n\\\\\n\\end{aligned}\n\nSince we have proved that the relations between \\mathrm{VC} (\\mathcal{H}_{1}), \\mathrm{VC} (\\mathcal{H}_{2}) and \\mathrm{VC} (\\mathcal{H}),\n\n\\begin{aligned}\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{1})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{2})} {n - 1 \\choose i}\n\\\\\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}) - 1} {n - 1 \\choose i}\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 1}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i - 1} + {n - 1 \\choose -1}\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i - 1}\n& [{n - 1 \\choose - 1} = 0]\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} \\left[\n    {n - 1 \\choose i} + {n - 1 \\choose i - 1}\n\\right]\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}.\n\\end{aligned}\n\nSince all of the above proof is for any \\mathcal{S}, it also works for the largest \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert,\n\n\\sup_{\\mathcal{S}} \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert = \\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i},\n\nwhich proves the lemma under the general case.\n\n\n\nThe following theorem uses Sauer’s lemma to provide a closed form upper-bound of the growth function of any hypothesis class with its VC dimension.\n\nTheorem 29.2 For any 1 &lt; d = \\mathrm{VC} (\\mathcal{H}) &lt; n, we have\n\n\\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{d} {n \\choose i} \\leq \\left(\n    \\frac{ e }{ d } n\n\\right)^{d} = O (n^d).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that (\\frac{ d }{ n })^{d} &lt; (\\frac{ d }{ n })^{i}, i &lt; d since d &lt; n, and therefore\n\n\\begin{aligned}\n\\sum_{i = 0}^{d} {n \\choose i} \\left(\n    \\frac{ d }{ n }\n\\right)^{d}\n& \\leq \\sum_{i = 0}^{d} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i}\n\\right]\n\\\\\n& = \\sum_{i = 0}^{d} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\\\\n& \\leq \\sum_{i = 0}^{n} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\end{aligned}\n\nBy applying Binomial theorem (x + y)^{n} = \\sum_{i = 0}^{n} {n \\choose i} x^{i} y^{n - i}\n\n\\begin{aligned}\n\\sum_{i = 0}^{d} {n \\choose i} \\left(\n    \\frac{ d }{ n }\n\\right)^{d}\n& \\leq \\sum_{i = 0}^{n} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\\\\n& = \\left(\n    \\frac{ d }{ n } + 1\n\\right)^{n}\n\\\\\n& \\leq e^{d}.\n\\end{aligned}\n\n\n\n\nThe theorem above shows that the VC dimension marks the threshold between the exponential growth and polynomial growth of the growth function.\n\nWhen n &lt; d, by the definition of the VC dimension, we can always find a set of instances of size n \\mathcal{H} can shatter, so the growth function \\Pi_{\\mathcal{H}} (n) = 2^{n}, which means it grows exponentially with a factor of 2 as n increases,\nWhen n &gt; d, by the theorem above, the growth function is upper bounded by n^{d}, so it only grows in polynomials as n increases."
  },
  {
    "objectID": "Learning Theory/4_Empirical_Risk_Minimization.html#no-free-lunch-theorem",
    "href": "Learning Theory/4_Empirical_Risk_Minimization.html#no-free-lunch-theorem",
    "title": "30  Empirical Risk Minimization",
    "section": "No free lunch theorem",
    "text": "No free lunch theorem\nThe no-free-lunch (NFL) theorem for machine learning states that there is no algorithm that can generate perfect hypothesis for any distribution using a finite training set.\n\nTheorem 30.1 TODO\n\nAnother interpretation of the NFL theorem is that any two algorithms are equivalent when their performance is averaged across all possible distributions. Therefore, one must make some biased assumptions about the underlying distribution about the targeted problems, so that the algorithm can be designed to have good performance on the interested problem but having bad performance on the problems that we don’t care."
  },
  {
    "objectID": "Learning Theory/4_Empirical_Risk_Minimization.html#erm-with-inductive-bias",
    "href": "Learning Theory/4_Empirical_Risk_Minimization.html#erm-with-inductive-bias",
    "title": "30  Empirical Risk Minimization",
    "section": "ERM with inductive bias",
    "text": "ERM with inductive bias\nTo apply the NFL theorem to ERM, we make assumptions about the underlying distribution by adding inductive bias to the ERM algorithm. The inductive bias implies that we have a predetermined preference for some hypotheses over other hypotheses. One reasonable approach to inductive bias is to restrict the hypothesis class that ERM considers. Therefore, instead of looking for the best hypothesis over all possible functions, ERM method minimizes the risk over a selected hypothesis class \\mathcal{H} to derive the empirical risk minimizer h_{n}\n\nh_{n} = \\argmin_{h \\in \\mathcal{H}} R_{n} (h).\n\n\nLemma 30.1 A special property of the ERM over any hypothesis class \\mathcal{H} is that\n\n\\forall h \\in \\mathcal{H}: R (h) - R (h_{n}) \\leq 2 \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition, for all h \\in \\mathcal{H},\n\n\\begin{aligned}\nR (h) - R (h_{n})\n& = (R (h) + R_{n} (h) - R_{n} (h)) - (R (h_{n}) + R_{n} (h_{n}) - R_{n} (h_{n}))\n\\\\\n& = (R_{n} (h) - R_{n} (h_{n})) + (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\\end{aligned}\n\nSince h_{n} is the one that minimizes R_{n},\n\nR_{n} (h) - R_{n} (h_{n}) \\geq 0,\n\nso\n\nR (h) - R (h_{n}) \\leq (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\nSince both h, h_{n} \\in \\mathcal{H},\n\nR (h) - R_{n} (h) \\leq \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert\n\\\\\nR_{n} (h_{n}) - R (h_{n}) \\leq \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert,\n\nso\n\nR (h) - R (h_{n}) \\leq (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\\leq 2 \\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert."
  },
  {
    "objectID": "Learning Theory/5_Uniform_Convergence.html#uniform-convergence-property",
    "href": "Learning Theory/5_Uniform_Convergence.html#uniform-convergence-property",
    "title": "31  Uniform Convergence",
    "section": "Uniform convergence property",
    "text": "Uniform convergence property\nThe uniform convergence property of a given hypothesis class \\mathcal{H} states that there exists a large enough sample size such that for all hypotheses in the class \\mathcal{H}, the empirical risk is close to the true risk with high probability, regardless of the underlying distribution \\mathbb{P}_{Z}.\n\nDefinition 31.1 (Uniform convergence property) A hypothesis class \\mathcal{H} has the uniform convergence property if,\n\ngiven a set of labeled instances \\mathcal{S}, where instances and labels are sampled from any joint distribution \\mathbb{P}_{Z} over the instance space and the label space, and there exists a function for some \\epsilon &gt; 0 and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nfor every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P}_{\\mathcal{S}} (\\lvert R (h) - R_{\\mathcal{S}} (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\nNote that this definition is stated using the sample complexity n_{\\mathcal{H}} (\\epsilon, \\delta), which is the sample size that we need to supply, so that with an arbitrarily high probability 1 - \\delta, the considered event has an arbitrarily small error \\epsilon."
  },
  {
    "objectID": "Learning Theory/5_Uniform_Convergence.html#uniform-convergence-results",
    "href": "Learning Theory/5_Uniform_Convergence.html#uniform-convergence-results",
    "title": "31  Uniform Convergence",
    "section": "Uniform convergence results",
    "text": "Uniform convergence results\nThe following theorems state that a hypothesis class has the uniform convergence property if either it has a finite number of hypotheses or has a finite VC dimension.\n\nTheorem 31.1 (Uniform convergence theorem) Any finite hypothesis class \\mathcal{H} has the uniform convergence property with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 2 }{ \\delta }\n}{\n    2 \\epsilon^{2}\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the true risk of a hypothesis is the expectation of the empirical risk with respect to the joint distribution \\mathbb{P}_{Z}\n\nR(h) = \\mathbb{E}_{Z} \\left[\n    R_{n} (h)\n\\right] = \\mathbb{E}_{Z} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (h (z_{i}))\n\\right]\n\nand we can view the 0-1 loss on an instance as a bounded random variable\n\nL_{i} = L (h (\\mathbf{X_{i}}), Y_{i}) = \\mathbb{1} \\left[\n    h (\\mathbf{X_{i}}) \\neq Y_{i}\n\\right] \\in [0, 1],\n\nwe can apply Hoeffding’s inequality on L_{i} for a fixed hypothesis h \\in \\mathcal{H},\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L_{i} - \\mathbb{E}_{Z} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L_{i}\n        \\right]\n    \\right\\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} \\epsilon^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    \\lvert R_{n} (h) - \\mathbb{E}_{Z} \\left[\n        R_{n} (h)\n    \\right] \\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} \\epsilon^{2} }{ n }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    \\lvert R_{n} (h) - R (h) \\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right].\n\\end{aligned}\n\nThe above inequality only works for one h \\in \\mathcal{F}. we can apply union bound to extend it for all f \\in \\mathcal{F},\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\exist f \\in \\mathcal{F}, \\lvert R_{n} (f) - R (f) \\rvert \\geq \\epsilon\n\\right)\n& \\leq \\sum_{i = 1}^{\\lvert \\mathcal{F} \\rvert} \\mathbb{P} \\left(\n    \\lvert R_{n} (f_{i}) - R (f_{i}) \\rvert \\geq \\epsilon\n\\right)\n\\\\\n& \\leq 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right].\n\\end{aligned}\n\nSince \\mathbb{P} (X \\geq a) = 1 - \\mathbb{P} (X \\leq a)\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\exist f \\in \\mathcal{F}, \\lvert R_{n} (f) - R (f) \\rvert \\leq \\epsilon\n\\right)\n& \\geq 1 - 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right]\n\\\\\n& \\geq 1 - \\delta\n\\end{aligned}\n\nwhere\n\n\\begin{aligned}\n\\delta\n& = 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right]\n\\\\\nn\n& = \\frac{\n    \\log \\lvert \\mathcal{F} \\rvert + \\log \\frac{ 2 }{ \\delta }\n}{\n    2 \\epsilon^{2}\n}.\n\\end{aligned}\n\n\n\n\n\nTheorem 31.2 Any infinite hypothesis class \\mathcal{H} with a finite VC dimension has the uniform convergence property with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = 8 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 4 }{ \\delta }\n}{\n    \\epsilon^{2}\n}\n\nwhere n is the number of samples in the training set.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet’s first define 3 “bad” events that are useful in the following proof.\nGiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, let B (\\mathcal{S}) denote the event that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its true risk and empirical risk on \\mathcal{S} is larger than \\epsilon,\n\nB (\\mathcal{S}) \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}} (h) -  R (h) \\rvert \\geq \\epsilon.\n\nand therefore we want to prove\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq \\delta.\n\nNow let’s draw the “ghost samples”, which is another set of i.i.d labeled instances \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} from the distribution \\mathbb{P}_{Z}, and define another event B' as a function of \\mathcal{S} and \\mathcal{S}', which states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S} and empirical risk on \\mathcal{S}' is larger than \\frac{ \\epsilon }{ 2 },\n\nB' (\\mathcal{S}, \\mathcal{S}') \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}} (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }.\n\nFinally, let’s define an event B (\\mathcal{S}, \\mathcal{S}', \\sigma) as a function of \\mathcal{S}, \\mathcal{S}', and a set of independent Rademacher random variables \\sigma_{1}, \\dots, \\sigma_{n} that takes values -1 or 1 with equal probabilities\n\n\\begin{aligned}\nB'' (\\mathcal{S}, \\mathcal{S}', \\sigma)\n& \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}_{\\sigma}} (h) - R_{\\mathcal{S}_{\\sigma}'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\\\\n& \\coloneqq \\exist h \\in \\mathcal{H}: \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n        \\mathbb{1} \\left[\n            h (\\mathbf{x}_{i}) \\neq y_{i}\n        \\right] - \\mathbb{1} \\left[\n            h (\\mathbf{x}_{i}') \\neq y_{i}'\n        \\right]\n    \\right)\n\\right\\rvert \\geq \\frac{ \\epsilon }{ 2 },\n\\\\\n\\end{aligned}\n\nwhere the samples \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' are created by swapping the labeled instances in \\mathcal{S}, \\mathcal{S}' based on the values of \\sigma\n\nz_{i} and z_{i}' are swapped if the corresponding \\sigma_{i} = 1,\nand z_{i} and z_{i}' are not swapped if the corresponding \\sigma_{i} = -1.\n\nThe event B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S}_{\\sigma} and empirical risk on \\mathcal{S}_{\\sigma}' is larger than \\frac{ \\epsilon }{ 2 },\nClaim 1: \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) is upper-bounded by 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')),\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\nSince the probability of an event cannot be larger than its conjunction with another event,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n& \\geq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\cap B (\\mathcal{S}))\n\\\\\n& = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n\\end{aligned}\n\nNow consider the probability of the event\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})),\n\nwhich can be written as\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\leq \\frac{ \\epsilon }{ 2 }\n\\right)\n\nbecause it is the same as the \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left( \\lvert R_{\\mathcal{S}} (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 } \\right) if the event B (\\mathcal{S}) \\coloneqq \\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\geq \\epsilon is given.\nSince R (h) is the mean of R_{\\mathcal{S}'} (h), the probability of the difference between R (h) and R_{\\mathcal{S}'} (h) can be upper bounded by applying Chebyshev’s inequality with X = R_{\\mathcal{S}'} (h), \\mu = R (h), t = \\frac{ \\epsilon }{ 2 }, \\sigma^{2} = \\mathrm{Var} [R_{\\mathcal{S}'} (h)]\n\n\\begin{aligned}\n\\mathbb{P}_{X} \\left(\n    \\lvert x - \\mu \\rvert \\geq t\n\\right)\n& \\leq \\frac{ \\sigma^{2} }{ t^{2} }\n\\\\\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R_{S'} (h) - R (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\leq \\frac{ 4 \\mathrm{Var} [R_{\\mathcal{S}'} (h)] }{\\epsilon^{2}}.\n\\end{aligned}\n\nNote that h (\\mathbf{x}_{i}) \\neq y_{i} is a Bernoulli random variable whose variance is less than \\frac{ 1 }{ 4 }\n\n\\mathrm{Var} [R_{\\mathcal{S}'} (h)] = \\mathrm{Var} \\left[\n    \\frac{ 1 }{ n } \\sum_{\\mathbf{x} \\in \\mathcal{S}'} h (\\mathbf{x}_{i}) \\neq y_{i}\n\\right] = \\frac{ 1 }{ n^{2} } \\sum_{\\mathbf{x_{i} \\in \\mathcal{S}'}} \\mathrm{Var} [h (\\mathbf{x}) \\neq y_{i}] \\leq \\frac{ 1 }{ 4 n },\n\nand therefore,\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right) \\leq \\frac{ 1 }{n \\epsilon^{2}}.\n\nAssume that n \\epsilon^{2} \\geq 2\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\leq \\frac{ 1 }{ 2 }\n\\\\\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\leq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\geq \\frac{ 1 }{ 2 }.\n\\\\\n\\end{aligned}\n\nThen we have proved the claim\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\frac{ 1 }{ 2 } \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\\end{aligned}\n\nClaim 2: the probability of event B' (\\mathcal{S}, \\mathcal{S}') is the same as the expectation of the probability that B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) happens given \\mathcal{S}, \\mathcal{S}'\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n= \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right].\n\nSince the event B' (\\mathcal{S}, \\mathcal{S}') and B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) only differ on the set of instances \\mathcal{S}, \\mathcal{S}' and \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' and they can both be seen as the set of instances i.i.d sampled from the \\mathbb{P}_{Z}, their probability should be the same\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')) = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)).\n\nThen, we can prove the claim by using marginalization of the probability\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)) = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')].\n\nClaim 3: \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')] is upper-bounded by 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right]\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\nConsider the following probability for a fixed h \\in \\mathcal{H},\n\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n            \\mathbb{1} \\left[\n                h (\\mathbf{x}_{i}) \\neq y_{i}\n            \\right] - \\mathbb{1} \\left[\n                h (\\mathbf{x}_{i}') \\neq y_{i}'\n            \\right]\n        \\right)\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right).\n\nSince \\mathcal{S}, \\mathcal{S}' are given, the value \\alpha_{i} = \\mathbb{1} \\left[ h (\\mathbf{x}_{i}) \\neq y_{i} \\right] - \\mathbb{1} \\left[ h (\\mathbf{x}_{i}') \\neq y_{i}' \\right] is a fixed value and therefore\n\n\\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n    \\mathbb{1} \\left[\n        h (\\mathbf{x}_{i}) \\neq y_{i}\n    \\right] - \\mathbb{1} \\left[\n        h (\\mathbf{x}_{i}') \\neq y_{i}'\n    \\right]\n\\right) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n\nis a random variable with\n\n\\mathbb{E}_{\\sigma} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n\\right] = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\mathbb{E}_{\\sigma} [\\sigma_{i}] = 0.\n\nApplying Hoeffding’s inequality with X_{i} = \\alpha_{i} \\sigma_{i}, \\mu = 0, t = \\frac{ \\epsilon }{ 2 },\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i} - \\mathbb{E} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i}\n        \\right]\n    \\right\\rvert \\geq t\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} t^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i} - 0\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n& \\leq 2 \\exp \\left[\n    - \\frac{ 2 n^{2} \\frac{ \\epsilon^{2} }{ 4 } }{ 4 n }\n\\right]\n\\\\\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n& \\leq 2 \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nTo get the probability for any h \\in \\mathcal{H}, we apply union bound on all possible label assignments that \\mathcal{H} can make over the set \\mathcal{S} \\cup \\mathcal{S}',\n\n\\begin{aligned}\n\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n& =\n\\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H}: \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}'): \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\sum_{h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}')} \\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\\\\n\\end{aligned}\n\nNote that the term 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right] doesn’t depend on \\mathcal{S}, \\mathcal{S}'. Since the expectation of a constant is that constant, we have proved the claim\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n        - \\frac{ n \\epsilon^{2} }{ 8 }\n    \\right]\n\\right] \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\nFinally we can prove the theorem by using all of the claims above\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n& = 2 \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right]\n\\\\\n& \\leq 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nBy setting \\delta = 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right],\n\n\\begin{aligned}\n\\delta\n& = 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right]\n\\\\\nn\n& = 8 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 4 }{ \\delta }\n}{\n    \\epsilon^{2}   \n}.\n\\end{aligned}"
  },
  {
    "objectID": "Learning Theory/6_PAC_Learning.html#realizable-case",
    "href": "Learning Theory/6_PAC_Learning.html#realizable-case",
    "title": "32  PAC Learning",
    "section": "Realizable case",
    "text": "Realizable case\nUnder the realizable assumption, it is assumed that there exists a perfect concept from a concept class c \\in \\mathcal{C} such that all labels of the instances are labeled according to c and the hypothesis class that our algorithm ERM considers is the concept class \\mathcal{H} = \\mathcal{C}.\n\nDefinition 32.1 (Consistent) We say that a hypothesis h is consistent with a set of labeled instances \\mathcal{S} = \\{ (\\mathbf{x}_{1}, y_{1}), \\dots, (\\mathbf{x}_{n}, y_{n}) \\} if h (\\mathbf{x}_{i}) = y_{i} for all i.\n\nTherefore, under the realizable assumption, ERM can always find a hypothesis that is consistent with any given training set, and therefore we say that ERM learns in the consistency model.\n\nConsistency model\nLearning in the consistency model requires the algorithm to always predict correctly on the training set, but doesn’t care much about the generalization of the performance on the test set.\n\nDefinition 32.2 (Consistency model) An algorithm A learns the hypothesis class \\mathcal{H} = \\mathcal{C} in the consistency model if\n\ngiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, where instances are sampled from any distribution \\mathbb{P}_{\\mathbf{X}} over the instance space and are labeled by any concept c \\in \\mathcal{C},\nA can find a concept h \\in \\mathcal{H} that is consistent with \\mathcal{S} if h exists, or A outputs False if no such concept exists.\n\n\n\n\nProbably Approximately Correct (PAC) model\nLearning in the PAC model is more applicable in real world, as it emphasizes more on the generalization ability of the learned function from the algorithm.\n\nDefinition 32.3 (PAC model) An algorithm A learns the concept class \\mathcal{C} in the PAC model by the hypothesis class \\mathcal{H} = \\mathcal{C} if,\n\ngiven a set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, where instances are sampled from any distribution \\mathbb{P}_{\\mathbf{X}} over the instance space and are labeled by any concept c \\in \\mathcal{C}, and there exists a function for some $&gt; 0 $ and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nA returns a hypothesis h \\in \\mathcal{H}, where its true risk is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P} (R (h) \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\n\n\nERM as a PAC learner\nHere we present some results about the generalization error of the algorithms using the definitions of consistency model and PAC model. Since ERM learns the hypothesis class in the consistency model, the following theorems naturally apply to it.\nThe following theorem states that a finite concept class \\mathcal{C} is PAC learnable by the same hypothesis class \\mathcal{H} = \\mathcal{C} if \\mathcal{C} is learnable in the consistency model, and proves its sample complexity as a function of the size of the hypothesis class.\n\nTheorem 32.1 If an algorithm A learns a finite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 1 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAnother way to state the PAC learnability with the consistency model is\n\n\\mathbb{P}_{\\mathcal{S}} (\\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon) \\leq \\delta\n\nwhen n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta).\nGiven h \\in \\mathcal{H}, by definition of the empirical risk we can write the probability that h is consistent with \\mathcal{S} as\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (R_{\\mathcal{S}} (h) = 0)\n& = \\mathbb{P}_{\\mathcal{S}} (h (\\mathbf{x_{i}}) = y_{i}, \\forall (\\mathbf{x}_{i}, y_{i}) \\in \\mathcal{S})\n\\\\\n& \\stackrel{(1)}{=} \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}} (h (\\mathbf{x}_{i}) = y_{i})\n\\\\\n& = \\prod_{i = 1}^{n} 1 - \\mathbb{P}_{\\mathbf{X}} (h (\\mathbf{x}_{i}) \\neq y_{i})\n\\\\\n& \\stackrel{(2)}{=} \\prod_{i = 1}^{n} 1 - R (h)\n\\\\\n& = (1 - R (h))^{n}.\n\\end{aligned}\n\n\n\nfollows because the labeled instances in \\mathcal{S} are independent.\n\n\nfollows because the true risk of h is the probability of h makes a mistake on a given labeled instance when the loss function is the 0-1 loss.\n\n\nIf we add the fact that R (h) \\geq \\epsilon,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon)\n& \\leq (1 - \\epsilon)^{n}\n\\\\\n& \\leq e^{- n \\epsilon}\n\\end{aligned}\n\nwhere the last inequality uses the fact that\n\n1 - x &lt; e^{-x}, \\forall x \\in [0, 1].\n\nWe can add the part \\exists h \\in \\mathcal{H} by applying the union bound\n\n\\mathbb{P}_{\\mathcal{S}} (\\exists h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon)\n\\leq \\lvert \\mathcal{H} \\rvert e^{- n \\epsilon},\n\nand make \\delta = \\lvert \\mathcal{H} \\rvert e^{- n \\epsilon}, we can derive\n\nn \\geq \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 1 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\nThe following theorem states a similar results as above: an infinite concept class \\mathcal{C} it is PAC learnable by the same hypothesis class \\mathcal{H} = \\mathcal{C} if \\mathcal{H} is learnable in the consistency model, and proves the sample complexity as a function of the growth function of \\mathcal{H}.\n\nTheorem 32.2 If an algorithm A learns an infinite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = 2 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet’s first define 3 “bad” events that are useful in the following proof.\nGiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, the difference between its true risk and empirical risk on \\mathcal{S} is larger than \\epsilon, let B (\\mathcal{S}) denote the event that there exists a hypothesis h \\in \\mathcal{H} that is consistent with \\mathcal{S} but has the true risk larger than \\epsilon\n\nB (\\mathcal{S}) \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0,  R (h) \\geq \\epsilon.\n\nand therefore we want to prove\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq \\delta.\n\nNow let’s draw the “ghost samples”, which is another set of i.i.d labeled instances \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} from the distribution \\mathbb{P}_{Z}, and define another event B' as a function of \\mathcal{S} and \\mathcal{S}', which states that there exists a hypothesis h \\in \\mathcal{H} that is consistent with \\mathcal{S} but has empirical risk on \\mathcal{S}' larger than \\frac{ \\epsilon }{ 2 }\n\nB' (\\mathcal{S}, \\mathcal{S}') \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0,  R_{S'} (h) \\geq \\frac{ \\epsilon }{ 2 }.\n\nFinally, let’s define an event B (\\mathcal{S}, \\mathcal{S}', \\sigma) as a function of \\mathcal{S}, \\mathcal{S}', and a set of independent Rademacher random variables \\sigma_{1}, \\dots, \\sigma_{n} that takes values -1 or 1 with equal probabilities\n\nB'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 }.\n\nwhere the samples \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' are created by swapping the labeled instances in \\mathcal{S}, \\mathcal{S}' based on the values of \\sigma\n\nz_{i} and z_{i}' are swapped if the corresponding \\sigma_{i} = 1,\nand z_{i} and z_{i}' are not swapped if the corresponding \\sigma_{i} = -1.\n\nThe event B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S}_{\\sigma} and empirical risk on \\mathcal{S}_{\\sigma}' is larger than \\frac{ \\epsilon }{ 2 }.\nClaim 1: \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) is upper-bounded by 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')),\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\nSince the probability of an event cannot be larger than its conjunction with another event,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n& \\geq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\cap B (\\mathcal{S}))\n\\\\\n& = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n\\end{aligned}\n\nNow consider the probability of the event\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})),\n\nwhich can be written as\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n\nbecause the event B' (\\mathcal{S}, \\mathcal{S}') is the event R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 } if the event B (\\mathcal{S}) is given.\nSince R (h) is the mean of R_{\\mathcal{S}'} (h), we can apply the lower tail case of the Chernoff bound for the average of Bernoulli variables and set X = R_{\\mathcal{S}'} (h), \\mu = R (h), \\delta = \\frac{ 1 }{ 2 }\n\n\\begin{aligned}\n\\mathbb{P}_{X} (X \\leq (1 - \\delta) \\mu)\n& \\leq \\exp \\left[\n    -\\frac{ n \\delta^{2} \\mu }{ 2 }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ R (h) }{ 2 }\n\\right)\n& \\leq \\exp \\left[\n    -\\frac{ n R (h) }{ 8 }\n\\right].\n\\end{aligned}\n\nSince R (h) \\geq \\epsilon and the assumption states that n &gt; \\frac{ 8 }{ \\epsilon }\n\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ \\epsilon }{ 2 }\n\\right) \\leq \\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ R (h) }{ 2 }\n\\right) \\leq \\exp \\left[\n    \\frac{ - n R(h) }{ 8 }\n\\right] \\leq \\exp \\left[\n    \\frac{ - R(h) }{ \\epsilon }\n\\right] \\leq \\frac{ 1 }{ e } \\leq \\frac{ 1 }{ 2 }\n\\\\\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 }\n\\right) \\geq \\frac{ 1 }{ 2 }\n\nThen we have proved the claim\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\frac{ 1 }{ 2 } \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\\end{aligned}\n\nClaim 2: the probability of event B' (\\mathcal{S}, \\mathcal{S}') is the same as the expectation of the probability that B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) happens given \\mathcal{S}, \\mathcal{S}'\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n= \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right].\n\nSince the event B' (\\mathcal{S}, \\mathcal{S}') and B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) only differ on the set of instances \\mathcal{S}, \\mathcal{S}' and \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' and they can both be seen as the set of instances i.i.d sampled from the \\mathbb{P}_{Z}, their probability should be the same\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')) = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)).\n\nThen, we can prove the claim by using marginalization of the probability\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)) = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')].\n\nClaim 3: \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')] is upper-bounded by \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon }{ 2 } \\right]\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon }{ 2 }\n\\right].\n\nRemember that \\mathcal{S}, \\mathcal{S}' all have n instances and therefore there are n pairs of instances (\\mathbf{x}_{1}, \\mathbf{x}_{1}'), \\dots, (\\mathbf{x}_{n}, \\mathbf{x}_{n}'). There are 3 cases for the corrections of the predictions made by h for each pair (h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}')).\n\nBoth h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}') are incorrect.\nEither h (\\mathbf{x}_{i}) or h (\\mathbf{x}_{i}') is incorrect (correct).\nBoth h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}') are correct.\n\nFirst if there is a pair in \\mathcal{S}, \\mathcal{S}' with case 1, then\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = 0\n\nbecause R_{\\mathcal{S}_{\\sigma}} (h) &gt; 0 no matter how to generate \\mathcal{S}_{\\sigma} by swapping instances in \\mathcal{S}, \\mathcal{S}'.\nThen denoted by r the number of pairs in \\mathcal{S}, \\mathcal{S}' that case 2 is true, if r &lt; \\frac{ \\epsilon n }{ 2 },\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = 0\n\nbecause R_{\\mathcal{S}_{\\sigma}'} (h) &lt; \\frac{ \\epsilon }{2} no matter how to generate \\mathcal{S}_{\\sigma}' by swapping instances in \\mathcal{S}, \\mathcal{S}'.\nWhen r \\geq \\frac{ \\epsilon n }{ 2 }, the event R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } is possible and its possibility is\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = \\left(\n    \\frac{ 1 }{ 2 }\n\\right)^{r} \\leq 2^{- \\frac{ \\epsilon n }{ 2 }}\n\nbecause the independent Rademacher random variables in \\sigma must take 1 with probability \\frac{ 1 }{ 2 } for all r' mistakes that were in \\mathcal{S} and swapped to be in \\mathcal{S}_{\\sigma}', and take -1 with probability \\frac{ 1 }{ 2 } for the r - r' mistakes that were in \\mathcal{S}' and are stayed in \\mathcal{S}_{\\sigma}'.\nSince the probability of the case 3 is already included in the calculation of the above probabilities, we can prove the claim by adding probabilities for all cases\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) \\leq 2^{- \\frac{ \\epsilon n }{ 2 }}.\n\nTo get the probability for any h \\in \\mathcal{H}, we apply union bound on all possible label assignments that \\mathcal{H} can make over the set \\mathcal{S} \\cup \\mathcal{S}',\n\n\\begin{aligned}\n\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}'): R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\sum_{h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}')} \\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ \\epsilon n }{ 2 }},\n\\\\\n\\end{aligned}\n\nwhere the last inequality is because of the definition of the growth function states that\n\n\\lvert \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}') \\rvert \\leq \\Pi_{H} (\\lvert \\mathcal{S} \\rvert + \\lvert \\mathcal{S}' \\rvert) = \\Pi_{\\mathcal{H}} (2 n).\n\nNote that the term \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }} doesn’t depend on \\mathcal{S}, \\mathcal{S}'. Since the expectation of a constant is that constant, we have proved the claim\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n        - \\frac{ n \\epsilon }{ 2 }\n    \\right]\n\\right] \\leq \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon }{ 2 }\n\\right].\n\nFinally we can prove the theorem by using all of the claims above\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n& = 2 \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right]\n\\\\\n& \\leq 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }}.\n\\end{aligned}\n\nBy setting \\delta = 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }},\n\n\\begin{aligned}\n\\delta\n& = 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }}\n\\\\\nn\n& = 2 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\\end{aligned}\n\n\n\n\nNow we can use the Sauer’s lemma to get a nice closed form expression on sample complexity result for the infinite class.\n\nTheorem 32.3 If an algorithm A learns an infinite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    8 d \\log \\frac{ 16 }{ \\epsilon} + 4 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n},\n\nwhere d = \\mathrm{VC} (\\mathcal{H}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Sauer’s lemma to the sample complexity results for the infinite classes\n\n\\begin{aligned}\n\\frac{\n    4 \\log \\Pi_{\\mathcal{H}} (2 n) + 2 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}\n& \\leq \\frac{\n    4 \\log \\left(\n        \\frac{ 2 e n }{ d }\n    \\right)^{d} + 2 \\log \\frac { 2 }{ \\delta }\n}{\n    \\epsilon\n}\n\\\\\n& = \\frac{ 4 d }{ \\epsilon } \\log n\n+ \\frac{ 4 d  }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\end{aligned}\n\nSince \\log x \\leq a x - \\log a - 1 for a, x &gt; 0, we can show that\n\n\\begin{aligned}\n\\log n\n& \\leq \\frac{ \\epsilon n }{ 8 d } - \\log \\frac{ \\epsilon }{ 8 d  } - 1\n\\\\\n\\frac{ 4 d }{ \\epsilon } \\log n\n& \\leq \\frac{ 4 d }{ \\epsilon } \\left(\n    \\frac{ \\epsilon n }{ 8 d } + \\log \\frac{ 8 d }{ \\epsilon } - 1\n\\right)\n\\\\\n& = \\frac{ n }{ 2 } + \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 8 d }{\\epsilon e }.\n\\end{aligned}\n\nBy combining the results above,\n\n\\begin{aligned}\n\\frac{\n    4 \\log \\Pi_{\\mathcal{H}} (2 n) + 2 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}\n& \\leq \\frac{ 4 d }{ \\epsilon } \\log n\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n& \\leq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 8 d }{\\epsilon e }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n& \\leq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }.\n\\end{aligned}\n\nTherefore, if we have a training set that has a number of instances\n\n\\begin{aligned}\nn\n& \\geq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n\\frac{ n }{ 2 }\n& \\geq \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\nn\n& \\geq \\frac{\n    8 d \\log \\frac{ 16 }{ \\epsilon} + 4 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\\end{aligned}"
  },
  {
    "objectID": "Learning Theory/6_PAC_Learning.html#unrealizable-case",
    "href": "Learning Theory/6_PAC_Learning.html#unrealizable-case",
    "title": "32  PAC Learning",
    "section": "Unrealizable case",
    "text": "Unrealizable case\nThe PAC learning in the unrealizable setting is also called agnostic PAC learning where the perfect concept cannot be realized because either one of the following events happens\n\nthe concept that the algorithm A learns is not in the hypothesis class that A considers,\nany instance can have contradictory labels, amd therefore there doesn’t exist a concept that can perfectly label all instances in the input space.\n\n\nAgnostic PAC model\n\nDefinition 32.4 An algorithm A learns the concept class \\mathcal{C} in the agnostic PAC model by the hypothesis class \\mathcal{H} if,\n\ngiven a set of labeled instances \\mathcal{S}, where instances and labels are sampled from any joint distribution \\mathbb{P}_{Z} over the instance space and the label space, and there exists a function for some $&gt; 0 $ and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nA returns a hypothesis h \\in \\mathcal{H}, where the difference between its true risk and the minimum true risk achieved by any hypothesis in \\mathcal{H} is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P} (\\lvert R (h) - \\min_{h \\in \\mathcal{H}} R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\n\n\nUniform convergence implies agnostic PAC of ERM\nThe uniform convergence result guarantees the agnostic PAC learnability of ERM.\n\nLemma 32.1 If A is the ERM algorithm that learns the hypothesis class \\mathcal{H}, which satisfies uniform convergence with sample complexity n_{\\mathcal{H}}^{u}, then A learns \\mathcal{H} in the agnostic PAC model with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = n_{\\mathcal{H}}^{u} (\\frac{ \\epsilon }{ 2 }, \\delta).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet h_{n} be the hypothesis learned by ERM. According the property of the ERM, we have\n\nR (h) - R (h_{n}) \\leq 2 \\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert.\n\nSince \\mathcal{H} has @def:uniform-convergence-property, if n \\geq n_{\\mathcal{H}}^{u} (\\hat{\\epsilon}, \\delta), then\n\n\\forall h \\in \\mathcal{H}, \\mathbb{P} (\\lvert R (h) - R_{n} (h) \\rvert \\leq \\hat{\\epsilon}) \\geq 1 - \\delta,\n\nwhich is equivalent of\n\n\\begin{aligned}\n\\mathbb{P} (\\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert \\leq \\hat{\\epsilon})\n& \\geq 1 - \\delta\n\\\\\n\\mathbb{P} (R (h) - R (h_{n}) \\leq 2 \\hat{\\epsilon})\n& \\geq 1 - \\delta.\n\\end{aligned}\n\nBy setting \\epsilon = 2 \\hat{\\epsilon}, we have the conclusion that if n \\geq n_{\\mathcal{H}}^{u} (\\frac{ \\epsilon }{ 2 }, \\delta), then\n\n\\mathbb{P} (R (h) - R (h_{n}) \\leq \\epsilon) \\geq 1 - \\delta,\n\nwhich is the definition of agonistic PAC learning.\n\n\n\nBy the lemma Lemma 32.1, we can easily derive the sample complexity results for agnostic PAC by plugging $ = $ to the sample complexity results of the uniform convergence."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#definitions",
    "href": "Learning Theory/7_Rademacher_Complexity.html#definitions",
    "title": "33  Rademacher Complexity",
    "section": "Definitions",
    "text": "Definitions\n\nDefinition 33.1 A Rademacher variable has a discrete probability distribution where X has the equal probability of being +1 and -1.\n\nThe empirical Rademacher complexity measures the ability of the functions in a function class \\mathcal{F} to fit the random noise for a fixed sample \\mathcal{S}, which is described by the maximum correlation over all f \\in \\mathcal{F} between f (z_{i}) and \\sigma_{i}.\n\nDefinition 33.2 Given an i.i.d sample \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} from the distribution \\mathbb{P}_{\\mathcal{Z}^{n}} and n independent Rademacher random variables \\sigma = \\{ \\sigma_{1}, \\dots, \\sigma_{n} \\}, the empirical Rademacher complexity of a class of binary function \\mathcal{F} is defined as\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F}) = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right],\n\nwhich is a function of the random variable \\mathcal{S} and therefore is a random variable.\n\nTherefore, the Rademacher complexity of \\mathcal{F} measures the expected noise-fitting-ability of \\mathcal{F} over all data sets \\mathcal{S} \\in \\mathcal{Z}^{n} that could be drawn according to the distribution \\mathbb{P}_{\\mathcal{Z}^{n}}.\n\nDefinition 33.3 Then the Rademacher complexity is defined as the expectation of the empirical Rademacher complexity over all i.i.d samples of size n\n\n\\mathrm{Rad}_{n} (\\mathcal{F}) = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n\\right] = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right]\n\\right].\n\n\nFor completeness, we include the definition of Rademacher average of a set of vectors.\n\nDefinition 33.4 Given n independent Rademacher random variables \\sigma = \\{ \\sigma_{1}, \\dots, \\sigma_{n} \\}, the Rademacher average of a set of vectors \\mathcal{A} \\subseteq \\mathbb{R}^{n} is\n\n\\mathrm{Rad}_{\\mathcal{A}} = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#rademacher-complexity-properties",
    "href": "Learning Theory/7_Rademacher_Complexity.html#rademacher-complexity-properties",
    "title": "33  Rademacher Complexity",
    "section": "Rademacher complexity properties",
    "text": "Rademacher complexity properties\n\nNon-negativity\nThe empirical Rademacher complexity and Rademacher complexity are non-negative.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right]\n\\\\\n& \\stackrel{(1)}{\\geq} \\sup_{f \\in \\mathcal{F}} \\mathbb{E}_{\\sigma} \\left[\n     \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right]\n\\\\\n& = \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{E}_{\\sigma} [ \\sigma_{i} ] f (z_{i})\n\\\\\n& \\stackrel{(2)}{=} 0.\n\\end{aligned}\n\nExplanations in the derivations\n\nSince \\sup is a convex function, (1) follows because of the Jensen’s inequality.\n\nfollows because of the definition of Rademacher variable.\n\n\n\n\n\n\n\nScaling and translation\nGiven any function class \\mathcal{F} and constants a, b \\in \\mathbb{R}, denote the function class \\mathcal{G} = \\{ g (x) = a f (x) + b \\}.\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{G}) = \\lvert a \\rvert \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n\n\n\\mathrm{Rad}_{n} (\\mathcal{G}) = \\lvert a \\rvert \\mathrm{Rad}_{n} (\\mathcal{F}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition of \\mathcal{G} and the empirical Rademacher complexity,\n\n\\begin{aligned}\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{G})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} (a f (z_{i}) + b)\n    \\right)\n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n        + \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b\n    \\right)\n\\right]\n\\\\\n& \\stackrel{(1)}{=} \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n    \\right)\n\\right]\n+ \\mathbb{E}_{\\sigma} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b\n\\right]\n\\\\\n& \\stackrel{(2)}{=} \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n    \\right)\n\\right]\n\\\\\n& \\stackrel{(3)}{=} \\lvert a \\rvert \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right)\n\\right]\n\\\\\n& = \\lvert a \\rvert \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F}).\n\\end{aligned}\n\nExplanations in the derivations\n\nSince the term \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b does not depend on f, it can be pulled out of \\sup_{f \\in \\mathcal{F}}. Then (1) follows because of the linearity of expectation.\n\nfollows because of the linearity of expectation and \\mathbb{E}_{\\sigma} [\\sigma_{i}] = 0.\n\nWhen a &lt; 0, \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i}) = \\lvert a \\rvert \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} - \\sigma_{i} f (z_{i}). Then (3) follows since \\sigma_{i} and -\\sigma_{i} have the same distribution."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#symmetrization-lemma",
    "href": "Learning Theory/7_Rademacher_Complexity.html#symmetrization-lemma",
    "title": "33  Rademacher Complexity",
    "section": "Symmetrization lemma",
    "text": "Symmetrization lemma\nHere we proved an important result with the Rademacher complexity using so called symmetrization technique, which involves creating a “ghost” sample as a hypothetical independent copy of the original sample.\n\nTheorem 33.1 For any class of measurable functions \\mathcal{F}, the expectation of the maximum error in estimating the mean of any function f \\in \\mathcal{F} is bounded by 2 times of the Rademacher complexity\n\n\\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n= \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right] \\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})\n\nwhere E_{\\mathcal{S}} (f) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f (z_{i}) is the estimated expectation of f on the sample \\mathcal{S}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy using the symmetrization technique, we introduce a ghost sample \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} that is also i.i.d drawn from \\mathbb{P}_{\\mathcal{Z}^{n}}, which means\n\n\\mathbb{E}_{\\mathcal{S}'} \\left[\n    E_{\\mathcal{S}'} (f)\n\\right] = \\mathbb{E}_{Z} [f (z_{i})].\n\nTherefore, we can get the following results\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right]\n& = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f (z_{i})\n        - \\mathbb{E}_{\\mathcal{S}'} \\left[\n            \\frac{ 1 }{ n } \\sum_{z_{i}' \\in \\hat{\\mathcal{S}}} f (z_{i}')\n        \\right]\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(1)}{=} \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\mathbb{E}_{\\mathcal{S}'} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n        \\right]\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(2)}{\\leq} \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n\\end{aligned}\n\nExplanations in the derivations\n\n\nuses the linearity of expectation and \\frac{ 1 }{ n } \\sum_{z_{i} \\in \\mathcal{S} f (z_{i})} is a constant.\n\n\nuses Jensen’s inequality since \\sup is a convex operator.\n\n\nSince f (z_{i}) - f (z_{i}') is invariant of sign change, we get\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n& = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}', \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(1)}{\\leq} \\mathbb{E}_{\\mathcal{S}, \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right\\rvert\n\\right]\n+ \\mathbb{E}_{\\hat{\\mathcal{S}}, \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} - \\sigma_{i} f (z_{i}')\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(2)}{=} 2 \\mathrm{Rad}_{n} (\\mathcal{F}).\n\\end{aligned}\n\nExplanations in the derivations\n\n\nfollows because of the \\sup_{f \\in \\mathcal{F}} operator,\n\n\nfollows because \\sigma_{i} = - \\sigma_{i} by the definition of Rademacher variable.\n\n\nTherefore we have reached our conclusion\n\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right]\n\\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#rademacher-based-uniform-convergence",
    "href": "Learning Theory/7_Rademacher_Complexity.html#rademacher-based-uniform-convergence",
    "title": "33  Rademacher Complexity",
    "section": "Rademacher-based uniform convergence",
    "text": "Rademacher-based uniform convergence\n\nTheorem 33.2 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}}, if the function class \\mathcal{F} only contains the functions f such that f (x) \\in [a, a + 1], then for every f \\in \\mathcal{F}, the difference between its true expectation and estimated expectation is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall f \\in \\mathcal{F}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{F}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nGiven a function f \\in \\mathcal{F}, the difference between its true expectation and estimated expectation on a sample \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} is less than the maximum difference of the expectations among all functions in \\mathcal{F}, which is denoted by \\phi (\\mathcal{S})\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f) \\rvert  \\leq \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}} (\\hat{f}) \\rvert] = \\phi (\\mathcal{S}).\n\nFirst we will prove the following property so that we can use McDiarmid’s inequality on \\phi (\\mathcal{S})\n\n\\sup_{\\mathcal{S}, \\mathcal{S}'} \\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert \\leq \\frac{ 1 }{ n }\n\nwhere \\mathcal{S}' = \\{ z_{1}, \\dots, z_{j}', \\dots, z_{n} \\} has z_{j}' different from z_{j} in \\mathcal{S}.\nLet f^{*} \\in \\mathcal{F} be the function that maximizes \\phi (\\mathcal{S})\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}} (f^{*}) \\rvert\n= \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}} (\\hat{f}) \\rvert]\n= \\phi (\\mathcal{S})\n\nand by definition\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}'} (f^{*}) \\rvert\n\\leq \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}'} (\\hat{f}) \\rvert]\n= \\phi (\\mathcal{S}').\n\nTherefore,\n\n\\begin{aligned}\n\\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert\n& \\leq \\lvert \\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}} (f^{*}) \\rvert\n- \\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}'} (f^{*}) \\rvert \\rvert\n\\\\\n& = \\lvert E_{\\mathcal{S}'} (f^{*}) - E_{\\mathcal{S}} (f^{*}) \\rvert\n\\\\\n& = \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}) - \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}')\n\\right\\rvert.\n\\end{aligned}\n\nSince \\mathcal{S} and \\mathcal{S}' only differ on two elements z_{j}, z_{j}', this becomes\n\n\\begin{aligned}\n\\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert\n& \\leq \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}) - \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}')\n\\right\\rvert\n\\\\\n& = \\frac{ 1 }{ n } \\left\\lvert \\left(\n        \\sum_{i \\neq j} f^{*} (z_{i}) + f^{*} (z_{j})\n    \\right) - \\left(\n        \\sum_{i \\neq j} f^{*} (z_{i}) + f^{*} (z_{j}')\n    \\right)\n\\right\\rvert\n\\\\\n& = \\frac{ 1 }{ n } \\left\\lvert f^{*} (z_{j}) - f^{*} (z_{j}') \\right\\rvert\n\\\\\n& \\leq \\frac{ 1 }{ n }.\n\\end{aligned}\n\nThis results show that the function \\phi follows the bounded difference property, so we can apply the McDiarmid’s inequality on \\phi,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (\\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\geq t)\n& \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} \\left(\n        \\frac{ 1 }{ n }\n    \\right)^{2} }\n\\right]\n\\\\\n& \\leq e^{- 2 m t^{2}}.\n\\end{aligned}\n\nBy setting \\delta = e^{-2 n t^{2}}, we can derive that t = \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}, so\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} \\left(\n    \\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\geq\n    \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\right)\n& \\leq \\delta\n\\\\\n\\mathbb{P}_{\\mathcal{S}} \\left(\n    \\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\leq\n    \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\right)\n& \\geq 1 - \\delta.\n\\end{aligned}\n\nwhich means we have the following fact with the probability larger than 1 - \\delta,\n\n\\phi (\\mathcal{S}) \\leq \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}.\n\nBy plugging back the result to the equation that we want to prove, we have the final results\n\n\\begin{aligned}\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f)] \\rvert\n& \\leq \\phi (\\mathcal{S})\n\\\\\n& \\leq \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\\\\n& \\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\end{aligned}\n\nwith the probability larger than 1 - \\delta.\n\n\n\n\nResults for risks\nGiven a hypothesis class \\mathcal{H}, a corresponding loss class with the 0-1 loss can be defined as\n\n\\mathcal{L} = \\{ l_{h} \\mid l_{h} (z) = L (h (\\mathbf{x}), y), h \\in \\mathcal{H}, z \\sim \\mathcal{Z} \\}\n\nand therefore the empirical risk and true risk can be defined as\n\nR_{\\mathcal{S}} (h) = E_{\\mathcal{S}} (l_{h}), R (h) = \\mathbb{E}_{\\mathcal{Z}} [l_{h} (z)].\n\nSince all the loss functions l_{h} \\in \\mathcal{L} have output range [0, 1], we can apply the uniform theorem above to the loss class \\mathcal{L} to derive the uniform convergence results for risks.\n\nCorollary 33.1 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}}, a hypothesis class \\mathcal{H}, and the corresponding 0-1 loss class \\mathcal{L}, for every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall h \\in \\mathcal{H}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{L}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}.\n\n\nBy using the following lemma, we can write the results in terms of the Rademacher complexity the hypothesis class \\mathcal{H} instead of the loss class \\mathcal{L}.\n\nLemma 33.1 Given a hypothesis class \\mathcal{H} and the corresponding loss class \\mathcal{L}, we have\n\n\\mathrm{Rad}_{n} (\\mathcal{H}) = 2 \\mathrm{Rad}_{n} (\\mathcal{L}).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy the definition of Rademacher complexity and 0-1 loss, we have\n$$\n\\begin{aligned}\n\\mathrm{Rad}_{S} (\\mathcal{H})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} \\mathbb{1} (h(x_{i}) \\neq y_{i})\n    \n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} \\left(\n        \\frac{ 1 }{ 2 } - y_{i} h (x_{i})\n    \\right)\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\left[\n        \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i}\n        + \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} (-y_{i} h (x_{i}))\n    \\right]\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i}\n    + \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} (- y_{i} h (x_{i}))\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} h (x_{i})\n\\right]\n& \\quad [\\mathbb{E}\\left[\\sum_{i=1}^{m}\\sigma_{i}\\right] = 0]\n\\\\\n& = \\frac{ 1 }{ 2 }\\text{Rad}_{S}(\\mathcal{H}).\n\\end{aligned}\n$$\n\n\n\n\nCorollary 33.2 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}} and a hypothesis class \\mathcal{H}, for every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall h \\in \\mathcal{H}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{H}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}."
  },
  {
    "objectID": "Learning Theory/7_Rademacher_Complexity.html#bounding-rademacher-complexity",
    "href": "Learning Theory/7_Rademacher_Complexity.html#bounding-rademacher-complexity",
    "title": "33  Rademacher Complexity",
    "section": "Bounding Rademacher complexity",
    "text": "Bounding Rademacher complexity\nThe Rademacher complexity can be upper bounded for any function class with a finite VC dimension.\n\nMassart’s lemma\n\nLemma 33.2 (Massart’s lemma) Given any set of vectors \\mathcal{A} \\subseteq \\mathbb{R}^{n} the empirical Rademacher average is upper-bounded\n\n\\mathrm{Rad} (\\mathcal{A}) \\leq \\frac{ R \\sqrt{2 \\log \\lvert \\mathcal{A} \\rvert} }{ n }\n\nwhere R = \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\lVert \\mathbf{a} \\rVert_{2}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy Jensen’s inequality, the following quantity can be upper-bounded\n\n\\begin{aligned}\n\\exp \\left[\n    s \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n        s \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& \\stackrel{(1)}{\\leq} \\mathbb{E}_{\\sigma} \\left[\n    \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n        s \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& = \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\mathbb{E}_{\\sigma} \\left[\n    \\prod_{i = 1}^{n} \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& \\stackrel{(2)}{=} \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\end{aligned}\n\nExplanations in the derivations\n\n\nfollows since \\exp is non-negative and therefore \\sup \\leq \\sum.\n\n\nfollows because of the independence between \\sigma_{i}.\n\n\nSince \\mathbb{E}_{\\sigma_{i} a_{i}} = 0, we can apply Hoeffding’s lemma with \\mu = 0\n\n\\begin{aligned}\n\\exp \\left[\n    s \\sigma_{i} a_{i}\n\\right]\n& \\leq \\exp \\left[\n    \\frac{ s^{2} (2 a_{i})^{2}}{ 8 }\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ s^{2} a_{i}^{2} }{ 2 }\n\\right],\n\\end{aligned}\n\nand therefore\n\n\\begin{aligned}\n\\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\exp \\left[\n    \\frac{ s^{2} a_{i}^{2} }{ 2 }\n\\right]\n\\\\\n& = \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sum_{i = 1}^{n} a_{i}^{2}\n\\right]\n\\\\\n& \\leq \\lvert \\mathcal{A} \\rvert \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}\n\\right].\n\\end{aligned}\n\nwhere the last inequality follows because \\sum_{i = 1}^{n} f (a_{i}) \\leq n \\sup_{a_{i}} f (a_{i}), \\forall f.\nCombining all pieces together,\n\n\\begin{aligned}\n\\exp \\left[\n    s \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\lvert \\mathcal{A} \\rvert \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}\n\\right]\n\\\\\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n\\end{aligned}\n\nwhere R^{2} = \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}.\nSince \\log \\frac{ \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 } is a convex function, we can minimize it with respect to s\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n& = 0\n\\\\\n- \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s^{2} } + \\frac{ R^{2} }{ 2 }\n& = 0\n\\\\\ns\n& = \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R }.\n\\end{aligned}\n\nPlugging it back\n\n\\begin{aligned}\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n\\\\\n& = \\frac{\n    \\log \\lvert \\mathcal{A} \\rvert\n}{\n    \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R }\n} + \\frac{\n    \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R } R^{2}\n}{\n    2\n}\n\\\\\n& = R \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert }\n\\\\\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ R \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ n },\n\\end{aligned}\n\nwhere the last equation is derived by dividing both sides by n.\n\n\n\n\n\nConnection with VC theory\n\nLemma 33.3 The Rademacher complexity of the hypothesis class \\mathcal{H} with the finite VC dimension d is upper-bounded\n\n\\mathrm{Rad}_{n} (\\mathcal{H})\n\\leq \\sqrt{\\frac{ 2 d \\log \\frac{ e n }{ d } }{ n }}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf \\mathcal{H} has a finite VC dimension, its projection to any sample \\mathcal{S} with size n is finite. Since each \\mathcal{H} (\\mathcal{S}) can be seen as a set of vectors of length n, we can replace the set of vectors \\mathcal{A} in Lemma 33.2 with \\mathcal{H} (\\mathcal{S}).\nSince h^{2} (z_{i}) = 1, \\forall z_{i}, \\forall h \\in \\mathcal{H},\n\nR = \\sup_{h \\in \\mathcal{H}} \\sqrt{\\sum_{i = 1}^{n} h^{2} (z_{i})} = \\sqrt{n}.\n\nso we have\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{H} (\\mathcal{S}))\n\\leq \\frac{ \\sqrt{n} \\sqrt{2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert} }{ n }\n= \\sqrt{\\frac{ 2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert }{ n }}.\n\nBy the definition of growth function and Sauer’s lemma \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert \\leq \\Pi_{\\mathcal{H}} (\\mathcal{S}) \\leq \\left( \\frac{ e }{ d } n \\right)^{d}, where d is the VC dimension of \\mathcal{H}, we can derive\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{H})\n\\leq \\sqrt{\\frac{ 2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert }{ n }}\n\\leq \\sqrt{\\frac{ 2 \\log \\Pi_{\\mathcal{H}} (\\mathcal{n}) }{ n }}\n\\leq \\sqrt{\\frac{ 2 d \\log \\frac{ e n }{ d } }{ n }}."
  },
  {
    "objectID": "Machine Learning/1_Linear_Discriminant.html#preliminary",
    "href": "Machine Learning/1_Linear_Discriminant.html#preliminary",
    "title": "34  Linear Discriminant",
    "section": "Preliminary",
    "text": "Preliminary\n\nLinear Algebra\n\nAffine space Section 11.4"
  },
  {
    "objectID": "Machine Learning/1_Linear_Discriminant.html#hyperplanes",
    "href": "Machine Learning/1_Linear_Discriminant.html#hyperplanes",
    "title": "34  Linear Discriminant",
    "section": "Hyperplanes",
    "text": "Hyperplanes\nRecall that a hyperplane \\mathcal{H} in \\mathbb{R}^{d} is an affine space that is expressed as\n\n\\mathcal{H} = \\left\\{ \\mathbf{x} \\mid \\mathbf{w} \\mathbf{x} + b = 0 \\right\\},\n\nwhich can be viewed as the subspace \\mathbf{w}^{\\perp} translated by the vector \\mathbf{x}_{0}\n\n\\mathcal{H} = \\left\\{ \\mathbf{u} + \\mathbf{x}_{0} \\mid \\mathbf{u} \\in \\mathbf{w}^{\\perp} \\right\\}.\n\nwhere\n\n\\mathbf{w}^{\\perp} is the subspace that is perpendicular to the vector \\mathbf{w},\n\\mathbf{v} = - \\frac{b}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}.\n\nAlso recall that given a vector \\mathbf{x} \\in \\mathbb{R}^{d}, its orthogonal projection onto \\mathcal{H} is\n\n\\mathbf{p} = \\mathbf{x} -\n    \\frac{\n        \\mathbf{w}^{T} \\mathbf{x} + b\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\mathbf{w}.\n\nTherefore, the distance between \\mathbf{x} and \\mathcal{H} is the length the vector that connects \\mathbf{p} and \\mathbf{x}\n\n\\begin{aligned}\n\\lVert \\mathbf{x} - \\mathbf{p} \\rVert\n& = \\left\\lVert \\frac{\n    \\mathbf{w}^{T} \\mathbf{x} + b\n}{\n    \\mathbf{w}^{T} \\mathbf{w}\n} \\mathbf{w} \\right\\rVert\n\\\\\n& = \\left\\lvert\n    \\frac{\n        \\mathbf{w}^{T} \\mathbf{x} + b\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\right\\rvert \\lVert \\mathbf{w} \\rVert\n\\\\\n& = \\frac{\n    \\lvert \\mathbf{w}^{T} \\mathbf{x} + b \\rvert\n}{\n    \\lVert \\mathbf{w} \\rVert^{2}\n}\n\\lVert \\mathbf{w} \\rVert\n\\\\\n& = \\frac{\n    \\lvert \\mathbf{w}^{T} \\mathbf{x} + b \\rvert\n}{\n    \\lVert \\mathbf{w} \\rVert\n}.\n\\end{aligned}\n\n\nHyperplanes as linear discriminants\nThe linear function defined by\n\nf (\\mathbf{x}) = \\mathbf{w} \\mathbf{x} + b\n\ndivides the points in \\mathbb{R}^{d} into 3 spaces\n\nf (\\mathbf{x}) = 0: the points on the hyperplane \\mathcal{H}.\nf (\\mathbf{x}) &gt; 0: the points on the positive side of f (\\mathbf{x}), which is the side that \\mathbf{w} points to.\nf (\\mathbf{x}) &lt; 0: the points on the negative side of f (\\mathbf{x}).\n\nTherefore, f (\\mathbf{x}) is a linear discriminant if we classify the points in \\mathbb{R}^{d} based on the following decision rule\n\ng (\\mathbf{x}) = \\text{sign} (f (\\mathbf{x})) = \\begin{cases}\n1 & f (\\mathbf{x}) &gt; 0 \\\\\n0 & f (\\mathbf{x}) &lt; 0 \\\\\n\\end{cases}"
  },
  {
    "objectID": "Machine Learning/1_Linear_Discriminant.html#margin",
    "href": "Machine Learning/1_Linear_Discriminant.html#margin",
    "title": "34  Linear Discriminant",
    "section": "Margin",
    "text": "Margin\nThe margin of the instance \\mathbf{x} is its signed distance from the hyperplane f\n\n\\gamma (\\mathbf{x}) = \\frac{\\hat{y} f (\\mathbf{x})}{\\lVert \\mathbf{w} \\rVert},\n\nwhere \\hat{y} \\in \\{1, -1\\} is the label of \\mathbf{x}, but the negative label is denoted by -1\nThe margin of a hyperplane f is the distance from the hyperplane to the closest point in the training set\n\n\\gamma = \\min_{i} \\frac{\n    \\lvert \\gamma (\\mathbf{x}) \\rvert\n}{\n    \\lVert \\mathbf{w} \\rVert\n} = \\min_{i} \\frac{\n    \\lvert f (\\mathbf{x}) \\rvert\n}{\n    \\lVert \\mathbf{w} \\rVert\n}.\n\nA linear discriminant with a positive margin classifies all training instances correctly\n\n\\gamma &gt; 0 \\Leftrightarrow y_i (\\mathbf{w} \\mathbf{x}_i + b) &gt; 0, \\forall i.\n\n\nMargin loss\nThe margin loss is a loss function that is defined with respect to the margin of the instances\n\nL (y, g (\\mathbf{x})) = \\phi (\\gamma (\\mathbf{x}))."
  },
  {
    "objectID": "Machine Learning/2_Perceptron.html#preliminary",
    "href": "Machine Learning/2_Perceptron.html#preliminary",
    "title": "35  Perceptron",
    "section": "Preliminary",
    "text": "Preliminary\n\nSupervised Learning\n\nLinear Discriminant"
  },
  {
    "objectID": "Machine Learning/2_Perceptron.html#the-perceptron-algorithm",
    "href": "Machine Learning/2_Perceptron.html#the-perceptron-algorithm",
    "title": "35  Perceptron",
    "section": "The Perceptron algorithm",
    "text": "The Perceptron algorithm\nThe perceptron algorithm is the first machine learning algorithm that learns a linear discriminant f (\\mathbf{x}) from a training set using gradient descent, which does binary classification using the following decision rule\n\ng (\\mathbf{x}) = \\text{sign} (f (\\mathbf{x})) = \\begin{cases}\n1 & f (\\mathbf{x}) &gt; 0 \\\\\n0 & f (\\mathbf{x}) &lt; 0 \\\\\n\\end{cases}.\n\nThe loss function for the Perceptron algorithm is\n\nL(f (\\mathbf{x}), y) = \\max (0, - y f (\\mathbf{x})) =\n\\begin{cases}\nf (\\mathbf{x}) & \\text{sign} (f (\\mathbf{x})) \\neq y \\\\\n0 & \\text {sign} (f (\\mathbf{x})) = y, \\\\\n\\end{cases}\n\nwhich is called the Perceptron loss."
  },
  {
    "objectID": "Machine Learning/2_Perceptron.html#convergence-analysis",
    "href": "Machine Learning/2_Perceptron.html#convergence-analysis",
    "title": "35  Perceptron",
    "section": "Convergence analysis",
    "text": "Convergence analysis\nTODO: why (\\frac{2R}{\\gamma})^2 instead of (\\frac{R}{\\gamma})^2\nhttps://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote03.html\nhttp://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf"
  },
  {
    "objectID": "Machine Learning/3_Logistic_Regression.html#preliminary",
    "href": "Machine Learning/3_Logistic_Regression.html#preliminary",
    "title": "36  Logistic Regression",
    "section": "Preliminary",
    "text": "Preliminary\n\nStatistical Learning\n\nBayesian Decision Theory (BDT)\nMaximum Likelihood Estimation (MLE)\n\n\n\nSupervised Learning\n\nLinear Discriminant"
  },
  {
    "objectID": "Machine Learning/3_Logistic_Regression.html#logistic-regression-as-a-gaussian-classifier",
    "href": "Machine Learning/3_Logistic_Regression.html#logistic-regression-as-a-gaussian-classifier",
    "title": "36  Logistic Regression",
    "section": "Logistic regression as a Gaussian classifier",
    "text": "Logistic regression as a Gaussian classifier\nLogistic regression is a classification model that models the posterior probability of the positive class and assigns labels based on the MAP rule\n\ny = \\begin{cases}\n1 & \\sigma (f (\\mathbf{x})) \\geq 0.5 \\\\\n0 & \\sigma (f (\\mathbf{x}))\n&lt; 0.5 \\\\\n\\end{cases},\n\nwhere \\sigma is the sigmoid function and f (\\mathbf{x}) is a linear function on the instance \\mathbf{x}.\n\nMAP rule and posterior probability\nRecall that the BDR with 0-1 loss is the MAP rule\n\nf (\\mathbf{x}) = \\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x})\n\nwhere \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x}) is the posterior probability that the true class for instance \\mathbf{x} is y.\nFor a binary classification problem, the MAP rule can be simplified to select the class 1 for \\mathbf{x} if\n\n\\begin{aligned}\n\\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n& \\geq \\mathbb{P}_{Y \\mid \\mathbf{X}} (0 \\mid \\mathbf{x})\n\\\\\n& \\geq 1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n\\\\\n& \\geq 0.5.\n\\end{aligned}\n\nUsing the Bayes theorem, the posterior probability of the positive class can be represented using the class conditional probabilities \\mathbb{P}_{\\mathbf{X} \\mid Y} and class probabilities \\mathbb{P}_{Y}\n\n\\begin{aligned}\n\\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n}{\n    \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x})\n}\n& [\\text{Bayes' theroem}]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n}{\n    \\mathbb{P}_{\\mathbf{X}, Y} (\\mathbf{x}, 0) + \\mathbb{P}_{\\mathbf{X}, Y} (\\mathbf{x}, 1)\n}\n& [\\text{Law of total probability}]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n}{\n    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 0) \\mathbb{P}_{Y} (0) + \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n}\n& [\\text{Chain rule}]\n\\\\\n& = \\left(1 + \\frac{\n        \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 0) \\mathbb{P}_{Y} (0)\n    }{\n        \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n    }\n\\right)^{-1}\n\\\\\n\\end{aligned}\n\n\n\nSigmoid function\nThe sigmoid function is a saturating function that maps the real number z into a number that ranges from 0 to 1\n\n\\sigma (z) = \\frac{ 1 }{ 1 + e^{- z}\n}.\n\nThe posterior probability is the result of the sigmoid function if we assume the class conditional probabilities are Gaussian distributions.\nRecall that the multivariate Gaussian with the mean \\boldsymbol{\\mu} and covariance matrix \\boldsymbol{\\Sigma} is\n\n\\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{\n    1\n}{\n    \\sqrt{(2 \\pi)^{2} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert}\n} \\exp \\left(\n    -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_{1})^T \\boldsymbol{\\Sigma}_{1}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_{1}})\n\\right),\n\nwhich can be compactly written as follows\n\n\\begin{aligned}\n\\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n& = \\frac{\n    1\n}{\n    \\sqrt{(2 \\pi)^{2} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert}\n} \\exp \\left(\n    -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_{1})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_{1}})\n\\right)\n\\\\\n& = \\exp \\left(\n    \\log \\left(\n        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma} \\rvert\n    \\right)^{-\\frac{1}{2}} - \\frac{1}{2} \\left(\n        \\mathbf{x} - \\boldsymbol{\\mu}\n    \\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left(\n        \\mathbf{x} - \\boldsymbol{\\mu}\n    \\right)\n\\right)\n\\\\\n& = \\exp \\left(\n    -\\frac{1}{2} \\log \\left(\n        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma} \\rvert\n    \\right) - \\frac{1}{2} \\left(\n        \\mathbf{x} - \\boldsymbol{\\mu}\n    \\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left(\n        \\mathbf{x} - \\boldsymbol{\\mu}\n    \\right)\n\\right)\n\\\\\n& = \\exp \\left(\n    -\\frac{1}{2} \\left(\n        \\log \\left(\n            (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma} \\rvert\n        \\right) + d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\boldsymbol{\\mu})\n    \\right)\n\\right),\n\\end{aligned}\n\nwhere d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\mathbf{y}) = \\frac{1}{2} \\left(  \\mathbf{x} - \\mathbf{y} \\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left(  \\mathbf{x} - \\mathbf{y} \\right) is the Mahalanobis distance between \\mathbf{x} and \\mathbf{y} with covariance matrix \\boldsymbol{\\Sigma}.\nIf we assume the class conditional probabilities for both classes are Gaussian distributions:\n\n\\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 0) = \\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}_{0}, \\boldsymbol{\\Sigma}_{0})\n\\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) = \\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}_{1}),\n\nthen the posterior possibility is\n\n\\begin{aligned}\n\\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n& = \\left(\n    1 + \\frac{\n        \\exp \\left(\n            -\\frac{1}{2} \\left(\n                \\log \\left(\n                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{0} \\rvert\n                \\right) + d_{\\boldsymbol{\\Sigma_{0}}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}})\n            \\right) \\mathbb{P}_{Y} (0)\n        \\right)\n    }{\n        \\exp \\left(\n            -\\frac{1}{2} \\left(\n                \\log \\left(\n                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert\n                \\right) + d_{\\boldsymbol{\\Sigma_{1}}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n            \\right) \\mathbb{P}_{Y} (1)\n        \\right)\n    }\n\\right)^{-1}\n\\\\\n& = \\left(\n    1 + \\frac{\n        \\exp \\left(\n            -\\frac{1}{2} \\left(\n                \\log \\left(\n                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{0} \\rvert\n                \\right) + d_{\\boldsymbol{\\Sigma_{0}}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}})\n            \\right) + \\log \\mathbb{P}_{Y} (0)\n        \\right)\n    }{\n        \\exp \\left(\n            -\\frac{1}{2} \\left(\n                \\log \\left(\n                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert\n                \\right) + d_{\\boldsymbol{\\Sigma_{1}}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n            \\right) + \\log \\mathbb{P}_{Y} (1)\n        \\right)\n    }\n\\right)^{-1}\n\\\\\n& = \\left(\n    1 + \\exp \\left(\n        - f (\\mathbf{x})\n    \\right)\n\\right)^{-1}\n\\\\\n& = \\sigma (f (\\mathbf{x}))\n\\end{aligned}\n\nwhere $f () = ( {0} - {1} + d_{} (, ) - d_{} (, ) + 2 ) $ and \\alpha_{i} = \\log \\left(  (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{i} \\rvert \\right).\n\n\nLinear function\nIf we further assume that the Gaussian distributions for both classes have the same covariance matrix \\boldsymbol{\\Sigma}_{0} = \\boldsymbol{\\Sigma}_{1} = \\boldsymbol{\\Sigma}, then f (\\mathbf{x}) is a linear function\n\n\\begin{aligned}\nf (\\mathbf{x})\n& = \\frac{1}{2} \\left(\n    \\alpha - \\alpha\n    + d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}})\n    - d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n    + 2 \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n\\right)\n\\\\\n& = \\frac{1}{2} \\left(\n    \\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} +\n    2\\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} +\n    \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} -\n    \\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} -\n    2\\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1} -\n    \\boldsymbol{\\mu}_{1}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}\n\\right) + \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n\\\\\n& = \\left(\n    \\boldsymbol{\\mu}_{0} - \\boldsymbol{\\mu}_{1}\n\\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} + \\frac{1}{2} \\left(\n    \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} -\n    \\boldsymbol{\\mu}_{1}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}\n\\right) + \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n\\\\\n& = \\mathbf{w}^{T} \\mathbf{x} + b\n\\end{aligned}\n\nwhere the weights \\mathbf{w} are\n\n\\mathbf{w}^{T} = \\left(\n    \\boldsymbol{\\mu}_{0} - \\boldsymbol{\\mu}_{1}\n\\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x},\n\nand the bias term is\n\nb = \\frac{1}{2} \\left(\n    \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} -\n    \\boldsymbol{\\mu}_{1}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}\n\\right) + \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}."
  },
  {
    "objectID": "Machine Learning/3_Logistic_Regression.html#learning-of-logistic-regression",
    "href": "Machine Learning/3_Logistic_Regression.html#learning-of-logistic-regression",
    "title": "36  Logistic Regression",
    "section": "Learning of logistic regression",
    "text": "Learning of logistic regression\nWith the generative approach, parameters \\boldsymbol{\\mu}_{0}, \\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}_{0}, and \\boldsymbol{\\Sigma}_{1} are learned from the training set using MLE. In particular, the parameters for the conditional probability of class j are learned by solving the following optimization problem\n\n\\arg\\max_{\\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}} \\prod_{y_{j} = j} \\mathbb{P}_{\\mathbf{X} \\mid Y} \\left(\n    \\mathbf{x}_{i} \\mid j\n\\right) = \\arg\\max_{\\boldsymbol{\\mu}_{j}, \\boldsymbol{\\Sigma}_{j}} \\prod_{y_{j} = j} \\mathcal{G} \\left(\n    \\mathbf{x}_{i}; \\boldsymbol{\\mu}_{j}, \\boldsymbol{\\Sigma}_{j}\n\\right).\n\nHowever, logistic regression is usually learned using a discriminative approach, where the parameters \\mathbf{w}, b are directly learned from the data by minimizing binary cross-entropy loss.\n\nLearning as a MLE problem\nRecall that the learning of the linear regression can be formulated as an MLE problem\n\n\\arg\\max_{\\mathbf{w}, b} \\prod_{i} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y_{i} \\mid \\mathbf{x}_{i}\n\\right) = \\arg\\max_{\\mathbf{w}, b} \\prod_{i} \\mathcal{G} \\left(\n    y_{i}; \\mathbf{w}^{T} \\mathbf{x}_{i} + b, \\sigma^{2}\n\\right),\n\nwhere the posterior probability of the label \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(  y_{i} \\mid \\mathbf{x}_{i} \\right) follows a univariate Gaussian distribution with the mean \\mathbf{w}^{T} \\mathbf{x} + b and a known variance \\sigma^{2}.\nFor logistic regression, the posterior probability of the label should be a Bernoulli distribution\n\n\\begin{aligned}\n\\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y \\mid \\mathbf{x}\n\\right)\n& = \\mathrm{Ber} \\left(\n    y; \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n\\right)\n\\\\\n& = \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})^{y} \\left(\n    1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n\\right)^{(1 - y)}\n\\end{aligned}\n\nand therefore the MLE problem is defined as\n\n\\begin{aligned}\n\\arg\\max_{\\mathbf{w}, b} \\prod_{i} \\mathrm{Ber} \\left(\n    y; \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n\\right)\n& = \\arg\\max_{\\mathbf{w}, b} \\sum_{i} \\log \\mathrm{Ber} \\left(\n    y; \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n\\right)\n\\\\\n& = \\arg\\max_{\\mathbf{w}, b} \\sum_{i} \\log \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})^{y_{i}} \\left(\n    1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n\\right)^{(1 - y_{i})}.\n\\end{aligned}\n\n\n\nBinary cross-entropy (BCE) loss\nThe binary cross-entropy loss is defined as\n\nL_{\\mathrm{BCE}} (y, \\hat{y}) =  - y \\log \\hat{y} - (1 - y) \\log (1 - \\hat{y})\n\nwhere y \\in \\{0, 1\\} is the binary label and \\hat{y} \\in [0, 1] is the probability of the positive class.\nSolving the MLE of parameters of the logistic regression problem is the same as minimizing the BCE loss\n\n\\begin{aligned}\n& \\arg\\max_{\\mathbf{w}, b} \\sum_{i} \\log \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})^{y_{i}} \\left(\n    1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n\\right)^{(1 - y_{i})}\n\\\\\n= & \\arg\\max_{\\mathbf{w}, b} \\sum_{i} y_{i} \\log \\sigma (f (\\mathbf{x}_{i})) + (1 - y_{i}) \\log \\left(\n    1 - \\sigma (f (\\mathbf{x}_{i}))\n\\right)\n\\\\\n= & \\arg\\min_{\\mathbf{w}, b} \\sum_{i} - y_{i} \\log \\sigma (f (\\mathbf{x}_{i})) - (1 - y_{i}) \\log \\left(\n    1 - \\sigma (f (\\mathbf{x}_{i}))\n\\right)\n\\\\\n= & \\arg\\min_{\\mathbf{w}, b} \\sum_{i} L_{\\mathrm{BCE}} (y_{i}, \\sigma (f (\\mathbf{x}_{i})).\n\\end{aligned}\n\nTherefore, logistic regression can be learned by minimizing the BCE loss between the predicted labels and training labels.\n\n\nMinimizing loss with gradient descent\nUnlike linear regression, the optimization problem of logistic regression\n\n\\arg\\min_{\\mathbf{w}, b} \\sum_{i} - y_{i} \\log \\sigma (f (\\mathbf{x}_{i})) - (1 - y_{i}) \\log \\left(\n    1 - \\sigma (f (\\mathbf{x}_{i}))\n\\right)\n\ncan not be analytically solved to obtain a closed-form solution because of the non-linear sigmoid function. Instead, gradient descent is used to solve the optimization problem numerically.\nTo simplify the derivation process, we will use a trick to make the bias term b as part of the weight vector\n\nf (\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} + b = \\hat{\\mathbf{w}}^{T} \\hat{\\mathbf{x}}.\n\nwhere \\hat{\\mathbf{w}} is defined by appending b to \\mathbf{w} as the last element\n\n\\hat{\\mathbf{w}} =\n\\begin{bmatrix}\nw_{1} \\\\\n\\vdots \\\\\nw_{d} \\\\\nb \\\\\n\\end{bmatrix},\n\nand \\hat{\\mathbf{x}} is formed by appending an 1 to \\mathbf{x} as the last element\n\n\\mathbf{\\hat{x}} =\n\\begin{bmatrix}\nx_{1} \\\\\n\\vdots \\\\\nx_{d} \\\\\n1 \\\\\n\\end{bmatrix}.\n\nFirst note that the function J (\\hat{\\mathbf{w}}) that we want to minimize\n\nJ (\\hat{\\mathbf{w}})\n= \\sum_{i} L_{\\mathrm{BCE}} (y_{i}, \\sigma (f (\\hat{\\mathbf{x}}_{i}))\n= \\sum_{i} - y_{i} \\log \\sigma (f (\\hat{\\mathbf{x}}_{i})) - (1 - y_{i}) \\log \\left(\n    1 - \\sigma (f (\\hat{\\mathbf{x}}_{i}))\n\\right)\n\nis a convex function with respect to \\hat{\\mathbf{w}} and thus the gradient descent can find the global minimum of J (\\hat{\\mathbf{w}}).\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO\n\n\n\nRecall that the gradient descent is to update the parameters \\mathbf{x} with negative gradient - \\nabla f (\\mathbf{x}) scaled by the learning rate \\eta\n\n\\mathbf{x} = \\mathbf{x} - \\eta \\nabla f (\\mathbf{x}).\n\nTo apply it to minimize the optimization function J (\\hat{\\mathbf{w}}), we will first select a learning rate \\eta and then learn \\hat{\\mathbf{w}} by doing\n\n\\begin{aligned}\n\\hat{\\mathbf{w}}\n& = \\hat{\\mathbf{w}} - \\eta \\nabla J (\\hat{\\mathbf{w}})\n\\\\\n& = \\hat{\\mathbf{w}} - \\eta \\sum_{i} (\\sigma (\\hat{\\mathbf{w}}^{T} \\hat{\\mathbf{x}}_{i}) - y_{i}) \\hat{\\mathbf{x}}_{i}\n\\end{aligned}\n\nin each iteration until convergence.\n\n\n\n\n\n\nProof\n\n\n\n\n\nGiven \\hat{\\mathbf{x}}_{i}, y_{i}, \\forall i \\in [1, n], the function J (\\hat{\\mathbf{w}}) can be expressed as a composite function of L_{\\mathrm{BCE}} (\\hat{y}), \\sigma (z), f (\\hat{\\mathbf{w}})\n\nJ (\\hat{\\mathbf{w}}) = \\sum_{i} L_{\\mathrm{BCE}} (\\sigma (f (\\hat{\\mathbf{w}}))),\n\nso \\nabla J (\\hat{\\mathbf{w}}) can be computed using the chain rule\n\n\\nabla J (\\hat{\\mathbf{w}}) = \\sum_{i} \\frac{\n    \\mathop{d} L_{\\mathrm{BCE}} (\\hat{y})\n}{\n    \\mathop{d} \\hat{y}\n} \\frac{\n    \\mathop{d} \\sigma (z)\n}{\n    \\mathop{d} z\n} \\nabla f (\\hat{\\mathbf{w}}).\n\nWe can calculate each component\n\n\\begin{aligned}\n\\frac{\n    \\mathop{d} L_{\\mathrm{BCE}} (\\hat{y})\n}{\n    \\mathop{d} \\hat{y}\n}\n& = \\frac{ \\mathop{d} }{ \\mathop{d} \\hat{y} } - y \\log \\hat{y} - (1 - y) \\log (1 - \\hat{y})\n\\\\\n& = - \\frac{ y }{ \\hat{y} } + \\frac{ 1 - y }{ 1 - \\hat{y} },\n\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{ \\mathop{d} \\sigma }{ \\mathop{d} z } (z)\n& = \\frac{ \\mathop{d} }{ \\mathop{d} z } \\frac{ 1 }{ 1 + e^{-z} }\n\\\\\n& = -(1 + e^{-z})^{-2} (- e^{-z})\n\\\\\n& = \\frac{e^{-z}}{(1 + e^{-z})^{2}}\n\\\\\n& = \\frac{ e^{-z} }{ 1 + e^{-z} } \\frac{ 1 }{ 1 + e^{-z} }\n\\\\\n& = \\frac{ e^{-z} }{ 1 + e^{-z} } \\left(\n    \\frac{ 1 + e^{-z} }{ 1 + e^{-z} } - \\frac{ e^{-z} }{ 1 + e^{-z} }\n\\right)\n\\\\\n& = \\frac{ e^{-z} }{ 1 + e^{-z} } \\left(\n    1 - \\frac{e^{-x}}{1 + e^{-x}}\n\\right)\n\\\\\n& = \\sigma (z) (1 - \\sigma (z)),\n\\\\\n\\end{aligned}\n\n\n\\nabla f (\\hat{\\mathbf{w}}) = \\nabla \\hat{\\mathbf{w}}^{T} \\hat{\\mathbf{x}} = \\hat{\\mathbf{x}},\n\nand put them together\n\n\\begin{aligned}\n\\nabla J (\\hat{\\mathbf{w}})\n& = \\sum_{i} \\frac{\n    \\mathop{d} L_{\\mathrm{BCE}} (\\hat{y})\n}{\n    \\mathop{d} \\hat{y}\n} \\frac{\n    \\mathop{d} \\sigma (z)\n}{\n    \\mathop{d} z\n} \\nabla f (\\hat{\\mathbf{w}})\n\\\\\n& = \\sum_{i} \\left(\n    - \\frac{ y_{i} }{ \\hat{y}_{i} } + \\frac{ 1 - y_{i} }{ 1 - \\hat{y}_{i} }\n\\right) \\sigma (z_{i}) (1 - \\sigma (z_{i})) \\hat{\\mathbf{x}}_{i}\n\\\\\n& = \\sum_{i} \\left(\n    - \\frac{ y_{i} }{ \\sigma (z_{i}) } + \\frac{ 1 - y_{i} }{ 1 - \\sigma (z_{i}) }\n\\right) \\sigma (z_{i}) (1 - \\sigma (z_{i})) \\hat{\\mathbf{x}}_{i}\n\\\\\n& = \\sum_{i} (\\sigma (z_{i}) - y_{i}) \\hat{\\mathbf{x}}_{i}\n\\end{aligned}\n\nwhere z_{i} = \\hat{\\mathbf{w}}^{T} \\hat{\\mathbf{x}}_{i}.\n\n\n\nIf we further assume that the sigmoid function can be applied to a matrix in a element-wise style\n\n\\sigma (\\mathbf{A}) =\n\\begin{bmatrix}\n\\sigma (a_{1, 1}) & \\dots & \\sigma (a_{1, n}) \\\\\n\\vdots & & \\vdots \\\\\n\\sigma (a_{m, 1}) & \\dots & \\sigma (a_{m, n}) \\\\\n\\end{bmatrix},\n\nthe training instances are ordered in columns of \\mathbf{X} and the training labels are ordered in the vector \\mathbf{y},\n\n\\mathbf{X} =\n\\begin{bmatrix}\n| & & | \\\\\n\\hat{\\mathbf{x}}_{1} & \\dots & \\hat{\\mathbf{x}}_{n} \\\\\n| & & | \\\\\n\\end{bmatrix}, \\quad\n\\mathbf{y} =\n\\begin{bmatrix}\ny_{1} \\\\\n\\vdots \\\\\ny_{n} \\\\\n\\end{bmatrix},\n\nthen the gradient descent algorithm to learn logistic regression can be stated using the matrix form\n\n\n\n\n\n\n\nInitialize the weight parameter \\mathbf{w} randomly\nWhile J (\\hat{\\mathbf{w}}) is decreasing\n\nCalculate the gradient \\nabla J (\\hat{\\mathbf{w}})\n\n\\nabla J (\\hat{\\mathbf{w}}) ="
  },
  {
    "objectID": "Machine Learning/4_Multi_Layer_Perceptron.html#preliminary",
    "href": "Machine Learning/4_Multi_Layer_Perceptron.html#preliminary",
    "title": "37  Multi-layer Perceptron",
    "section": "Preliminary",
    "text": "Preliminary\n\nCalculus\n\nChain Rule ?sec-chain-rule\n\n\n\nSupervised Learning\n\nPerceptron ?sec-perceptron\nLogistic Regression ?sec-logistic-regression"
  },
  {
    "objectID": "Machine Learning/4_Multi_Layer_Perceptron.html#backpropagation",
    "href": "Machine Learning/4_Multi_Layer_Perceptron.html#backpropagation",
    "title": "37  Multi-layer Perceptron",
    "section": "Backpropagation",
    "text": "Backpropagation"
  },
  {
    "objectID": "Machine Learning/5_Boosting.html#preliminary",
    "href": "Machine Learning/5_Boosting.html#preliminary",
    "title": "38  Boosting",
    "section": "Preliminary",
    "text": "Preliminary\n\nSupervised Learning\n\nLinear Discriminant"
  },
  {
    "objectID": "Machine Learning/5_Boosting.html#boosting-for-learning-an-ensemble-learner",
    "href": "Machine Learning/5_Boosting.html#boosting-for-learning-an-ensemble-learner",
    "title": "38  Boosting",
    "section": "Boosting for learning an ensemble learner",
    "text": "Boosting for learning an ensemble learner\n\nEnsemble classifier\nBoosting is a framework to learn a function that is a weighted sum of an ensemble of some base functions\n\nf (\\mathbf{x}) = \\sum_{i} w_{i} h_{i} (\\mathbf{x}),\n\nwhich is also named generalized additive models (GAMs) in statistics.\n\n\nGradient descent in functional space\nInstead of minimizing the empirical risk function over the space of possible parameters, boosting minimizes the risk over the space of a set of functions \\mathcal{U}\n\nR [f] = \\frac{ 1 }{ n } \\sum_{i=1}^{n} L[f].\n\nLet f_{t + 1} denote the classifier learned at the iteration t + 1 by minimizing a differentiable empirical risk function R [f] using gradient descent.\n\n\\begin{aligned}\nf_{t + 1}\n& = f_{t} - \\eta_{t} \\nabla R [f_{t}]\n\\\\\n& = \\left(\n    f_{t - 1} - \\eta_{t - 1} \\nabla R [f_{t - 1}]\n\\right) - \\eta_{t} \\nabla R [f_{t}]\n\\\\\n& = f_{t - 1} - \\left(\n    \\eta_{t - 1} \\nabla R [f_{t - 1}] + \\eta_{t} \\nabla R [f_{t}]\n\\right)\n\\\\\n& = ...\n\\\\\n& = f_{1} - \\sum_{i}^{t} \\eta_{t} \\nabla R [f_{i}].\n\\end{aligned}\n\nIf f_{1} is initialized to be 0 and the step size \\eta_{i} is different in each iteration, then f_{t + 1} can be interpreted as an ensemble of all gradients as the classifiers and the step sizes as weights\n\n\\begin{aligned}\nf_{t + 1}\n& = \\sum_{i = 1}^{t} \\eta_{i} \\left(\n    - \\nabla R [f_{i}]\n\\right)\n\\\\\n& = \\sum_{i = 1}^{t} w_{i} h_{i}.\n\\end{aligned}\n\nTherefore, the boosting learning algorithm can be characterized as performing the gradient descent in functional space.\n\n\nBoosting framework\n\n\n\n\n\n\n\nInitialize f_{t} = 0\nWhile R [f_{t}] is decreasing\n\nCompute the negative function gradient - \\nabla R [f_{t}] as the function h_{t}, which is the steepest direction among the possible directions that the empirical risk function decreases the fastest.\n\nh_{t} = - \\nabla R [f_{t}]\n\nCompute the step size \\eta_{t} as the function weight w_{t}, which is how much step we should make along the fastest direction.\n\nw_{t} = \\eta_{t}\n\nUpdate the learned function\n\nf_{t + 1} = f_{t} + w_{t} h_{t} = f_{t} - \\eta_{t} \\nabla R [f_{t}]\n\n\n\n\n\n\nTo design a boosting algorithm, we need to specify the following.\n\nThe Loss function with respect to a function L [f].\nThe base functions \\mathcal{U}.\nThe learning rate \\eta_{t} in each iteration."
  },
  {
    "objectID": "Machine Learning/5_Boosting.html#adaboost",
    "href": "Machine Learning/5_Boosting.html#adaboost",
    "title": "38  Boosting",
    "section": "Adaboost",
    "text": "Adaboost\n\nBase function\nAdaboost requires that the type of the functions learned in each iteration is also a binary classifier\n\nh_{t} (\\mathbf{x}) \\in \\{-1, 1\\}, \\forall \\mathbf{x}, t,\n\nin which case the ensemble function f (\\mathbf{x}) is a true voting classifier.\n\nh_{t} (\\mathbf{x}) can vote for the positive and negative classes with the weight w_{t}.\nThe ensemble function makes the decisions based on the difference between the weighted strength of positive and negative votes\n\n  f (\\mathbf{x}) = \\sum_{t} w_{t} h_{t} (\\mathbf{x}) = \\sum_{t \\mid h_{t} (\\mathbf{x}) = 1} w_{t} - \\sum_{t \\mid h_{t} (\\mathbf{x}) = -1} w_{t}.\n  \n\n\n\nExponential loss\nAdaboost minimizes the exponential loss\n\nL (y, f (\\mathbf{x}) = \\phi (y f(\\mathbf{x})) = \\exp (-y f (\\mathbf{x}))\n\nwhich takes the exponential on the margins of the examples.\n\nThe exponential loss is an example of margin-enforcing loss, which encourages the classifier to have a large margin by penalizing both negative margins and small positive margins.\nThe exponential loss is an upper bound on the 0-1 loss.\n\nBy taking the functional gradients of empirical risk with the exponential loss with respect to the current function f_{t} at the iteration t, we can see how h_{t} is selected:\n\n\\begin{aligned}\n\\nabla R [f_{t}]\n& = \\arg\\max_{u} D_{u} R [f_{t}]\n\\\\\n& = \\arg\\max_{u} \\frac{ d }{ d \\epsilon } R [f_{t} + \\epsilon u] \\Big|_{\\epsilon = 0}\n\\\\\n& = \\arg\\max_{u} \\frac{ d }{ d \\epsilon } \\frac{ 1 }{ n } \\sum_{i}^{n} \\exp \\left(\n    - y_{i} \\left(\n        f_{t} (\\mathbf{x}_{i}) + \\epsilon u (\\mathbf{x}_{i})\n    \\right)\n\\right) \\Big|_{\\epsilon = 0}\n\\\\\n& = \\arg\\max_{u} \\frac{ 1 }{ n } \\sum_{i}^{n} u (\\mathbf{x}_{i}) \\exp \\left(\n    - y_{i} \\left(\n        f_{t} (\\mathbf{x}_{i}) + \\epsilon u (\\mathbf{x}_{i})\n    \\right)\n\\right) \\Big|_{\\epsilon = 0}\n\\\\\n& = \\arg\\max_{u} \\frac{ 1 }{ n } \\sum_{i}^{n} - y_{i}  u (\\mathbf{x}_{i}) \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right).\n\\end{aligned}\n\nAfter simplifying the equation, the gradient function h_{t} (\\mathbf{x}) learned in the iteration t is\n\nh_{t} (\\mathbf{x}) = - \\nabla R [f_{t}]  = \\arg\\max_{u} \\sum_{i}^{n}  y_{i} u (\\mathbf{x}_{i}) \\exp (- y_{i} f_{t} (\\mathbf{x}_{i})).\n\nTherefore, the function learned in each iteration is the one that maximizes the sum of the weighted margins on the training examples.\n\ny_{i} u (\\mathbf{x}_{i}) is the margin of example \\mathbf{x}_{i} with respect to the function u.\n\\exp (- y_{i} f_{t} (\\mathbf{x}_{i})) is the weight of example \\mathbf{x}_{i} for learning h_{t}, which is large if \\mathbf{x}_{i} has large negative margin for the current function f_{t}, and close to 0 if \\mathbf{x} has positive margin. Therefore, the weights select the function u that focuses on the examples that are hard to classify correctly by the current function f_{t}.\n\n\n\nStep size\nThe optimal step size is calculated using line search algorithm in Adaboost\n\n\\begin{aligned}\nw_{t}\n& = \\arg\\min_{\\eta} R [f_{t} + \\eta h_{t}]\n\\\\\n& = \\arg\\min_{\\eta} \\frac{ 1 }{ n } \\exp(- y_{i} (f_{t} (\\mathbf{x}_{i}) + \\eta h_{t} (\\mathbf{x}_{i})))\n\\\\\n& = \\arg\\min_{\\eta} c (\\eta).\n\\end{aligned}\n\nSince the function c (\\eta) = \\exp(- a + b \\eta)) is a convex function with respect to the variable \\eta, its minimum can be obtained by setting its derivative to 0\n\n\\begin{aligned}\n\\frac{ \\mathop{d} c }{ \\mathop{d} \\eta } (\\eta)\n& = 0\n\\\\\n\\sum_{i}^{n} - y_{i} h_{t} (\\mathbf{x}_{i}) \\exp(- y_{i} (f_{t} (\\mathbf{x}_{i}) + \\eta h_{t} (\\mathbf{x}_{i})))\n& = 0\n\\\\\n\\sum_{i}^{n} - y_{i} h_{t} (\\mathbf{x}_{i}) \\exp(- y_{i} f_{t + 1} (\\mathbf{x}_{i}; \\eta))\n& = 0\n& [f_{t + 1} = f_{t} + \\eta h_{t}]\n\\\\\n\\end{aligned}\n\nThe closed-form expression of the step-size can be derived since h_{t} (\\mathbf{x}_{i}) \\in \\{1, -1\\}\n$$\n\\begin{aligned}\n\\sum_{i}^{n} - y_{i} h_{t} (\\mathbf{x}_{i}) \\exp(- y_{i} (f_{t} (\\mathbf{x}_{i}) + \\eta h_{t} (\\mathbf{x}_{i})))\n& = 0\n\\\\\n\\sum_{i}^{n} - y_{i} h_{t} (\\mathbf{x}_{i}) \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (- y_{i} \\eta h_{t} (\\mathbf{x}_{i}))\n& = 0\n\\\\\n\\sum_{i \\mid y_{i} = h_{t} (\\mathbf{x}_{i})}^{n} - \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (- \\eta) + \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (\\eta)\n& = 0\n\\\\\n\\sum_{i \\mid y_{i} = h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (- \\eta)\n& = \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (\\eta)\n\\\\\n\\frac{\n    \\sum_{i \\mid y_{i} = h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}{\n\n    \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}\n& = \\frac{\n    e^{\\eta}\n}{\n    e^{-\\eta}\n}\n\\\\\n\\frac{\n    \\sum_{i = 1}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) -\n    \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}{\n    \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}\n& = e^{2 \\eta}\n\\end{aligned}\n$$\nDivide both numerator and denominator by \\sum_{i = 1}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})),\n\n\\begin{aligned}\n\\eta = \\frac{ 1 }{ 2 } \\log \\frac{ 1 - \\epsilon }{ \\epsilon },\n\\end{aligned}\n\nwhere\n\n\\epsilon = \\frac{\n    \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}{\n    \\sum_{i = 1}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}.\n\n\\epsilon is the weighted error of the week learner h_{t}, as it divides the sum of the weights for the incorrectly classified examples by the sum of the weights of all examples.\n\n\nWeak learner\nThe base function h_{t} in Adaboost is called the weak learner, because Adaboost can always converge even if h_{t} is not a good learner.\nThe empirical risk R can decrease if\n\n\\begin{aligned}\n\\lVert \\nabla R [f_{t}] \\rVert\n& &gt; 0\n\\\\\n\\sum_{i}^{n} - y_{i}  h_{t} (\\mathbf{x}_{i}) \\exp \\left(\n    y_{i} f_{t} (\\mathbf{x}_{i})\n\\right)\n& &gt; 0\n\\\\\n\\sum_{i \\mid y_{i} = h_{t} (\\mathbf{x}_{i})}^{n} \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right) - \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right)\n& &gt; 0\n\\\\\n\\sum_{i}^{n} \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right) - 2 \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right)\n& &gt; 0\n\\\\\n\\epsilon\n& &lt; \\frac{ 1 }{ 2 },\n\\end{aligned}\n\nwhich require that the weak learner in each iteration makes no more than half incorrect predictions on the training set.\nTherefore, Adaboost can also be seen as an algorithm that combines weak learners into a strong learner."
  },
  {
    "objectID": "Machine Learning/6_Support_Vector_Machine.html",
    "href": "Machine Learning/6_Support_Vector_Machine.html",
    "title": "39  Support Vector Machine",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "Machine Learning/7_Decision_Tree.html#tree-basics",
    "href": "Machine Learning/7_Decision_Tree.html#tree-basics",
    "title": "40  Decision Tree",
    "section": "Tree basics",
    "text": "Tree basics\nDecision tree is composed of nodes and edges.\n\nEach node corresponds to a subset of the original dataset.\nThe root node is the original training dataset provided to train the decision tree.\nThe path from the root node to a node specifies how the subset of the dataset is partitioned from the original training dataset.\n\nIn the following sections, we will use the following notations.\n\nA node: t\nA decision tree is a set of nodes: T\nThe original training set: \\mathbf{X}\nThe subset of the dataset that corresponds to node t: \\mathbf{X}_{t}"
  },
  {
    "objectID": "Machine Learning/7_Decision_Tree.html#impurity-function",
    "href": "Machine Learning/7_Decision_Tree.html#impurity-function",
    "title": "40  Decision Tree",
    "section": "Impurity function",
    "text": "Impurity function\nThe impurity function F (\\mathbf{y}) measures the impureness of a set of labels \\mathbf{y}.\n\nF (\\mathbf{y}) achieves maximum when the labels in \\mathbf{y} are in uniform distribution.\nF (\\mathbf{y}) achieves minimum when there is only one unique label in \\mathbf{y}.\n\nWe use F (\\mathbf{X}) in the following context to compute the impureness of the labels in the dataset \\mathbf{X} using the impurity function F.\n\nClassification\nGiven a dataset \\mathbf{X} with C unique labels, \\mathbb{P} (c) is the probability of label c in the dataset, which is computed by dividing the number of instances with label c by the total number instances in \\mathbf{X}.\n\nWhen there is only one class in \\mathbf{X}, the dataset is pure and thus impurity functions should return 0.\nOn the contrary, if all possible labels are in \\mathbf{X} and the numbers of all unique labels are the same, \\mathbf{X} achieves the maximum impureness.\n\n\nGini Impurity Index (Gini Impurity)\nGini impurity measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n\n\\begin{aligned}\nF (\\mathbf{X})\n& = \\sum_{c \\in C} \\mathbb{P} (c) (1 - \\mathbb{P} (c))\n\\\\\n& = \\sum_{c \\in C} \\mathbb{P} (c) - \\mathbb{P} (c)^{2}\n\\\\\n& = 1 - \\sum_{c \\in C} \\mathbb{P} (c)^2.\n\\\\\n\\end{aligned}\n\n\n\nShannon Entropy (Entropy)\nEntropy is defined as the expectation of the log value\n\nF (\\mathbf{X}) = \\sum_{c \\in C} \\mathbb{P} (c) \\log \\mathbb{P} (c).\n\nEntropy can be thought as the difference measured by KL Divergence between the probability distribution of the unique labels represented in the current dataset \\mathbf{X} and the distribution of the most impure dataset. Therefore, The larger the entropy, the more far away from a uniform distribution is the distribution of the labels represented by \\mathbf{X}.\n\n\n\nRegression\nGiven a dataset \\mathbf{X} with continuous labels \\{y_{1}, y_{2}, \\dots, y_{n}\\}, impurity functions can be defined a similar way.\n\nIf the labels in \\mathbf{X} are very similar (low variance), the impurity functions should return a value closed to 0.\nIf the labels in \\mathbf{X} are very different from each other (high variance), impurity functions should return a very large value.\n\n\nMean squared error\nA dataset’s impurity can be simply measured by the mean squared error.\n F (\\mathbf{X}) = \\frac{ 1 }{ N } \\sum_{i}^{n} (y_{i} - \\bar{y})^{2} \nwhere \\bar{y} is the mean value of the labels in dataset \\mathbf{X}."
  },
  {
    "objectID": "Machine Learning/7_Decision_Tree.html#splitting-criteria",
    "href": "Machine Learning/7_Decision_Tree.html#splitting-criteria",
    "title": "40  Decision Tree",
    "section": "Splitting criteria",
    "text": "Splitting criteria\nA split is a way that divides a feature space into different groups and is used in the tree building process to split a node to children nodes.\n\nBinary split (2-way split): split a feature space into 2 groups. A node will have 2 sub-nodes.\nk-way split: split a feature space into k groups. A node will have k sub-nodes.\n\nThe most important question of building a decision tree is how to choose the best split from a set of valid splits to split a node (dataset) into different child nodes (sub-datasets).\n\nA splitting criteria is a function that measures the impurity difference between the dataset before splitting and the datasets after splitting.\nThe best split s for the node t should be the one that has the maximum splitting criteria \\Delta F(\\mathbf{X}_{t}, s).\n\nGiven a set of datasets \\mathbf{X}_{1}, \\dots, \\mathbf{X}_{k} created by applying split s to the dataset \\mathbf{X}_{t}, the splitting criteria is defined as:\n\n\\Delta F (\\mathbf{X}_{t}, s) = F (\\mathbf{X}_{t}) - \\frac{ 1 }{ \\lvert \\mathbf{X}_{t} \\rvert } \\sum_{i = 1}^{k} \\lvert \\mathbf{X}_{i} \\rvert F (\\mathbf{X}).\n\nIf F is the entropy function, \\Delta F(\\mathbf{X}, s) is called Information Gain."
  },
  {
    "objectID": "Machine Learning/7_Decision_Tree.html#stopping-condition",
    "href": "Machine Learning/7_Decision_Tree.html#stopping-condition",
    "title": "40  Decision Tree",
    "section": "Stopping condition",
    "text": "Stopping condition\nEach split produces new nodes that recursively become the starting points for new splits.\n\nA node stops splitting when certain stopping conditions are satisfied and such nodes are leaf nodes.\nThe leaf node doesn’t have children but has a label according to the dataset it corresponds to.\n\nThe basic stopping condition is that the dataset that the leaf node corresponds to has impureness of 0 (single training instance or all training instance have the same label), in which case the splitting stops because there is no need to reduce the impureness.\n\nEarly stopping\nHowever, always splitting into pureness usually induces overfitting. Thus, there are other stopping conditions that can achieve early stopping to avoid overfitting.\n\nDataset size is below a threshold.\nSplitting criteria improvement is below a threshold.\nTree depth is above a threshold.\nNumber of nodes is above a threshold."
  },
  {
    "objectID": "Machine Learning/7_Decision_Tree.html#label-assignment",
    "href": "Machine Learning/7_Decision_Tree.html#label-assignment",
    "title": "40  Decision Tree",
    "section": "Label assignment",
    "text": "Label assignment\nFor each node, we can assign a label to the node according to the labels of the dataset it corresponds to.\n\nClassification: majority label.\nRegression: mean label.\n\nEvery node can have a label assigned, but only leaf nodes actually use labels.\n\nMisclassification cost\nAssuming the assigned label of the node t is y_{t}, the misclassification cost r(t) of a node t for classification is defined as.\n\nr (t) = 1 - \\mathbb{P} (y_{t}),\n\nand for regression is\n\nr(t) = \\frac{1}{N(t)} \\sum_{y \\in t} (y - y_{t}),\n\nwhere y \\in t means all labels in the dataset that node t corresponds to.\nThen weighted misclassification cost of the node t is defined as the product of misclassification cost and the probability of picking an instance that is in the node t.\n\nR (t) = \\mathbb{P} (\\mathbf{x} \\in \\mathbf{X}_{t}) r(t) = \\frac{\\lvert \\mathbf{X}_{t} \\rvert}{\\lvert \\mathbf{X} \\rvert} r(t).\n\nThen the misclassification cost of the a tree T is\n\nR (T) = \\sum_{t \\in \\hat{T}} R (t),\n\nwhere \\hat{T} is the set of the leaf nodes in tree T.\nThe weighted misclassification cost of a node t is always higher than the sum of the weighted misclassification costs of the children nodes T_{c} = \\{t_{1}, t_{2}, \\dots, t_{k}\\} that t splits to\n\nR (t) \\geq \\sum_{t_{i} \\in T_{c}} R (t_{i}).\n\nThus, if we split one of the leaf nodes of T to get a new and larger tree T',\n\nR (T) \\geq R (T'),\n\nwhich shows that the misclassification cost of a tree will always decrease or stay the same if we continue to split its leaf nodes."
  },
  {
    "objectID": "Machine Learning/7_Decision_Tree.html#pruning",
    "href": "Machine Learning/7_Decision_Tree.html#pruning",
    "title": "40  Decision Tree",
    "section": "Pruning",
    "text": "Pruning\nInstead of doing early stopping while building the decision tree to avoid overfitting, another way is to do pruning after the tree has build. Pruning is the process to make some internal nodes to be leaf nodes, and remove their children from a sufficiently large tree.\n\nMinimal cost-complexity pruning\nPreviously we show that R (T) might not a good measure of the performance of a tree in the sense that it always favors a larger tree. Thus we introduce another metric called cost-complexity that also considers the size of the tree.\n\nIf we consider each node has a complexity of \\alpha, the cost-complexity R_{\\alpha} (t) of a node t is\n\n  R_{\\alpha} (t) = R (t) + \\alpha  \n  \nThus, the cost-complexity R_{\\alpha} (T) of a tree T is\n\n  R_{\\alpha} (T) = \\sum_{t \\in \\hat{T}} R_{\\alpha} (t) = \\sum_{t \\in \\hat{T}} (R (t) + \\alpha) = R (T) + \\alpha \\lvert \\hat{T} \\rvert,\n  \nwhere \\hat{T} is the set of leaf nodes of T.\n\nCost-complexity can be seen as adding a regularization term that penalize the complexity of the tree to the misclassification cost.\n\n\\alpha is the regularization parameter that balances the training accuracy and tree complexity.\nGiven \\alpha, the goal of pruning of T is to get a pruned tree \\hat{T} (a subtree of T that has the same root with T) that minimizes R_{\\alpha} (\\hat{T}).\n\n\n\nWeakest-link cutting\nWeakest-link cutting is an efficient way of doing the minimal cost-complexity pruning.\nIf the tree T is pruned by deleting subtree T_{t} rooted at the node t (replacing tree T with node t), the cost-complexity difference between pruned tree \\hat{T} and unpruned tree T is\n\nR_{\\alpha} (\\hat{T}) - R_{\\alpha} (T) = R_{\\alpha} (t) - R_{\\alpha} (T_{t}).\n\n\nIf \\alpha = 0, R_{\\alpha} (t) - R_{\\alpha} (T_{t}) = R (t) - R (T_{t}) \\geq 0.\nAs \\alpha becomes larger, R_{\\alpha} (t) - R_{\\alpha} (T_{t}) is getting smaller and will eventually becomes &lt; 0, since \\alpha is increasing slower than \\alpha \\lvert \\hat{T} \\rvert.\nGiven a sufficiently large \\alpha, R_{\\alpha} (t) - R_{\\alpha} (T_{t}) &lt; 0, which means that the cost-complexity of the node t is better than its subtree T_{t}, and thus T_{t} should be pruned.\n\nGiven a tree T, the weakest link \\bar{t} is the internal node in T that achieves R_{\\alpha} (\\bar{t}) - R_{\\alpha} (T_{\\bar{t}}) = 0 with the smallest \\alpha value.\n\nThe \\alpha value that achieves R_{\\alpha} (t) - R_{\\alpha} (T_{t}) = 0 can be directly calculated.\n\n  \\begin{aligned}\n  R_{\\alpha} (t) - R_{\\alpha} (T_{t})\n  & = 0\n  \\\\\n  R (t) + \\alpha - (R (T) + \\alpha \\lvert \\hat{T} \\rvert)\n  & = 0\n  \\\\\n  R (t) - R (T) + \\alpha (1 + \\lvert \\hat{T} \\rvert)\n  & = 0\n  \\\\\n  \\alpha\n  & = \\frac{ R (t) - R (T_{t}) }{ \\lvert T_{t} \\rvert - 1 }\n  \\\\\n  \\end{aligned}\n  \nThe weakest link is defined as\n\n  \\bar{t} = \\arg \\min_{t \\in T \\setminus \\hat{T}} \\frac{ R (t) - R (T_{t}) }{ \\lvert T_{t} \\rvert - 1 }\n  \nwhere T \\setminus \\hat{T} means the set of the internal nodes of T.\nIf there are more than 1 internal node that achieves R_{\\alpha} (\\bar{t}) - R_{\\alpha} (T_{\\bar{t}}) = 0 with same minimum \\alpha value, they are all called the weakest links.\n\nWeakest-link cutting finds the optimal subtree \\hat{T} of T_{max} that minimizes R_{\\alpha} (\\hat{T}) with a predefined threshold \\alpha_{max} in a iterative way.\n\nWe start the pruning process by first removing from T_{max} the subtrees T_{t} rooted at nodes t that have already achieved R_{\\alpha} (t) - R_{\\alpha} (T_{t}) = 0. We denote the resulting tree T_{0}.\nIn each iteration i, the weakest link(s) \\bar{t} of tree T_{i - 1} is identified by calculating\n\n  \\bar{t} = \\arg \\min_{t \\in T_{i - 1} \\setminus \\hat{T}_{i - 1}} \\frac{\n      R (t) - R (T_{t})\n  }{\n      \\lvert T_{t} \\rvert - 1\n  }.\n  \nIn the meantime, we can also calculate the \\alpha_{i} that identifies the weakest links.\n\n  \\alpha_{i} = \\min_{t \\in T_{i - 1} \\setminus \\hat{T}_{i - 1}} \\frac{\n      R (t) - R (T_{t})\n  }{\n      \\lvert T_{t} \\rvert - 1\n  }.\n  \nWe replace T_{\\bar{t}} (the subtree rooted at \\bar{t}) by \\bar{t} and denote the resulting tree T_{i}.\nContinue the iteration until the minimum \\alpha required to achieve R_{\\alpha} (\\bar{t}) - R_{\\alpha} (T_{\\bar{t}}) = 0 is above a predefined threshold \\alpha_{max}."
  },
  {
    "objectID": "Machine Learning/7_Decision_Tree.html#cart-tree-building",
    "href": "Machine Learning/7_Decision_Tree.html#cart-tree-building",
    "title": "40  Decision Tree",
    "section": "CART Tree building",
    "text": "CART Tree building\n\nIdentify all possible splits\nCART considers binary split of a single feature for each node (each node only splits a one feature and only has 2 children).\n\nFor a categorical feature that has k distinct values, CART considers all possible ways to split the k distinct values into 2 groups.\n\nThe maximum ways of splitting is 2^{k - 1} - 1.\ne.g. If the categorical feature has 4 distinct values: \\{1, 2, 3, 4\\}, then all possible splits are\n\n\n\n\n\n\n\n\n\n\n\n\n\nindex\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nleft child\n{1}\n{2}\n{3}\n{4}\n{1, 2}\n{1, 3}\n{1, 4}\n\n\nright child\n{2, 3, 4}\n{1, 3, 4}\n1, 2, 4}\n{1, 2, 3}\n{3, 4}\n{2, 4}\n{2, 3}\n\n\n\n\nFor a numerical feature that has k distinct values appeared in the dataset, CART considers all the intervals between 2 consecutive values as the splits.\n\nThe maximum ways of splitting is k - 1.\ne.g. If the numerical feature has 6 distinct values: \\{-5.0, 1.0, 3.0, 5.0, 7.0, 11.0\\}, then all possible splits are\n\n\n\n\n\n\n\n\n\n\n\nindex\n1\n2\n3\n4\n5\n\n\n\n\nleft child\n \\leq -2.0 \n \\leq 2.0 \n \\leq 4.0 \n \\leq 6.0 \n \\leq 9.0 \n\n\nright child\n &gt; -2.0 \n &gt; 2.0 \n &gt; 4.0 \n &gt; 6.0 \n &gt; 9.0 \n\n\n\n\n\nAt a given node, CART considers all possible splits of all features and chooses the one that has the maximum splitting criteria.\n\n\nRecursive tree building\n\nIdentify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either treat it as categorical feature by discretizing it or sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split.\nCalculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split.\nOnce a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset.\nThe splitting on a node stops when no further splitting can be made (the dataset contains only one class) or satisfies the preset early-stopping conditions."
  },
  {
    "objectID": "Machine Learning/7_Decision_Tree.html#references",
    "href": "Machine Learning/7_Decision_Tree.html#references",
    "title": "40  Decision Tree",
    "section": "References",
    "text": "References\n\nhttps://victorzhou.com/blog/intro-to-random-forests/\nhttps://www.math.snu.ac.kr/~hichoi/machinelearning/lecturenotes/CART.pdf\nhttps://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote17.html\nhttp://www.odbms.org/wp-content/uploads/2014/07/DecisionTrees.pdf\nhttps://scientistcafe.com/ids/splitting-criteria.html\nhttps://online.stat.psu.edu/stat508/book/export/html/647"
  },
  {
    "objectID": "Machine Learning/K-means.html#preliminary",
    "href": "Machine Learning/K-means.html#preliminary",
    "title": "41  K-means",
    "section": "Preliminary",
    "text": "Preliminary\n\n\nStatistics\n\nEstimator\nIn statistics, an estimator is a function that takes as inputs a set of observations sampled from an unknown probability distribution P_{\\theta}(X) with the true parameter \\theta and outputs the best guess of the parameter \\hat{\\theta} of P_{\\theta}(X).\n\nAn estimator is also a random variable.\n\n\n\nBias\nThe bias of an estimator measures whether the expectation of the estimator is the same as the true parameter. Let \\hat{\\theta} be the output of an estimator for \\theta. The bias of \\hat{\\theta} as an estimator for \\theta is\n \\operatorname{Bias}(\\hat{\\theta}, \\theta) = \\mathbb{E}[\\hat{\\theta}] - \\theta \n\n\\operatorname{Bias}(\\hat{\\theta}, \\theta) = 0, then we say \\hat{\\theta} is an unbiased estimator of \\theta.\n\\operatorname{Bias}(\\hat{\\theta}, \\theta) &gt; 0, \\hat{\\theta} typically overestimates \\theta.\n\\operatorname{Bias}(\\hat{\\theta}, \\theta) &lt; 0, \\hat{\\theta} typically underestimates \\theta.\n\n\n\nVariance (of an estimator)\nThe variance of an estimator measures the degree to which the estimated parameters vary depending on the sampled observations. Let \\hat{\\theta} be the output of an estimator for \\theta. The variance of \\hat{\\theta} as an estimator is\n \\operatorname{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^{2}] \n\n\nMean squared error\nIn statistics, the mean squared error (MSE) is a risk (loss) function measures the difference between an estimator \\hat{\\theta} with the true parameter \\theta.\n \\operatorname{MSE}(\\hat{\\theta}, \\theta) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^{2}] \nIf the estimator \\hat{\\boldsymbol{\\theta}} and the true parameter \\boldsymbol{\\theta} are vectors,\n \\operatorname{MSE}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta}) = \\mathbb{E}[\\lVert \\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta} \\rVert^{2}] \n\n\nBias-variance decomposition\nA risk (loss) function can be often decomposed into a bias, a variance and a noise term. Here we take MSE as an example and expand it into a bias and variance term (noise is omitted).\n \\operatorname{MSE}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta}) = \\operatorname{Var}(\\hat{\\boldsymbol{\\theta}}) + \\operatorname{Bias}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta})^{2} \n:::{admonition} Proof :class: dropdown\n\n\\begin{align}\n\\operatorname{MSE}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta}) & = \\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta} \\rVert^{2} ] \\\\\n& = \\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] + \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert^{2}] & [\\text{add and subtract } \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] ] \\\\\n& = \\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert^{2} + 2 \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert + \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert^{2} ] & [(a + b)^{2} = a^{2} + 2ab + b^{2}] \\\\\n& = \\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert^{2} ] + \\mathbb{E}[ 2 \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert ] + \\mathbb{E}[ \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert^{2} ] & [\\text{Linearity of Expectation}] \\\\\n& = \\operatorname{Var}(\\hat{\\boldsymbol{\\theta}}) + 2\\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert] \\mathbb{E}[ \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert ] + \\mathbb{E}[ \\operatorname{Bias}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta})^{2} ] \\\\\n& = \\operatorname{Var}(\\hat{\\boldsymbol{\\theta}}) + \\operatorname{Bias}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta})^{2} & [\\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert] = \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert = 0] \\\\\n\\end{align}\n\n:::"
  },
  {
    "objectID": "Machine Learning/K-means.html#problem-formulation",
    "href": "Machine Learning/K-means.html#problem-formulation",
    "title": "41  K-means",
    "section": "Problem formulation",
    "text": "Problem formulation\n\n\nK-means clustering algorithm is a unsupervised learning algorithm that aims to cluster similar instances into the same group.\nThe input to the algorithm is n instances \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{n} \\in \\mathbb{R}^{d} without labels and the output of the algorithm is k centroids \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{k} \\in \\mathbb{R}^{d}, where k is a hyperparameter used to control the number of centroids desired.\nThe goal of K-means clustering is to find the best k centroids that minimizes a loss function (often MSE loss of the L_{2} norm or euclidean distance of the difference vector) that captures the overall distances between the instances and the centroids assigned.\n \\operatorname{loss}(\\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k}) = \\sum_{i = 1}^{n} \\lVert \\mathbf{x}_{i} - \\mathbf{u}_{x_{i}} \\rVert^{2} \nwhere \\mathbf{u}_{x_{i}} is the centroid that instance \\mathbf{x}_{i} is assigned to.\nAnother way to write the loss function is to consider the distances between each centroid and the instances in the cluster that the centroid represents. The benefit of this formulation is that the clusters that the centroids represent are separated as variables, which is easier to do algorithm analyzing.\n \\operatorname{loss}(\\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k}, C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}}) = \\sum_{i = 1}^{k} \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}_{i}}} \\lVert \\mathbf{x} - \\mathbf{u}_{i} \\rVert^{2} \nwhere C_{\\mathbf{u}_{i}} is the cluster that centroid \\mathbf{u}_{i} represents."
  },
  {
    "objectID": "Machine Learning/K-means.html#k-means-using-lloyds-algorithm",
    "href": "Machine Learning/K-means.html#k-means-using-lloyds-algorithm",
    "title": "41  K-means",
    "section": "K-means using Lloyd’s algorithm",
    "text": "K-means using Lloyd’s algorithm\n\n\nAlgorithm\n\nFunction: K-means\nInput: a set of instances \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{n} and a hyperparameter k.\nOutput: a set of centroids \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{k}. 1. Initialize cluster centroids \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{k} by randomly drawing k instances as the centroids. 2. Repeat until convergence or a fixed number of iterations: 1. Assignment step: get the nearest centroid \\mathbf{c}_{i} for each instance \\mathbf{x}_{i}:\n    $$ \\mathbf{c}_{i} = \\arg \\min_{j} \\lVert \\mathbf{x}_{i} - \\mathbf{u}_{j} \\rVert^{2} $$\n\n2. **Refitting step**: update each centroid based on the instances in its cluster:\n\n    $$ \\mathbf{u}_{j} = \\frac{\\sum_{i}^{m} \\mathbb{1}_{\\mathbf{c}_i = j} \\mathbf{x}_{i}}{\\sum_{i}^{m} \\mathbb{1}_{\\mathbf{c}_{i} = j}} $$\n\n\n\nConvergence of Lloyd’s algorithm\nK-means solved using Lloyd’s algorithm is guaranteed to converge to a local minimum because: - The loss value is guaranteed to be smaller or stay the same in the assignment step because each instance \\mathbf{x}_{i} gets the nearest centroid.\n$$ \\operatorname{loss}( \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k}, (C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}})^{t + 1} ) \\leq \\operatorname{loss}( \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k}, (C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}})^{t} ) $$\n    \n\nThe loss value is guaranteed to be smaller or stay the same in the refitting step.\n \\operatorname{loss}( (\\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k})^{t + 1}, (C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}})^{t + 1} ) \\leq \\operatorname{loss}( (\\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k})^{t}, (C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}})^{t + 1} ) \nTo see why this is true, consider a single centroid-cluster pair \\mathbf{u} and C_{\\mathbf{u}}, for all instances \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{n} that belongs to the cluster C_{\\mathbf{u}}, the loss function \\operatorname{loss}(\\mathbf{u}, C_{\\mathbf{u}}) will be minimized when \\mathbf{u} is the average of the instances in C_{\\mathbf{u}}:\n \\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\mathbf{x}_{i} = \\arg \\min_{\\mathbf{u} \\in \\mathbb{R}^{d}} \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} - \\mathbf{u} \\rVert^{2} = \\arg \\min_{\\mathbf{u} \\in \\mathbb{R}^{d}} \\operatorname{loss}(\\mathbf{u}, C_{\\mathbf{u}}) \nbecause of the equation below derived from the bias-variance decomposition of MSE function:\n \\operatorname{loss}(\\mathbf{u}, C_{\\mathbf{u}}) = \\operatorname{loss}(\\mathbf{\\boldsymbol{\\mu}}, C_{\\mathbf{\\mathbf{u}}}) + n \\lVert \\boldsymbol{\\mu} - \\mathbf{u} \\rVert \nwhere n is the number of instances in the cluster C_{\\mathbf{u}}.\n:::{admonition} Proof :class: dropdown\n\n  \\begin{align}\n  \\operatorname{loss}(\\mathbf{u}, C_{\\mathbf{u}}) & = \\operatorname{loss}(\\boldsymbol{\\mu}, C_{\\mathbf{u}}) + n\\lVert \\boldsymbol{\\mu} - \\mathbf{u} \\rVert^{2} \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} - \\boldsymbol{\\mu} \\rVert^{2} + n\\lVert \\boldsymbol{\\mu} - \\mathbf{u} \\rVert^{2} \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\left( \\lVert \\mathbf{x} \\rVert^{2} - 2\\lVert \\mathbf{x} \\rVert \\lVert \\boldsymbol{\\mu} \\rVert + \\lVert \\boldsymbol{\\mu} \\rVert^{2} \\right) + n\\left( \\lVert \\boldsymbol{\\mu} \\rVert^{2} - 2 \\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + \\lVert \\mathbf{u} \\rVert^{2} \\right) \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\left( \\lVert \\mathbf{x} \\rVert^{2} - 2\\lVert \\mathbf{x} \\rVert \\lVert \\frac{S}{n} \\rVert + \\lVert \\frac{S}{n} \\rVert^{2} \\right) + n\\left( \\lVert \\frac{S}{n} \\rVert^{2} - 2 \\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + \\lVert \\mathbf{u} \\rVert^{2} \\right) & \\left[ \\text{replace some } \\boldsymbol{\\mu} \\text{ with } \\frac{S}{n} \\text{ where } S = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\mathbf{x} \\right] \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} \\rVert^{2} - \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} 2\\lVert \\mathbf{x} \\rVert \\lVert \\frac{S}{n} \\rVert + \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\frac{S}{n} \\rVert^{2} + n\\lVert \\frac{S}{n} \\rVert^{2} - 2n\\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + n\\lVert \\mathbf{u} \\rVert^{2} \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} \\rVert^{2} - 2 \\lVert S \\rVert \\lVert \\frac{S}{n} \\rVert + n\\lVert \\frac{S}{n} \\rVert^{2} + n\\lVert \\frac{S}{n} \\rVert^{2} - 2n\\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + n\\lVert \\mathbf{u} \\rVert^{2} & \\left[ \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert x \\rVert = \\lVert S \\rVert \\text{ and } \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} 1 = n \\right] \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} \\rVert^{2} - 2n\\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + n\\lVert \\mathbf{u} \\rVert^{2} & \\left[ \\lVert S \\rVert \\lVert \\frac{S}{n} \\rVert = n\\lVert \\frac{S}{n} \\rVert^{2}  \\right] \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} ( \\lVert \\mathbf{x} \\rVert^{2} - 2\\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + \\lVert \\mathbf{u} \\rVert^{2} ) \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} - \\mathbf{u} \\rVert^{2} \\\\\n  \\end{align}\n  \nSince \\lvert C_{\\mathbf{\\mathbf{u}}} \\rvert \\cdot \\lVert \\boldsymbol{\\mu} - \\mathbf{u} \\rVert is always positive,\n \\operatorname{loss}(\\boldsymbol{\\mu}, C_{\\mathbf{u}}) \\leq \\operatorname{loss}(\\mathbf{\\mathbf{u}}, C_{\\mathbf{\\mathbf{u}}}) \n:::"
  },
  {
    "objectID": "Machine Learning/K-means.html#the-k-means-initializer",
    "href": "Machine Learning/K-means.html#the-k-means-initializer",
    "title": "41  K-means",
    "section": "The K-means++ initializer",
    "text": "The K-means++ initializer\n\nAlthough the default behavior of the K-means algorithm is to initialize the centroids randomly, the quality of the final solution depends heavily on the initialization because K-means is only guaranteed to converge to a local point.\nThe K-means++ initializer is a special way of initializing the centroids so that - the convergence of K-means is faster, - the final loss is bounded (the quality of the final solution won’t be very bad).\n\n\nPick an instance \\mathbf{x} uniformly at random and set T \\gets \\{\\mathbf{x}\\}\nWhile \\lvert T \\rvert &lt; k:\n\nPick an instance \\mathbf{x} at random, with probability proportional to\n \\operatorname{cost}(\\mathbf{x}, T) = \\min_{\\mathbf{u} \\in T} \\lVert \\mathbf{x} - \\mathbf{u} \\rVert^{2} \nAdd \\mathbf{x} to T."
  },
  {
    "objectID": "Machine Learning/K-means.html#reference",
    "href": "Machine Learning/K-means.html#reference",
    "title": "41  K-means",
    "section": "Reference",
    "text": "Reference\n\n\nhttps://stanford.edu/~cpiech/cs221/handouts/kmeans.html\nhttps://cseweb.ucsd.edu/~dasgupta/291-geom/kmeans.pdf"
  },
  {
    "objectID": "Machine Learning/K-means.html#implementation",
    "href": "Machine Learning/K-means.html#implementation",
    "title": "41  K-means",
    "section": "Implementation",
    "text": "Implementation\n\n#https://takoscribe.com/2020/12/29/kmeans-clustering-with-pytorch/\n\nimport functools\n\nimport tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nclass GradientKMeans(nn.Module):\n    def __init__(self, num_centroids, n_epochs, batch_size, lr=1e-2):\n        super().__init__()\n        \n        self.num_centroids = num_centroids\n        self.n_epochs = n_epochs\n        self.batch_size = batch_size\n        self.lr = lr\n\n    def _initialize(self, x):\n        assignment = [i % self.num_centroids for i in range(x.size(0))]\n        random_indices = torch.randperm(len(assignment))\n        random_assignment = torch.LongTensor(assignment)[random_indices]\n        for i in range(self.num_centroids):\n            self.centroids.data[i] = x[random_assignment == i].mean(0)\n        \n    def _assign(self, x):\n        indices = ((x[:,None] - self.centroids) ** 2).mean(2).argmin(1)\n        \n        return indices\n \n    def forward(self, x):\n        return self._assign(x)\n    \n    def fit(self, X):\n        self.centroids = nn.Parameter(torch.zeros(self.num_centroids, X.shape[1]))\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        self.loss = nn.MSELoss()\n        centroids_init = False\n        \n        cost_window = 25\n        costs = []\n\n        X_t = torch.utils.data.TensorDataset(torch.Tensor(X), torch.zeros((X.shape[0], )))\n        iterator = torch.utils.data.DataLoader(X_t, batch_size=self.batch_size, shuffle=True)\n        for i in range(self.n_epochs):\n            with tqdm.tqdm(total=len(X) // self.batch_size) as progress_bar:\n                for x, _ in iterator:\n                    if not centroids_init: \n                        self._initialize(x)\n                        centroids_init = True\n\n                    assignment = self._assign(x)\n                    \n                    self.optimizer.zero_grad()\n                    means = self.centroids[assignment]\n                    cur_cost = self.loss(x, means)\n                    cur_cost.backward()\n                    self.optimizer.step()\n                    \n                    costs.append(cur_cost.item())\n                    \n                    progress_bar.set_postfix({\n                        'KMeans': float(functools.reduce(lambda x, y: x + y, costs[-cost_window:])) /  len(costs[-cost_window:])\n                    })\n                    progress_bar.update(1) # 1 step\n                    \n    def predict(self, X):\n        Y = self(torch.Tensor(X))\n        \n        return Y\n    \n    def get_centroids(self):\n        centroids = self.centroids.cpu().detach().numpy()\n        \n        return centroids\n\ncenters = [[-1, 1], [1, 1], [-1, -1], [1, -1]]\nn_clusters = len(centers)\nn_samples = 6000\nX, Y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=0.7, random_state=40)\n\ngradient_kmeans = GradientKMeans(2, 10, 64)\ngradient_kmeans.fit(X)\n\ny_true = gradient_kmeans.predict(X)\ncentroids = gradient_kmeans.get_centroids()\n\nplt.figure(1)\nfor k, col in enumerate([\"r\", \"b\", \"g\", \"m\", \"y\", \"c\"]):\n    cluster_data = y_true == k\n    plt.scatter(X[cluster_data, 0], X[cluster_data, 1], c=col, marker=\".\", s=10)\n\nplt.scatter(centroids[:, 0], centroids[:, 1], c=\"w\", s=50)\nplt.show()\n94it [00:00, 591.45it/s, KMeans=1.01]                                                                                                                                                                                                                                                   \n94it [00:00, 1217.76it/s, KMeans=0.943]                                                                                                                                                                                                                                                 \n94it [00:00, 1212.54it/s, KMeans=0.921]                                                                                                                                                                                                                                                 \n94it [00:00, 1192.24it/s, KMeans=0.941]                                                                                                                                                                                                                                                 \n94it [00:00, 1196.55it/s, KMeans=0.928]                                                                                                                                                                                                                                                 \n94it [00:00, 1179.02it/s, KMeans=0.89]                                                                                                                                                                                                                                                  \n94it [00:00, 1181.19it/s, KMeans=0.922]                                                                                                                                                                                                                                                 \n94it [00:00, 1178.50it/s, KMeans=0.932]                                                                                                                                                                                                                                                 \n94it [00:00, 1096.57it/s, KMeans=0.932]                                                                                                                                                                                                                                                 \n94it [00:00, 1188.89it/s, KMeans=0.909]                                                                                                                                                                                                                                                 \n\n\n\npng"
  }
]