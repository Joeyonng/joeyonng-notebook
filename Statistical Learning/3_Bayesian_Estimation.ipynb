{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6d3d13",
   "metadata": {},
   "source": [
    "*Updated 01-11-2023 (First commited 01-08-2023)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e07b6-ac0b-4a5f-87b6-7ff0fa0a48dc",
   "metadata": {},
   "source": [
    "(bayesian-estimation)=\n",
    "# Bayesian Estimation\n",
    "\n",
    "- The form of the density $\\mathbb{P}_{\\mathbf{X} \\mid \\boldsymbol{\\Theta}}(\\mathbf{x} \\mid \\boldsymbol{\\theta})$ is assumed to be known, but the value of the parameter vector $\\boldsymbol{\\theta}$ is not known exactly.\n",
    "\n",
    "- Our initial knowledge about $\\boldsymbol{\\theta}$ is assumed to be contained in a known a priori density $\\mathbb{P}_{\\boldsymbol{\\Theta}}(\\boldsymbol{\\theta})$.\n",
    "\n",
    "- The rest of our knowledge about $\\boldsymbol{\\theta}$ is contained in a set $\\mathcal{D}$ of n samples $\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n}$ drawn independently according to the unknown probability density $\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb52f26-19f5-4432-80ff-d2ddc6a1f646",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### Statistical Learning\n",
    "\n",
    "- [Bayesian Decision Theory (BDT)](bayesian-decision-theory)\n",
    "\n",
    "- [Maximum Likelihood Estimation (MLE)](maximum-likelihood-estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef13f21-740d-4556-ba1d-d1ddf7c4cd7f",
   "metadata": {},
   "source": [
    "## Views on parameter estimation\n",
    "\n",
    "There are two different frameworks on statistical inferences: frequentist view and Bayesian view. \n",
    "Both views have the same definition of the probability, but they have different views on how probability of an event should be calculated or accessed. \n",
    "Thus, the way to do parameter estimation is different under the two frameworks. \n",
    "\n",
    "### Frequentist view\n",
    "\n",
    "Frequentists believe that the probability of an event is a measure of relative frequency and should be calculated by observing how many times the event happens in a large number of trials.\n",
    "\n",
    "- Probability: the probability of any event is objective and doesn't change with different beliefs to the event.\n",
    "\n",
    "- Parameters: if the parameters of the distribution are unknown, the parameters must be fixed constants. \n",
    "That is, they must be certain determined values. \n",
    "\n",
    "- Estimation: the single best estimation of the parameter can be derived using single dataset and its goodness (bias and variance) can be measured by sampling different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f23f9-bad5-4cc4-96c4-a4fb03197463",
   "metadata": {},
   "source": [
    "### Bayesian view\n",
    "\n",
    "Bayesian approach believes that a probability of an event includes not only the relative frequency, but also the subjective beliefs. \n",
    "That is, the degree of belief on the outcomes of the experiment. \n",
    "\n",
    "- Probability: the subjective beliefs can be very different from person to person, and thus the probability of any event is very subjective. \n",
    "\n",
    "- Parameters: the unknown parameters of the distribution are viewed as random variables, and thus include subjective beliefs. \n",
    "\n",
    "- Estimation: the subjective beliefs of the parameters are specified using a prior distribution, which is then updated using the single dataset observed. \n",
    "The result of the estimation is a posterior probabilities of a range of parameter values, which include both prior beliefs and relative frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34d845-eaa4-4d7b-a2ad-d9897199a71a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bayesian estimation\n",
    "\n",
    "In Bayesian estimation, the unknown parameters are treated as random variables. \n",
    "\n",
    "- The subjective beliefs about the parameters that we want to estimate before observing any dataset are encoded using a distribution\n",
    "\n",
    "    $$\n",
    "    \\mathbb{P}_{\\boldsymbol{\\Theta}} (\\boldsymbol{\\theta}),\n",
    "    $$\n",
    "    \n",
    "    which specifies the prior probabilities of all possible values of the parameters. \n",
    "\n",
    "- The likelihood of a dataset $\\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}$ given parameters $\\boldsymbol{\\theta}$ is a conditional probability\n",
    "\n",
    "    $$\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid \\boldsymbol{\\Theta}} \\left(\n",
    "        \\mathcal{X} \\mid \\boldsymbol{\\theta}\n",
    "    \\right).\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe221d-d938-4bca-9c3b-06ec1f772953",
   "metadata": {},
   "source": [
    "Unlike maximum likelihood estimation, which is under frequentist view and gives only the single best parameter, the result of Bayesian estimation is a posterior distribution that informs us how the observed data update the prior. According to Bayes Theorem, the posterior distribution can be calculated by:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}_{\\boldsymbol{\\Theta} \\mid \\mathbf{X}} \\left(\n",
    "    \\boldsymbol{\\theta} \\mid \\mathcal{X}\n",
    "\\right) = \\frac{\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid \\boldsymbol{\\Theta}} \\left (\n",
    "        \\mathcal{X} \\mid \\boldsymbol{\\theta} \n",
    "    \\right) \\mathbb{P}_{\\boldsymbol{\\Theta}} \\left(\n",
    "        \\boldsymbol{\\theta}\n",
    "    \\right)\n",
    "}{\n",
    "    \\mathbb{P}_{\\mathbf{X}} \\left(\n",
    "        \\mathcal{X}\n",
    "    \\right)\n",
    "}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130d739-d81c-449c-abc1-9cdc40229412",
   "metadata": {},
   "source": [
    "### Maximum a posteriori (MAP) estimation\n",
    "\n",
    "In the case where we want a single estimate for the parameter using Bayesian estimation, MAP estimation chooses the value of the parameter that has the largest probability in the posterior distribution\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}_{MAP} = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{\\boldsymbol{\\Theta} \\mid \\mathbf{X}} \\left(\n",
    "    \\boldsymbol{\\theta} \\mid \\mathcal{X}\n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a22cb-5806-430a-ac0c-0f61b517e40e",
   "metadata": {},
   "source": [
    "## Bayesian BDR\n",
    "\n",
    "> TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e2150-34b7-4fe9-860c-2acb770d1026",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example: mean of the univariate Gaussian\n",
    "\n",
    "Here we shows an example of estimating the posterior probability of the mean parameter of a univariate normal distribution using Bayesian estimation. \n",
    "\n",
    "Consider the univariate case where the probability of the instance $x$ follows a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^{2}$:\n",
    "\n",
    "$$ \n",
    "\\mathbb{P}_{X \\mid \\mu} (x \\mid \\mu) \\sim \\mathcal{N} (\\mu, \\sigma^{2}) = \\mathcal{G} (x, \\mu, \\sigma^{2}), \n",
    "$$\n",
    "\n",
    "and we assume whatever prior knowledge we have about $\\mu$ can be expressed by another normal distribution with known mean $\\mu_{0}$ and known variance $\\sigma_{0}^{2}$:\n",
    "\n",
    "$$ \n",
    "\\mathbb{P}_{\\mu} (\\mu) \\sim \\mathcal{N} (\\mu_{0}, \\sigma_{0}^{2}) = \\mathcal{G} (\\mu, \\mu_{0}, \\sigma_{0}^{2}). \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d946ec6-d65e-47fd-b568-9d76645f5a95",
   "metadata": {},
   "source": [
    "### Posterior distribution of mean parameter\n",
    "\n",
    "Suppose now that $n$ samples $\\mathcal{X} = \\{x_{1}, \\dots, x_{n}\\}$ are independently sampled. We can use Bayes formula to obtain the posterior probability:\n",
    "\n",
    "$$ \n",
    "\\mathbb{P}_{\\mu \\mid \\mathcal{X}} (\\mu \\mid \\mathcal{X}) = \\frac{ \n",
    "    \\mathbb{P}_{X \\mid \\mu} (\\mathcal{X} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu) \n",
    "}{\n",
    "    \\mathbb{P}_{X} (\\mathcal{X})\n",
    "}. \n",
    "$$\n",
    "\n",
    "Since $\\mathbb{P}_{X}(\\mathcal{X})$ is a normalization factor that doesn't depend on $\\mu$, we now omit it for simplicity: \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) \n",
    "& \\propto \\mathbb{P}_{X \\mid \\mu} (\\mathcal{X} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu) \n",
    "\\\\\n",
    "& = \\prod_{i=1}^{n} \\mathbb{P}_{X \\mid \\mu} (x_{i} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu) \n",
    "& [\\text{independent assumption}].\n",
    "\\\\\n",
    "\\end{aligned} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69e540-dab3-434e-b69a-3c71514b3a96",
   "metadata": {},
   "source": [
    "After expanding the normal distribution definition and some simplifications, we can see that $\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X})$ also follows a normal distribution:\n",
    "\n",
    "$$ \n",
    "\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) \\sim \\mathcal{N} (\\mu_{n}, \\sigma_{n}^{2}) \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$ \n",
    "\\mu_{n} = \\frac{ \n",
    "    \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n",
    "}{ \n",
    "    \\sigma^{2} + n \\sigma_{0}^{2} \n",
    "},\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma_{n}^{2} = \\frac{\n",
    "    \\sigma_{0}^{2} \\sigma^{2}\n",
    "}{\n",
    "    n \\sigma_{0}^{2} + \\sigma^{2}\n",
    "}. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a31a94-a2c6-4e20-a52a-aa87e3365bb7",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::{admonition} Proof: $\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) \\sim \\mathcal{N} (\\mu_{n}, \\sigma_{n}^{2})$\n",
    ":class: dropdown\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\mathbb{P}_{\\mu \\mid \\mathcal{D}}(\\mu \\mid \\mathcal{D}) \n",
    "& \\propto \\mathbb{P}_{\\mathcal{D} \\mid \\mu}(\\mathcal{D} \\mid \\mu) \\mathbb{P}_{\\mu}(\\mu) \\\\\n",
    "& = \\prod_{i=1}^{n} \\mathbb{P}_{X \\mid \\mu}(x_{i} \\mid \\mu) \\mathbb{P}_{\\mu}(\\mu) \\\\\n",
    "& = \\prod_{i=1}^{n} \\frac{ 1 }{\\sqrt{2 \\pi \\sigma^{2}}} \\exp{ - \\frac{ (x_{i} - \\mu)^{2} }{ 2 \\sigma^{2} } } \\frac{ 1 }{ \\sqrt{2 \\pi \\sigma_{0}^{2}} } \\exp{ - \\frac{ (\\mu - \\mu_{0})^{2} }{ 2 \\sigma_{0}^{2} } } \\\\\n",
    "& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\prod_{i=1}^{n} \\exp{ \\left[ - \\frac{ (x_{i} - \\mu)^{2} }{ 2 \\sigma^{2}} - \\frac{ (\\mu - \\mu_{0})^{2} }{ 2 \\sigma_{0}^{2} } \\right] } & [\\text{merging constants and exponentials}] \\\\\n",
    "& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ \\sum_{i=1}^{n} - \\frac{ (x_{i} - \\mu)^{2} }{ 2 \\sigma^{2} } - \\frac{ (\\mu - \\mu_{0})^{2} }{ 2 \\sigma_{0}^{2} } \\right] } \\\\\n",
    "& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ \\sum_{i=1}^{n} - \\frac{ x_{i}^{2} - 2 x_{i} \\mu + \\mu^{2}}{2 \\sigma^{2} } - \\frac{ \\mu^{2} - 2 \\mu \\mu_{0} + \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } & [\\text{expanding squares}] \\\\\n",
    "& = \\frac{ 1 }{\\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\frac{ \\sum_{i=1}^{n} \\left[ x_{i}^{2} - 2 x_{i} \\mu + \\mu^{2} \\right] }{ 2 \\sigma^{2} } - \\frac{ \\mu^{2} - 2 \\mu \\mu_{0} + \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2}} \\right] } \\\\\n",
    "& = \\frac{ 1 }{\\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\frac{ \\sum_{i=1}^{n} x_{i}^{2} - \\sum_{i=1}^{n} 2 x_{i} \\mu + n \\mu^{2} }{2 \\sigma^{2}} - \\frac{ \\mu^{2} - 2 \\mu \\mu_{0} + \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } & [\\text{reordering sums}] \\\\\n",
    "& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\frac{ \\sum_{i=1}^{n} x_{i}^{2} }{ 2 \\sigma^{2} } + \\frac{ \\sum_{i=1}^{n} 2 x_{i} \\mu }{ 2 \\sigma^{2} } - \\frac{ n \\mu^{2} }{ 2 \\sigma^{2} } - \\frac{ \\mu^{2} }{ 2 \\sigma_{0}^{2} } + \\frac{ 2 \\mu \\mu_{0} }{ 2 \\sigma_{0}^{2} } - \\frac{ \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } \\\\\n",
    "& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\left( \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \\right) \\mu^{2} + 2 \\left( \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \\right) \\mu - \\frac{ \\sum_{i=1}^{n} x_{i}^{2} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } & [\\text{grouping } \\mu] \\\\\n",
    "& = \\frac{ \\exp{ \\left[ - \\frac{ \\sum_{i=1}^{n} x_{i}^{2} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\left( \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \\right) \\mu^{2} + 2 \\left( \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \\right) \\mu \\right] } & [\\text{extracting out terms without } \\mu] \\\\\n",
    "& \\propto \\exp{ \\left[ - \\left( \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \\right) \\mu^{2} + 2 \\left( \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \\right) \\mu \\right] } & [\\text{removing terms without } \\mu] \\\\\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "Using the *completing the squares* trick\n",
    "    \n",
    "$$ \n",
    "\\begin{aligned}\n",
    "ax^{2} + 2bx + c  \n",
    "& = a \\left( x^{2} + 2 \\frac{ b }{ a } x + \\frac{ c }{ a } \\right) \\\\\n",
    "& = a \\left( x^{2} + 2 \\frac{ b }{ a } x + \\left( \\frac{ b }{ a } \\right)^{2} - \\left( \\frac{ b }{ a } \\right)^{2} + \\frac{ c }{ a } \\right) \\\\\n",
    "& = a \\left( x + \\frac{ b }{ a } \\right)^{2} + c - \\frac{ b^{2} }{ a }, \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and treating \n",
    "\n",
    "$$ a = - \\left( \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \\right), $$\n",
    "\n",
    "$$ b = \\left( \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \\right), $$\n",
    "\n",
    "we can have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}_{\\mu \\mid \\mathcal{D}}(\\mu \\mid \\mathcal{D}) \n",
    "& \\propto \\exp{ \\left[ \n",
    "    - \\left( \n",
    "        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \n",
    "    \\right) \\mu^{2} \n",
    "    + 2 \\left( \n",
    "        \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \n",
    "    \\right) \\mu \n",
    "\\right] } \n",
    "\\\\\n",
    "& \\propto \\exp{ \\left[\n",
    "    - \\left( \n",
    "        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \n",
    "    \\right) \n",
    "    \\left( \n",
    "        \\mu - \\frac{ \n",
    "            \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \n",
    "        }{ \n",
    "            \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \n",
    "        } \n",
    "    \\right)^{2} \n",
    "\\right] } \n",
    "& [\\text{remove } \\frac{ b^{2} }{ a } \\text{ as it doesn't depend on } \\mu] \n",
    "\\\\\n",
    "& = \\exp{ \\left[ \n",
    "    - \\left( \n",
    "        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \n",
    "    \\right) \n",
    "    \\left( \n",
    "        \\mu - \\frac{ \n",
    "            \\frac{\n",
    "                \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n",
    "            }{\n",
    "                2 \\sigma^{2} \\sigma_{0}^{2}\n",
    "            }\n",
    "        }{ \n",
    "            \\frac{ \n",
    "                \\sigma^{2} + n \\sigma_{0}^{2} \n",
    "            }{ \n",
    "                2 \\sigma^{2} \\sigma_{0}^{2} \n",
    "            } \n",
    "        } \n",
    "    \\right)^{2} \n",
    "\\right] } \n",
    "\\\\\n",
    "& = \\exp{ \\left[ \n",
    "    - \\left( \n",
    "        \\frac{ 2 \\sigma^{2} \\sigma_{0}^{2} }{ \\sigma^{2} + n \\sigma_{0}^{2} }\n",
    "    \\right)^{-1}\n",
    "    \\left( \n",
    "        \\mu - \\frac{ \n",
    "            \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n",
    "        }{ \n",
    "            \\sigma^{2} + n \\sigma_{0}^{2} \n",
    "        } \n",
    "    \\right)^{2} \n",
    "\\right] } \n",
    ".\n",
    "\\\\\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f902a46-e423-4757-9b34-df82553e9223",
   "metadata": {},
   "source": [
    "This means that the unknown parameter $\\mu$ estimated by a set of instances $\\mathcal{X}$ using Bayesian estimation has a probability $\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X})$ that follows a normal distribution that has mean $\\mu_{n}$ and variance $\\sigma_{n}^{2}$.\n",
    "\n",
    "- Since $\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X})$ is a normal distribution, $\\mathbb{P}_{\\mu \\mid X} (\\mu_{n} \\mid \\mathcal{X})$ is largest and thus we can view $\\mu_{n}$ as our best guess for $\\mu$ after observing $\\mathcal{X}$. \n",
    "\n",
    "- Then we can view the variance $\\sigma_{n}^{2}$ as the uncertainty about this best guess.\n",
    "\n",
    "- Since $\\sigma_{n}^{2}$ decreases monotonically with $n$, each additional observation decreases our uncertainty of the best guess. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40192298-7424-4a10-b79a-4225dd639151",
   "metadata": {},
   "source": [
    "%%markdown\n",
    "\n",
    "Since $\\mu_{n}$ can be further rewritten as \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\mu_{n} \n",
    "& = \\frac{ \n",
    "    \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n",
    "}{ \n",
    "    \\sigma^{2} + n \\sigma_{0}^{2} \n",
    "}\n",
    "\\\\\n",
    "& = \\frac{\n",
    "    \\sigma_{0}^{2}\n",
    "}{\n",
    "    \\sigma^{2} + n \\sigma_{0}^{2}\n",
    "} \\sum_{i=1}^{n} x_{i} + \\frac{\n",
    "    \\sigma^{2}\n",
    "}{\n",
    "    \\sigma^{2} + n \\sigma_{0}^{2}\n",
    "} \\mu_{0} \n",
    "\\\\\n",
    "& = \\frac{\n",
    "    n \\sigma_{0}^{2}\n",
    "}{\n",
    "    n \\sigma_{0}^{2} + \\sigma^{2}\n",
    "} \\bar{x}_{n} + \\left( \n",
    "    1 - \\frac{\n",
    "        n \\sigma_{0}^{2}\n",
    "    }{\n",
    "        n \\sigma_{0}^{2} + \\sigma^{2}\n",
    "    } \n",
    "\\right) \\mu_{0} \n",
    "& [\\bar{x}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}]\n",
    "\\\\\n",
    "& = \\alpha_{n} \\bar{x}_{n} + (1 - \\alpha_{n}) \\mu_{0} \n",
    "& [\\alpha_{n} = \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}}] \n",
    ",\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "the final equation shows that $\\mu_{n}$ is a combination of the maximum likelihood estimate $\\bar{x}_{n}$ and the prior information $\\mu_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec96087-9c16-44e7-9de3-b63fb08c68bb",
   "metadata": {},
   "source": [
    "Since \n",
    "\n",
    "$$  \n",
    "\\begin{aligned}\n",
    "\\lim_{n \\to \\infty} \\mu_{n}\n",
    "& = \\bar{x}_{n} \n",
    "& [\\lim_{n \\to \\infty} \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}} = 1]\n",
    "\\\\\n",
    "\\lim_{n \\to 0} \\mu_{n}\n",
    "& = \\mu_{0}\n",
    "& [\\lim_{n \\to 0} \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}} = 0],\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- If there is large amount of data, $\\mu_{n}$ converges to maximum likelihood estimate.\n",
    "\n",
    "- If there is no observed data, $\\mu_{n}$ converges to the mean of the prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b276d-7d1d-4b8f-b4c6-2bb81297b7e8",
   "metadata": {},
   "source": [
    "If the number of sampled data $n$ is fixed, \n",
    "\n",
    "$$  \n",
    "\\begin{aligned}\n",
    "\\lim_{\\sigma_{0} \\to \\infty} \\mu_{n}\n",
    "& = \\bar{x}_{n} \n",
    "& [\\lim_{\\sigma_{0} \\to \\infty} \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}} = 1]\n",
    "\\\\\n",
    "\\lim_{\\sigma_{0} \\to 0} \\mu_{n}\n",
    "& = \\mu_{0}\n",
    "& [\\lim_{\\sigma \\to \\infty} \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}} = 0],\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which means \n",
    "\n",
    "- $\\mu_{n}$ will converge to the maximum likelihood estimate $\\bar{x}_{n}$ if our prior knowledge of the $\\mu$ indicates that we have no certainty about $\\mu_{n}$ (infinite variance).\n",
    "\n",
    "- $\\mu_{n}$ will converge to the mean of our prior knowledge $\\mu_{0}$ if our prior knowledge of the $\\mu$ indicates that we are very certain about $\\mu_{0}$ (zero variance). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cff25f-60cc-4ac4-b098-79f54991f8f6",
   "metadata": {},
   "source": [
    "### Predictive distribution function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dc0d5-42e7-401b-8104-1eb6e3c40ebc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}_{X \\mid X}(x \\mid \\mathcal{X}) \n",
    "& = \\int \\mathbb{P}_{X \\mid \\mu}(x \\mid \\mu) \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) \\mathop{d \\mu}\n",
    "\\\\\n",
    "& \\sim \\mathcal{N}(\\mu_{n}, \\sigma^{2} + \\sigma_{n}^{2})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ac2ca-30b3-47cc-bbb5-a720f53677ab",
   "metadata": {},
   "source": [
    "### Selecting parameter priors\n",
    "\n",
    "When the number of samples $n$ is large, the predictive distribution will not change much if we select different mean parameter priors. Consider the 2 extreme cases of the mean parameter priors.\n",
    "\n",
    "1. Uniform prior (normal distribution with infinite variance).\n",
    "\n",
    "    Since \n",
    "    \n",
    "    $$ \n",
    "    \\begin{aligned}\n",
    "    \\lim_{\\sigma_{0}^{2} \\to \\infty} \\mu_{n} \n",
    "    & = \\bar{x}_{n},\n",
    "    \\\\\n",
    "    \\lim_{\\sigma_{0}^{2} \\to \\infty} \\sigma_{n} \n",
    "    & = \\frac{\\sigma^{2}}{n},\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "    the predictive distribution is \n",
    "    \n",
    "    $$ \n",
    "    \\mathbb{P}_{X \\mid X}(x \\mid \\mathcal{X}) \\sim \\mathcal{N} \\left( \n",
    "        \\bar{x}_{n}, \\sigma^{2} + \\frac{\\sigma^{2}}{n} \n",
    "    \\right). \n",
    "    $$\n",
    "\n",
    "2. Dirac delta prior (normal distribution with zero variance).\n",
    "\n",
    "    Since \n",
    "    \n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\lim_{\\sigma_{0} \\to 0} \\mu_{n}\n",
    "    & = \\mu_{0},\n",
    "    \\\\\n",
    "    \\lim_{\\sigma_{0} \\to 0} \\sigma_{n}\n",
    "    & = 0,\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "    the predictive distribution is \n",
    "    \n",
    "    $$ \n",
    "    \\mathbb{P}_{X \\mid X}(x \\mid \\mathcal{X}) \\sim \\mathcal{N} \\left( \n",
    "        \\mu_{0}, \\sigma^{2} \\right\n",
    "    ). \n",
    "    $$\n",
    "    \n",
    "    Since $\\mu_{0}$ is $\\bar{x}_{n}$ with extra points, $\\mu_{0} = \\bar{x}_{n}$ when $n$ is large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e1eba-25ee-48a6-84b5-a690ebf9fcbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "- https://www.bu.edu/sph/files/2014/05/Bayesian-Statistics_final_20140416.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
