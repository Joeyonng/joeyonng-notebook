{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421f4df9",
   "metadata": {},
   "source": [
    "*Updated 01-11-2023 (First commited 01-08-2023)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563a962-5d10-448c-9d8d-8d82fb79953b",
   "metadata": {},
   "source": [
    "(expectation-maximization)=\n",
    "# Expectation-maximization \n",
    "\n",
    "Expectation-maximization (EM) is an iterative algorithm that solves **maximum likelihood (ML)** problems of estimating parameters in case there are **missing or latent (unobserved) variables** (features or labels). \n",
    "EM iterates between E-step and M-step until the convergence criterion is met. \n",
    "In E-step, the expected value of the complete data log-likelihood with respect to the probability of the missing variables is computed as a function of the unknown parameters. \n",
    "In M-step, we choose the parameters that maximize the expected value derived in the E-step.\n",
    "EM is proved to converge to a local minimum while the global minimum is not guaranteed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7177fab-869f-47aa-99a1-1a79c2c58fa2",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### Statistics \n",
    "\n",
    "- [KL-divergence]()\n",
    "\n",
    "- [Entropy]()\n",
    "\n",
    "### Statistical Learning\n",
    "\n",
    "- [Maximum Likelihood Estimation](maximum-likelihood-estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df1dd7-a703-46b3-b76e-b8bd5e2cae58",
   "metadata": {},
   "source": [
    "## The EM algorithm\n",
    "\n",
    "### Problem statements\n",
    "\n",
    "In EM problems, we have two types of variables:\n",
    "\n",
    "- Observed variables: a set of random variables $\\mathbf{X}$ from which we can draw samples.\n",
    "\n",
    "- Hidden variables: another set of random variables $\\mathbf{Z}$ from which we cannot draw samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e622e88-d93a-4250-940a-6adab73815ed",
   "metadata": {},
   "source": [
    "Although we cannot draw samples from $\\mathbf{Z}$, we assume there exists a $\\mathbf{z}$ for each $\\mathbf{x}$ observed. Thus we define two types of datasets.\n",
    "\n",
    "- Incomplete data: samples drawn from the observed variables\n",
    "\n",
    "    $$ \n",
    "    \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}. \n",
    "    $$\n",
    "\n",
    "- Complete data: the sample pairs of the observed and hidden variables that we never be able to observe\n",
    "\n",
    "    $$ \n",
    "    \\mathcal{X}, \\mathcal{Z} = \\{ (\\mathbf{x}_{1}, \\mathbf{z}_{1}), \\dots, (\\mathbf{x}_{n}, \\mathbf{z}_{n}) \\}. \n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637ecd9-c863-47ce-a3fd-02a34741f1f7",
   "metadata": {},
   "source": [
    "Given the incomplete data $\\mathcal{X}$, the goal is to find the **maximum likelihood estimate** of parameters $\\Psi$ for the log-likelihood of the **incomplete data** $\\mathcal{X}$\n",
    "\n",
    "$$ \n",
    "\\Psi^{*} = \\arg\\max_{\\Psi} \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi). \n",
    "$$\n",
    "\n",
    "- If we knew what the model $\\mathbb{P}_{\\mathbf{X}} (\\mathbf{x})$ is without introducing other variables, the problem is a standard MLE problem and usually can be easily solved.\n",
    "    \n",
    "- If instead the joint probability $\\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathbf{x}, \\mathbf{z})$ is known or can be derived, we can still solve the problem by maximizing the equation that marginalizes out the latent variables from the joint probability\n",
    "\n",
    "    $$\n",
    "    \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) = \\log \\int \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n",
    "        \\mathcal{X}, \\mathcal{Z}; \\Psi\n",
    "    ) \\partial \\mathbf{z},\n",
    "    $$\n",
    "    \n",
    "    which in most of the cases doesn't have a closed-form solution and thus needs to be solved using iterative algorithms. \n",
    "\n",
    "- EM is one of such algorithm with some unique properties that other algorithms don't have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75fc4fe-de6f-42b2-9339-3bb994e8a3a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EM procedures\n",
    "\n",
    "#### 1. E-Step\n",
    "\n",
    "Given parameters $\\hat{\\Psi}$ estimated at the current iteration and incomplete data $\\mathcal{X}$, compute the $Q$ function, which is the expected value of the log likelihood of the complete data $\\mathcal{X}, \\mathcal{Z}$ over the distribution of $\\mathbf{Z}$\n",
    "\n",
    "$$ \n",
    "Q_{\\hat{\\Psi}} (\\Psi) = \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[ \n",
    "    \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z} ; \\Psi) \\mid \\mathcal{X}\n",
    "\\right]. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616c218-1a32-46fc-b024-fbcbda79e853",
   "metadata": {},
   "source": [
    "- Since $\\mathcal{X}$ is given but we never be able to see $\\mathcal{Z}$, the log-likelihood is a function of both $\\Psi$ and $\\mathbf{Z}$,\n",
    "\n",
    "    $$ \n",
    "    L(\\mathcal{Z}, \\Psi) = \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \\mid \\mathcal{X}. \n",
    "    $$\n",
    "\n",
    "- The $Q$ function only depend on $\\Psi$ but not $\\mathcal{Z}$ because taking the expected value of $L(\\mathcal{Z}, \\Psi)$ over the distribution of $\\mathbf{Z}$ eliminates $\\mathcal{Z}$ in the expression.\n",
    "\n",
    "- The estimate of the parameter $\\hat{\\Psi}$ in the current iteration is used in calculating the probability of $\\mathcal{Z}$ given $\\mathcal{X}$, which is needed in the expectation calculation process. \n",
    "\n",
    "- Depending the problem setups, the expressions for both $\\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)$ and $\\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi})$ (used in $\\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}; \\hat{\\Psi}} [\\cdot]$) should be already known or can be derived from the known probabilistic models between $\\mathbf{X}$ and $\\mathbf{Z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea0bc5-974f-437d-8c1c-60a19d6c2346",
   "metadata": {},
   "source": [
    "#### 2. M-Step\n",
    "\n",
    "Find the parameter $\\hat{\\Psi}$ that maximizes the expected value derived in the E-step. \n",
    "\n",
    "$$ \n",
    "\\hat{\\Psi} = \\arg\\max_{\\Psi} Q_{\\hat{\\Psi}} (\\Psi) \n",
    "$$\n",
    "    \n",
    "Repeat procedure 1 and 2 until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cabf8e-ab16-4b66-99d0-3e618e28ea1e",
   "metadata": {},
   "source": [
    "## Derivation of EM\n",
    "\n",
    "The log-likelihood of the incomplete data can be decomposed into 2 components\n",
    "\n",
    "$$ \n",
    "\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) = G (\\Psi, q) +  D (\\Psi, q)\n",
    "$$\n",
    "\n",
    "where $q$ is an arbitrary distribution of the latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d5324-30b5-405b-8d56-e8a3b8e3a4ae",
   "metadata": {},
   "source": [
    "- The lower bound of EM\n",
    "\n",
    "    $$ \n",
    "    G (\\Psi, q) = \\mathbb{E}_{\\mathbf{Z}} \\left[\n",
    "        \\log \\frac{\n",
    "            \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \n",
    "        }{\n",
    "            q (\\mathcal{Z})\n",
    "        } \n",
    "    \\right]\n",
    "    $$\n",
    "    \n",
    "- The KL-divergence of between two distributions of $\\mathbf{Z}$, i.e. $q(\\mathcal{Z})$ and $\\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)$\n",
    "\n",
    "    $$\n",
    "    D (\\Psi, q) = \\mathrm{KL} \\left[ \n",
    "        q \\mathrel{\\Vert} \\mathbb{P}_{\\mathbf{X} \\mid \\mathbf{Z}} \n",
    "    \\right] = \\mathbb{E}_{\\mathbf{Z}} \\left[ \n",
    "        \\log \\frac{\n",
    "            q (\\mathcal{Z})\n",
    "        }{\n",
    "            \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n",
    "        }\n",
    "    \\right].\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de31525-d92e-42b2-b585-cfdb7cb2503b",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::{admonition} Proof: $\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) = Q (\\Psi, q) + \\mathrm{H} (q) + D (\\Psi, q)$\n",
    ":class: dropdown\n",
    "\n",
    "Since $\\log \\mathbb{P}_\\mathbf{X} (\\mathbf{x})$ is not dependent on $\\mathbf{Z}$, taking its expectation over any distribution of $\\mathbf{Z}$ is equal to itself. Thus, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) \n",
    "& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n",
    "    \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) \n",
    "\\right]\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n",
    "    \\log \\frac{\n",
    "        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \n",
    "    }{\n",
    "        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n",
    "    }\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n",
    "    \\log \\frac{\n",
    "        q (\\mathcal{Z})\n",
    "    }{\n",
    "        q (\\mathcal{Z})\n",
    "    } \\frac{\n",
    "        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \n",
    "    }{\n",
    "        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n",
    "    }\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n",
    "    \\log \\frac{\n",
    "        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \n",
    "    }{\n",
    "        q (\\mathcal{Z})\n",
    "    } \\frac{\n",
    "        q (\\mathcal{Z})\n",
    "    }{\n",
    "        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n",
    "    }\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n",
    "    \\log \\frac{\n",
    "        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \n",
    "    }{\n",
    "        q (\\mathcal{Z})\n",
    "    } \n",
    "\\right] + \\mathbb{E}_{\\mathbf{Z}} \\left[ \n",
    "    \\log \\frac{\n",
    "        q (\\mathcal{Z})\n",
    "    }{\n",
    "        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n",
    "    }\n",
    "\\right]\n",
    "\\\\\n",
    "& = G (\\Psi, q) + D (\\Psi, q)\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n",
    "    \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \n",
    "\\right] - \\mathbb{E}_{\\mathbf{Z}} \\left[\n",
    "    \\log q (\\mathcal{Z})\n",
    "\\right] + \\mathbb{E}_{\\mathbf{Z}} \\left[ \n",
    "    \\log \\frac{\n",
    "        q (\\mathcal{Z})\n",
    "    }{\n",
    "        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n",
    "    }\n",
    "\\right]\n",
    "\\\\\n",
    "& = Q (\\Psi, q) + \\mathrm{H} (q) + D (\\Psi, q)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3b38a-d808-4088-91e4-1effec9ac727",
   "metadata": {},
   "source": [
    "### Lower bound\n",
    "\n",
    "Since KL-divergence $D (\\Psi, q)$ is proved to be non-negative, $G(\\Psi, q)$ can be seen as a lower bound of $\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)$\n",
    "\n",
    "$$\n",
    "\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) \\geq G(\\Psi, q),\n",
    "$$\n",
    "\n",
    "which can also be proven using Jensen's inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c0af4-229d-4efe-aa36-da816fe4f7d2",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::{admonition} Proof: $\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) \\geq G(\\Psi, q)$ using Jensen's inequality.\n",
    ":class: dropdown\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) \n",
    "& = \\log \\int \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n",
    "    \\mathcal{X}, \\mathcal{Z}; \\Psi\n",
    ") \\partial \\mathbf{z}\n",
    "\\\\\n",
    "& = \\log \\int q (\\mathcal{Z}) \\frac{\n",
    "    \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n",
    "        \\mathcal{X}, \\mathcal{Z}; \\Psi\n",
    "    )\n",
    "}{\n",
    "    q (\\mathcal{Z})\n",
    "} \\partial \\mathbf{z}\n",
    "\\\\\n",
    "& = \\log \\mathbb{E}_{\\mathbf{Z}} \\left[\n",
    "    \\frac{\n",
    "        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n",
    "            \\mathcal{X}, \\mathcal{Z}; \\Psi\n",
    "    )\n",
    "    }{\n",
    "        q (\\mathcal{Z})\n",
    "    }\n",
    "\\right] \n",
    "\\\\\n",
    "& \\geq \\mathbb{E}_{\\mathbf{Z}} \\left[ \\log\n",
    "    \\frac{\n",
    "        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n",
    "            \\mathcal{X}, \\mathcal{Z}; \\Psi\n",
    "        )\n",
    "    }{\n",
    "        q (\\mathcal{Z})\n",
    "    }\n",
    "\\right] \n",
    "\\\\\n",
    "& \\geq G (\\Psi, q) \n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf5acc-2310-4992-9903-e6fe4b050695",
   "metadata": {},
   "source": [
    "### EM as coordinate ascent on lower bound\n",
    "\n",
    "The EM is essentially doing **coordinate ascent** on $G (\\Psi, q)$, which is believed to be easier to optimize than directly optimizing $\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)$, while guaranteeing the $\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)$ is non-decreasing as $G (\\Psi, q)$ is optimized.\n",
    "\n",
    "Coordinate ascent is a optimization method that optimize a single variable or 1 dimension of the variable at a time, while fixing the values of the rest of the variables from the last iteration unchanged. \n",
    "In the case of applying to $G (\\Psi, q)$ function, $\\Psi$ and $q$ are separately maximized in different steps of each iteration. \n",
    "\n",
    "- E-step: given the parameter $\\hat{\\Psi}$ estimated in the last iteration, the choice of the $q$ function is optimized to maximize the value of $G (\\Psi, q)$.\n",
    "\n",
    "- M-step: given the $\\hat{q}$ function selected in E-step, $\\Psi$ is optimized to maximize the value of $G (\\Psi, \\hat{q})$ and will be used in the E-step of the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a295f-5a74-4ed6-a8e1-be3e4d0235ea",
   "metadata": {},
   "source": [
    "### E-step\n",
    "\n",
    "Since the value of $\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)$ doesn't depend on the choice of $q$, the choice of $q$ only affect the balance between $G (\\Psi, q)$ and $D (\\Psi, q)$ when the $\\Psi$ is fixed. \n",
    "\n",
    "Thus, given the parameters $\\hat{\\Psi}$ estimated in the last iteration, $G (\\hat{\\Psi}, q)$ is maximized with respect to $q$ when $D (\\hat{\\Psi}, q)$ is minimized. \n",
    "Since the minimized value of $D (\\Psi, q)$ is 0, we have \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D (\\hat{\\Psi}, q) \n",
    "& = 0\n",
    "\\\\\n",
    "\\mathbb{E}_{\\mathbf{Z}} \\left[ \n",
    "    \\log \\frac{\n",
    "        q (\\mathcal{Z})\n",
    "    }{\n",
    "        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi})\n",
    "    }\n",
    "\\right]\n",
    "& = 0\n",
    "\\\\\n",
    "q (\\mathcal{Z}) \n",
    "& = \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which shows that $G (\\hat{\\Psi})$ is maximized when the distribution of latent variables is chosen to be the probability of the latent variables given the observed data and the current estimate of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5366e1-a2d0-4a9e-a231-bbc25e73ac7d",
   "metadata": {},
   "source": [
    "A nice property of optimizing $q$, even though it doesn't affect the value of $\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)$ at all, is that the value of KL-divergence $D (\\Psi, \\hat{q})$ will also be non-decreasing no matter what $\\Psi$ is selected in the M-step by maximizing $G (\\Psi, \\hat{q})$. \n",
    "\n",
    "- This can be seen from the fact that $D (\\Psi, \\hat{q}) = 0$ when $\\hat{p}$ is selected to maximize $G (\\hat{\\Psi}, q)$, and thus any $\\Psi$ will guarantee that the value of $D (\\Psi, \\hat{q})$ is larger or equal to 0.\n",
    "\n",
    "- This property implicitly prove the convergence of the EM algorithm in that both $G (\\Psi, q)$ and $D (\\Psi, q)$ will be non-decreasing during each iteration, and therefore, the value of $\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)$ is non-decreasing in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a129fe-139f-46f4-b8ad-4a3b4a2dada5",
   "metadata": {},
   "source": [
    "### M-step\n",
    "\n",
    "$G (\\Psi, q)$ can be further decomposed into two components\n",
    "\n",
    "$$\n",
    "G (\\Psi, q) = Q (\\Psi, q) + \\mathrm{H} (q).\n",
    "$$\n",
    "\n",
    "- The expected value of the complete data with respect to the distribution $q$\n",
    "\n",
    "    $$ \n",
    "    Q (\\Psi, q) = \\mathbb{E}_{\\mathbf{Z}} \\left[ \n",
    "        \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \n",
    "    \\right].\n",
    "    $$\n",
    "\n",
    "- The entropy of the latent variables\n",
    "\n",
    "    $$\n",
    "    \\mathrm{H} (q) = - \\mathbb{E}_{\\mathbf{Z}} \\left[ \n",
    "        \\log q (\\mathcal{Z}) \n",
    "    \\right].\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf513b84-c81d-41a3-8f55-3c5738f0b080",
   "metadata": {},
   "source": [
    "Since $\\mathrm{H} (q)$ doesn't depend on $\\Psi$, it will stay as a constant in the process of maximizing $G (\\Psi, q)$ with respect to $\\Psi$.\n",
    "\n",
    "Given $\\hat{q} = \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi})$, maximizing $G (\\Psi, \\hat{q})$ is the same as maximizing $Q$ function we defined above:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\arg\\max_{\\Psi} G (\\Psi, \\hat{q}) \n",
    "& = \\arg\\max_{\\Psi} Q_{\\hat{\\Psi}} (\\Psi) \n",
    "\\\\\n",
    "& = \\arg\\max_{\\Psi} \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n",
    "    \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z} ; \\Psi)\n",
    "\\right]. \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b15ad92-d1e5-41dc-aa96-7d0f5b0d65d0",
   "metadata": {},
   "source": [
    "## Example: mixture model\n",
    "\n",
    "One application of EM algorithm is to obtain MLE of the parameters in a mixture models. \n",
    "\n",
    "### Mixture model\n",
    "\n",
    "We say random variable $\\mathbf{X}$ follows a mixture model if its distribution is a weighted combination of multiple components, where each component has a simple parametric distributions. Thus mixture model can represent distributions that cannot be expressed using a single parametric distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eec519-f252-4b85-b8aa-eb4ff4be1437",
   "metadata": {
    "tags": []
   },
   "source": [
    "Each sample $\\mathbf{x}$ is associated with a latent random variable $z$ that indicates which component (parametric distribution) that $\\mathbf{x}$ should be drawn. Thus the sample $\\mathbf{x}$ has the conditional probability in a parametric form with parameters $\\boldsymbol{\\theta}$\n",
    "\n",
    "$$ \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x} \\mid z ; \\boldsymbol{\\theta}) $$\n",
    "\n",
    "if we know the latent variable $z$ for the sample. \n",
    "\n",
    "Assuming in total we have $c$ latent variables for all samples and each latent variable has the probability $\\mathbb{P}_{Z} (z)$, the probability of the sample is \n",
    "\n",
    "$$ \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}) = \\sum_{z=1}^{c} \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x} \\mid z) \\mathbb{P}_{Z} (z). $$\n",
    "\n",
    "The Gaussian mixture model is simply a mixture model in which all components are Gaussian distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe9596-184a-4810-98b9-f49335e107d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EM for mixture model\n",
    "\n",
    "If we knew what $z$ is for each $\\mathbf{x}$, the estimate of parameters for each component can be easily derived by sampling a dataset $\\mathcal{X}_{z}$ from the conditional distribution and applying MLE. \n",
    "\n",
    "However, in practice, we never know which component each sample belongs to, and thus we apply EM by treating $\\mathbf{X}$ as observed variables and $Z$ as the hidden variable. \n",
    "\n",
    "The goal of applying EM is to find the parameters $\\Psi$ in the mixture model including: \n",
    "\n",
    "- The parameters for the parametric distribution of each component $\\{ \\boldsymbol{\\theta}_{1}, \\dots, \\boldsymbol{\\theta}_{c} \\}$.\n",
    "\n",
    "- The probability of each component $\\{ \\pi_{1}, \\dots, \\pi_{c} \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded3f5f-c179-47e6-a500-b8195a6bba18",
   "metadata": {},
   "source": [
    "#### E-step: complete data likelihood\n",
    "\n",
    "To derive the EM procedure, we first need to write out the log-likelihood of the complete data in terms of the known parametric distributions\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "L(\\mathcal{Z}, \\Psi) \n",
    "& = \\log \\mathbb{P}_{\\mathbf{X}, Z} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n",
    "\\\\\n",
    "& = \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathcal{X} \\mid \\mathcal{Z} ; \\boldsymbol{\\theta}) \\mathbb{P}_{Z} (\\mathcal{Z})\n",
    "\\\\ \n",
    "& = \\log \\prod_{i=1}^{n} \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid z_{i} ; \\boldsymbol{\\theta}_{z_{i}}) \\pi_{z_{i}}\n",
    "& [\\text{i.i.d assumption}],\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $x_{i}$ is a sample, $z_{i}$ indicates the component that $x_{i}$ belongs to, $\\boldsymbol{\\theta}_{z_{i}}$ is the parameters of the component $z_{i}$, and $\\pi_{z_{i}}$ is the probability of the component $z_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae7cce-05fd-4e42-aa41-fb660dcd3247",
   "metadata": {},
   "source": [
    "Since $z$ is discrete and range from $1$ to $c$, any function of $z$ can be written as\n",
    "\n",
    "$$ f(z) = \\prod_{i=1}^{c} f(i)^{\\mathbb{1}(z = i)}, $$\n",
    "\n",
    "where $z$ is extracted out from the function to the power of the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db819644-ae58-4ca2-bba1-7f1c82f6ef76",
   "metadata": {},
   "source": [
    "Thus, the complete data likelihood can be further simplified to\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\mathcal{Z}, \\Psi) \n",
    "& = \\log \\prod_{i=1}^{n} \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid z_{i} ; \\boldsymbol{\\theta}_{z_{i}}) \\pi_{z_{i}}\n",
    "\\\\\n",
    "& = \\log \\prod_{i=1}^{n} \\prod_{j=1}^{c} \\left[\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}) \\pi_{j}\n",
    "\\right]^{\\mathbb{1}(z_{i} = j)}\n",
    "& [f(z_{i}) = \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid z_{i} ; \\boldsymbol{\\theta}_{z_{i}}) \\pi_{z_{i}}]\n",
    "\\\\\n",
    "& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\log \\left[\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}) \\pi_{j}\n",
    "\\right]^{\\mathbb{1}(z_{i} = j)}\n",
    "\\\\\n",
    "& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\mathbb{1}(z_{i} = j) \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}) \\pi_{j}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30848fdc-c8ce-474a-a4db-1920eb993d38",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### E-step: $Q$ function\n",
    "\n",
    "Taking the expectation of complete data log-likelihood over $Z$ gives us the $Q$ function that doesn't depend on $Z$  \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "Q_{\\hat{\\Psi}} (\\Psi) \n",
    "& = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n",
    "    L(\\mathcal{Z}, \\Psi) \n",
    "\\right]\n",
    "\\\\\n",
    "& = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n",
    "    \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\mathbb{1}(z_{i} = j) \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n",
    "        \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n",
    "    \\right) \\pi_{j}\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[ \n",
    "    \\mathbb{1}(z_{i} = j) \n",
    "\\right] \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n",
    "    \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n",
    "\\right) \\pi_{j}\n",
    "\\\\\n",
    "& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} h_{i, j} \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n",
    "    \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n",
    "\\right) \\pi_{j},\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $h_{i, j} = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[ \\mathbb{1}(z_{i} = j) \\right] $ is a constant value that can be computed given that we have parameter $\\hat{\\Psi}$ estimated in the last iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038da0b0-05e3-44f3-8088-a44ca920153e",
   "metadata": {},
   "source": [
    "%%markdown\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_{i, j}\n",
    "& = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n",
    "    \\mathbb{1} (z_{i} = j)\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\sum_{k=1}^{c} \\mathbb{P}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left(\n",
    "    k \\mid \\mathbf{x}_{i}\n",
    "\\right)\n",
    "\\mathbb{1} (k = j)\n",
    "\\\\\n",
    "& = \\mathbb{P}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left(\n",
    "    j \\mid \\mathbf{x}_{i}\n",
    "\\right)\n",
    "& [\\mathbb{1} (k = j) = 0 \\text{ for } k \\neq j]\n",
    "\\\\\n",
    "& = \\frac{\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n",
    "        \\mathbf{x}_{i} \\mid j; \\hat{\\boldsymbol{\\theta}}_{j}\n",
    "    \\right) \\hat{\\pi}_{j}\n",
    "}{\n",
    "    \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}_{i})\n",
    "}\n",
    "& [\\text{Bayes' Theorem}]\n",
    "\\\\\n",
    "& = \\frac{\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n",
    "        \\mathbf{x}_{i} \\mid j ; \\hat{\\boldsymbol{\\theta}}_{j}\n",
    "    \\right) \\hat{\\pi}_{j}\n",
    "}{\n",
    "    \\sum_{k=1}^{c} \\mathbb{P}_{\\mathbf{X}, Z} \\left(\n",
    "        \\mathbf{x}_{i}, k\n",
    "    \\right)\n",
    "}\n",
    "& [\\text{marginalization}]\n",
    "\\\\\n",
    "& = \\frac{\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n",
    "        \\mathbf{x}_{i} \\mid j; \\hat{\\boldsymbol{\\theta}}_{j}\n",
    "    \\right) \\hat{\\pi}_{j}\n",
    "}{\n",
    "    \\sum_{k=1}^{c} \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n",
    "        \\mathbf{x}_{i} \\mid k; \\hat{\\boldsymbol{\\theta}}_{k}\n",
    "    \\right) \\hat{\\pi}_{k}\n",
    "}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14937e9b-d7df-4c81-8e98-b23185d3ad41",
   "metadata": {},
   "source": [
    "#### M-step: maximizes $Q$ function\n",
    "\n",
    "Computing $\\hat{\\Psi}_{\\text{new}}$ that maximizes $Q$ function for the next iteration is an optimization problem\n",
    "\n",
    "$$ \n",
    "\\hat{\\Psi}_{\\text{new}} = \\arg\\max_{\\Psi} \\sum_{i=1}^{n} \\sum_{j=1}^{c} h_{i, j} \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n",
    "    \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n",
    "\\right) \\pi_{j},\n",
    "$$\n",
    "\n",
    "which can usually be solved analytically depending on the mathematical form of the parametric distribution of the component $\\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x} \\mid z)$.\n",
    "\n",
    "Again, $\\hat{\\Psi}_{\\text{new}}$ is the estimated parameters that include \n",
    "\n",
    "- parameters $\\{ \\hat{\\boldsymbol{\\theta}}_{1}, \\dots, \\hat{\\boldsymbol{\\theta}}_{c} \\}$ in the $c$ components of the mixture model.\n",
    "\n",
    "- probability parameters $\\{ \\hat{\\pi}_{1}, \\dots, \\hat{\\pi}_{c} \\}$ of $c$ components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e02ab8-2880-446b-a6ad-d9274aa2e0d9",
   "metadata": {},
   "source": [
    "## Reference \n",
    "\n",
    "- http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf\n",
    "- https://gregorygundersen.com/blog/2019/11/10/em/\n",
    "- https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1268&context=gc_cs_tr\n",
    "- https://mbernste.github.io/posts/em/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f16b7e4d9ebe7317f950c4b0fbd4eebd34b61d616679c6e2b8cfe795bbc808e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
