{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e65806",
   "metadata": {},
   "source": [
    "*Updated 01-11-2023 (First commited 01-08-2023)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbca415-32da-4fce-978e-824a34e64d9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "(bayesian-decision-theory)=\n",
    "# Bayesian Decision Theory (BDT)\n",
    "\n",
    "**Bayesian decision theory** is a **statistical** view to the machine learning problems. \n",
    "It makes the assumption that the decision problem is posed in probabilistic terms, and that all of the relevant probability values are known.\n",
    "The classifiers obtained under the framework of BDT will always give the best decision rule for each given test instance to minimize the expected total cost defined by the loss function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be11a7-c69d-4f8e-afa0-8deec9c81235",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### Statistics \n",
    "\n",
    "- [Joint Probability]()\n",
    "\n",
    "- [Chain rule (probability)]()\n",
    "\n",
    "- [Bayes Theorem]()\n",
    "\n",
    "- [The log trick]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c46b3-4fe3-4de0-ac81-57f3760ece93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic concepts\n",
    "\n",
    "### Probability view of machine learning\n",
    "\n",
    "When modeling a machine learning problem in the probability setting, both instances $\\mathbf{x}$ and labels $y$ are sampled from different random variables. \n",
    "\n",
    "- All instances with $d$ features are sampled from a random process of $d$ random variables $\\mathbf{X} = \\{ X_{1}, \\dots, X_{d} \\}$.\n",
    "\n",
    "- All possible labels are also sampled from a random variable $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789dd4f-d68c-441a-a26e-f2045700d2f4",
   "metadata": {},
   "source": [
    "Thus, there is always a probability associated with each term:\n",
    "\n",
    "- $\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x})$: the probability that the instance $\\mathbf{x}$ happens in the real world (the joint probability of different features that happen in the real world).\n",
    "\n",
    "- $\\mathbb{P}_{Y}(y)$: the probability that label $y$ happens in the real world.\n",
    "\n",
    "- $\\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}, y)$: the joint probability that the both $\\mathbf{x}$ and $y$ happens in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f9204-0ea1-4185-a59b-45f71b4bedee",
   "metadata": {},
   "source": [
    "We are particularly interested in $\\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}, y)$, as we can know what should be the correct label $y_{t}$ for the test instance $\\mathbf{x}_{t}$ by selecting $y$ that has the highest $\\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}_{t}, y)$.\n",
    "\n",
    "We can decompose the joint probability according to the chain rule:\n",
    "\n",
    "$$ \n",
    "\\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}, y) = \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y), \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y)$ is called class conditional probability, which gives the probability of the instance if we know the label is $y$.\n",
    "\n",
    "- $\\mathbb{P}_{Y}(y)$ is the class probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434eb5b-c2cc-4c63-8d76-e00f51ba013b",
   "metadata": {},
   "source": [
    "### Decision function\n",
    "\n",
    "Given an instance $\\mathbf{x}$, the decision function $g(\\cdot)$ determines its label $\\hat{y}$ according to some rules. \n",
    "\n",
    "$$ \n",
    "\\hat{y} = g(\\mathbf{x}). \n",
    "$$\n",
    "\n",
    "### Loss function\n",
    "\n",
    "Given two labels, which usually are the predicted label from the decision function $\\hat{y}$ and an arbitrary label $y$, the loss function $L(\\hat{y}, y)$ defines the cost of predicting label $\\hat{y}$ with respect to the label $y$.  \n",
    "\n",
    "- The cost returned by loss functions should be a non-negative value.\n",
    "\n",
    "- For classification problems where the label is a discrete random variable, the loss function can be specified by a matrix $\\mathbf{L} = \\mathbb{R}^{d \\times d}$, where the cost of predicting label 1 with respect to the label 2 is $\\mathbf{L}_{1, 2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5f172-c882-46d6-8079-7cce106162f5",
   "metadata": {},
   "source": [
    "## Bayes decision rule\n",
    "\n",
    "### Risk\n",
    "\n",
    "Assuming we have a probability model $\\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}, y)$ of the joint probability of $\\mathbf{X}$ and $Y$, the **risk** function of the decision function $g$ is defined as the expectation of the loss function over the joint probability\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R(g) \n",
    "& = \\mathbb{E}_{\\mathbf{X}, Y} \\left[\n",
    "    L (g(\\mathbf{x}), y)\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\int \\int \\mathbb{P}_{\\mathbf{X}, Y} (\\mathbf{X}, y) L (g(x), y) \\mathop{d \\mathbf{x}} \\mathop{dy}\n",
    "& [\\text{definition of expectation}]\n",
    "\\\\\n",
    "& = \\int \\int \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{X}) \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}) L (g(x), y) \\mathop{d \\mathbf{x}} \\mathop{dy}\n",
    "& [\\text{probability chain rule}]\n",
    "\\\\\n",
    "& = \\int \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}) \\int \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{X}) L (g(x), y) \\mathop{dy} \\mathop{d \\mathbf{x}}\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\mathbf{X}} \\left[\n",
    "    \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "        L (g(x), y)\n",
    "    \\right]\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\mathbb{E}_{\\mathbf{X}} \\left[\n",
    "    R \\left(\n",
    "        \\mathbf{x}, g\n",
    "    \\right)\n",
    "\\right],\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $R(\\mathbf{x}, g)$ is **conditional risk** (Bayes risk?), which is the risk given that $\\mathbf{x}$ is known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663bb550-61f4-42ce-8a99-f8421d62f584",
   "metadata": {},
   "source": [
    "### Bayes decision rule (BDR)\n",
    "\n",
    "**Bayes decision rule** is the particular decision function $g^{*}(\\mathbf{x})$ that minimizes the risk\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "g^{*} (\\mathbf{x}) \n",
    "& = \\arg\\min_{g (\\mathbf{x})} R (g)\n",
    "\\\\\n",
    "& = \\arg\\min_{g (\\mathbf{x})} R (\\mathbf{x}, g) & [\\text{only } R (\\mathbf{x}, g) \\text{ contains } g (\\mathbf{x})].\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The risk that Bayes decision rule achieves is called **Bayes Risk**, which is the minimum risk that any decision function can achieve, if we know the true probability model and its parameters $\\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966de14b-f4db-417f-bc4e-9ec928b4eb1f",
   "metadata": {},
   "source": [
    "## Example: BDR with 0-1 loss\n",
    "\n",
    "Often time, we are dealing with the classification problem where $\\mathbf{X}$ is a group of continuous random variables and $Y$ is a discrete random variable with $m$ unique values. \n",
    "0-1 loss is frequently used for the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02044d47-bf78-4fdc-b6dd-a86815ad465c",
   "metadata": {},
   "source": [
    "### 0-1 loss\n",
    "\n",
    "The 0-1 loss is a simple and robust loss function for the classification problems. The 0-1 loss function can be written as:\n",
    "\n",
    "$$ \n",
    "L(g(\\mathbf{x}), y) = \n",
    "\\begin{cases}\n",
    "1 & g(\\mathbf{x}) \\neq y \\\\\n",
    "0 & g(\\mathbf{x}) = y, \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which can also be written as a matrix of $\\mathbb{R}^{m}$, where the entries in the diagonal are all $0$ ($g(\\mathbf{x}) = y$) and rest are all $1$ ($g(\\mathbf{x}) \\neq y$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f6785-f723-4594-90c6-d8aa2ef6934f",
   "metadata": {},
   "source": [
    "### MAP rule\n",
    "\n",
    "If we choose 0-1 loss as the loss function for BDR, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g^{*} (\\mathbf{x}) \n",
    "& = \\arg\\min_{g(\\mathbf{x})} \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "    L (g(x), y)\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\arg\\min_{g (\\mathbf{x})} \\sum_{y=1}^{m} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x}) L (g(\\mathbf{x}), y) \n",
    "\\\\\n",
    "& = \\arg\\min_{g (\\mathbf{x})} \\sum_{y = g (\\mathbf{x})}^{m} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x}) \\times 0 + \\sum_{y \\neq g(\\mathbf{x})}^{m} \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x}) \\times 1 \n",
    "\\\\\n",
    "& = \\arg\\min_{g (\\mathbf{x})} \\sum_{y \\neq g (\\mathbf{x})}^{m} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x}) \n",
    "\\\\\n",
    "& = \\arg\\min_{g (\\mathbf{x})} 1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (g (\\mathbf{x}) \\mid \\mathbf{x}) \n",
    "\\\\\n",
    "& = \\arg\\max_{g (\\mathbf{x})} \\mathbb{P}_{Y \\mid \\mathbf{X}} (g (\\mathbf{x}) \\mid \\mathbf{x}) & [\\arg\\min_{x} (1 - f(x)) = \\arg\\max_{x} (f(x))] \n",
    "\\\\\n",
    "& = \\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since the last equation is maximizing the posterior probability according to Bayes Theorem, the optimal decision rule for 0-1 loss is also called **maximum a-posteriori probability (MAP) rule**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ccea0-23f5-47a1-91a6-ade63d205d03",
   "metadata": {},
   "source": [
    "According to Bayes Theorem, \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x}) \n",
    "& = \\arg\\max_{y} \\frac{\\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)}{\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x})} \n",
    "\\\\\n",
    "& = \\arg\\max_{y} \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y) & [\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}) \\text{ doesn't depend on } y], \n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "MAP rule can thus be computed using the class conditional probability (likelihood) and the class probability (prior). \n",
    "\n",
    "In practice, the class conditional probability and class probability can be more easily obtained from the data than the posterior probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a0c02-9c4c-4b8c-b003-1e6ab53d5734",
   "metadata": {},
   "source": [
    "### The log trick\n",
    "\n",
    "Using the log trick, the BDR for 0-1 loss is often calculated using: \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\arg\\max_{y} \\ln \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x}) \n",
    "& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y) \n",
    "\\\\\n",
    "& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) + \\ln \\mathbb{P}_{Y}(y).\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a76a8e-f9cb-460b-9681-593291059190",
   "metadata": {},
   "source": [
    "## Example: BDR with squared error loss\n",
    "\n",
    "For regression problems where the labels are continuous values, a common loss function is squared error loss\n",
    "\n",
    "$$\n",
    "L (g(x), y) = (g(x) - y)^{2}.\n",
    "$$\n",
    "\n",
    "Plug the loss function in BDR\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g^{*} (\\mathbf{x}) \n",
    "& = \\arg\\min_{g(\\mathbf{x})} \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "    L (g(x), y)\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\arg\\min_{g(\\mathbf{x})} \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "    (g (\\mathbf{x}) - y)^{2}\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\arg\\min_{g(\\mathbf{x})} \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "    (g (\\mathbf{x})^{2} - 2 g (\\mathbf{x})^{2} y + y^{2}\n",
    "\\right]\n",
    "\\\\\n",
    "& = \\arg\\min_{g(\\mathbf{x})} g (\\mathbf{x})^{2} - 2 g (\\mathbf{x}) \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "    y\n",
    "\\right] + \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "    y^{2}\n",
    "\\right].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The minimization problem can be solved by setting the its derivative w.r.t $g (\\mathbf{x})$ to 0\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\n",
    "    \\mathop{d}\n",
    "}{\n",
    "    \\mathop{d g (\\mathbf{x})} \n",
    "} \\left[\n",
    "    g (\\mathbf{x})^{2} - 2 g (\\mathbf{x}) \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "        y\n",
    "    \\right] + \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "        y^{2}\n",
    "    \\right]\n",
    "\\right] \n",
    "& = 0\n",
    "\\\\\n",
    "2 g (\\mathbf{x}) - 2 \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "    y\n",
    "\\right]\n",
    "& = 0\n",
    "\\\\\n",
    "g (\\mathbf{x}) \n",
    "& = \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n",
    "    y\n",
    "\\right]\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18f83a-81a1-4fac-a2cf-ab4f7a932ff2",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- http://pillowlab.princeton.edu/teaching/mathtools16/slides/lec18_BayesianEstim.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
