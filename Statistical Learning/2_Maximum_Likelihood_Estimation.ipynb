{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbada94d",
   "metadata": {},
   "source": [
    "*Updated 01-11-2023 (First commited 01-08-2023)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0dc5f-99f7-495e-bef2-b6a708ea7e05",
   "metadata": {},
   "source": [
    "(maximum-likelihood-estimation)=\n",
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Maximum likelihood estimation (MLE)** is a principle to find the best parameter for a probability distribution that **best explains** a sampled dataset. \n",
    "The instances in the dataset are supposed be independently sampled from the same distribution. \n",
    "MLE gives a relatively easy way to estimate parameters from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45d530-0f83-44f2-a54c-df85e6ccc82a",
   "metadata": {},
   "source": [
    "## Maximum likelihood\n",
    "\n",
    "### Likelihood function\n",
    "\n",
    "Given the dataset $\\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}$, the term **likelihood function** simply refers to the probability function of $\\mathcal{X}$ with the distribution parameters for $\\mathbf{X}$ being $\\boldsymbol{\\theta}$\n",
    "\n",
    "$$ \n",
    "f(\\boldsymbol{\\theta}) = \\mathbb{P}_{\\mathbf{X}}(\\mathcal{X} ; \\boldsymbol{\\theta}), \n",
    "$$\n",
    "\n",
    "which measures how likely are the parameters $\\boldsymbol{\\theta}$ given the data $\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce910a-431c-4c06-97f1-80111f5a203f",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Since the dataset are known, the probability function is only dependent on the parameters $\\boldsymbol{\\theta}$, and thus doesn't have the same shape as the probability density of the variable $\\mathbf{X}$. \n",
    "\n",
    "- The semicolon indicate that $\\boldsymbol{\\theta}$ is a parameter (fixed value) instead of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aac7f5-a265-41d5-b731-ca6a4728a8a2",
   "metadata": {},
   "source": [
    "### MLE Procedures \n",
    "\n",
    "1. We collect a set of instances $\\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}$ that we believe should be from the same distribution. \n",
    "\n",
    "1. We select a parametric model $\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta})$ that we think can best explains the data.\n",
    "\n",
    "1. We select the parameters $\\boldsymbol{\\theta}^{*}$ to be the ones that maximize the probability of the data: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb9434-eb54-49c3-9e90-6191d6facfa9",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\theta}^{*} \n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{\\mathbf{X}}(\\mathcal{X}; \\boldsymbol{\\theta}) \n",
    "\\\\\n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}_{i} ; \\boldsymbol{\\theta}) & [\\mathcal{X} \\text{ are i.i.d samples }] \n",
    "\\\\\n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} \\ln \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}_{i} ; \\boldsymbol{\\theta}) & [\\text{the log trick}] \n",
    "\\\\\n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i = 1}^{n} \\log \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}_{i} ; \\boldsymbol{\\theta}).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4520f8-4c5b-447b-8906-a0a43fa9d6a0",
   "metadata": {},
   "source": [
    "### Optimization methods\n",
    "\n",
    "If the likelihood function is concave, the best parameters $\\boldsymbol{\\theta}^{*}$ that maximize the likelihood function are the ones that make the gradient of $f(\\boldsymbol{\\theta})$ with respect to $\\boldsymbol{\\theta}$ $0$ and at the same time has negative semidefinite hessian matrix.  \n",
    "\n",
    "$$ \n",
    "\\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} \\mathbb{P}_{\\mathbf{X}}(\\mathcal{X} ; \\boldsymbol{\\theta}) = 0 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\nabla_{\\boldsymbol{\\theta}}^{2} f(\\boldsymbol{\\theta}) \\preceq 0 \n",
    "$$\n",
    "\n",
    "Otherwise, other numerical methods or algorithms might need to employed to solve the optimization problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa73476-f390-47ca-a848-038f8bfdc962",
   "metadata": {},
   "source": [
    "## Example: linear regression\n",
    "\n",
    "Given an instance $\\mathbf{x} \\in \\mathbb{R}^{d}$, the function $f (\\cdot)$ is a linear function if it has the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f (\\mathbf{x}) \n",
    "& = \\mathbf{w}^{T} \\mathbf{x} + b\n",
    "\\\\\n",
    "& = \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}},\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\mathbf{w}$ is weight vector and $b$ is the bias term,\n",
    "\n",
    "- $\\boldsymbol{\\theta}$ is the parameter vector that includes both weights and bias\n",
    "\n",
    "    $$ \n",
    "    \\boldsymbol{\\theta} = [ \\mathbf{w}, b ]^{T},\n",
    "    $$\n",
    "\n",
    "- $\\hat{\\mathbf{x}}$ is the instance vector appended with a constant $1$\n",
    "\n",
    "    $$\n",
    "    \\hat{\\mathbf{x}} = [ \\mathbf{x}, 1 ]^{T}.\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73285269-647c-4183-b8e2-5bfedd4f2383",
   "metadata": {},
   "source": [
    "Given a dataset with instances $\\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}$ and labels $\\mathcal{Y} = \\{ y_{1}, \\dots, y_{n} \\}$, linear regression is often formulated as a problem of finding parameters $\\boldsymbol{\\theta}$ that minimize the mean squared error (MSE) loss function\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\arg\\min_{\\boldsymbol{\\theta}} L_{\\text{MSE}}\n",
    "\\\\\n",
    "& = \\arg\\min_{\\boldsymbol{\\theta}} \\frac{1}{n} \\sum_{i=1}^{n} \\left(\n",
    "    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}\n",
    "\\right)^{2}\n",
    "\\\\\n",
    "& = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left(\n",
    "    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}\n",
    "\\right)^{2}\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which can be written in matrix notations by treating $\\mathcal{X}$ as a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and $\\mathcal{Y} \\in \\mathbb{R}^{n \\times 1}$ as a vector $\\mathbf{y}$\n",
    "\n",
    "$$\n",
    "\\arg\\min_{\\boldsymbol{\\theta}} \\lVert \\mathbf{y} - \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \\rVert^{2}_{2},\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mathbf{X}}$ is the instance matrix append with a column of $1$ in the last column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d65597-f83d-4e09-8994-37fae5774db8",
   "metadata": {},
   "source": [
    "### Linear regression as MLE\n",
    "\n",
    "Solving the optimization problem of linear regression with MSE loss can be formulated as solving MLE of parameters for a univariate normal distribution. \n",
    "\n",
    "First we can consider the difference $\\epsilon$ between $y$ and $\\boldsymbol{\\theta}^{T} \\mathbf{x}$ for any instance $\\mathbf{x}$ as a random variable that follows the a univariate normal distribution with zero mean and known variance\n",
    "\n",
    "$$\n",
    "y - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}} = \\epsilon \\sim \\mathcal{G} \\left( \n",
    "    \\epsilon, 0, \\sigma^{2}\n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab318ec7-d421-41ef-b3c6-3a903b9a931d",
   "metadata": {},
   "source": [
    "Since $\\boldsymbol{\\theta}^{T} \\mathbf{x}$ is a not a random variable (constant), the label $y$ for any instance $\\mathbf{x}$ is thus a univariate normal random variable with its mean being the predicted label of the linear function\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y\n",
    "& = \\epsilon + \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}\n",
    "\\\\\n",
    "& \\sim \\mathcal{G} \\left(\n",
    "    y, \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}, \\sigma^{2}\n",
    "\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In other words, the conditional probability of any label given the instance $\\mathbf{x}$ follows a univariate normal distribution\n",
    "\n",
    "$$\n",
    "\\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n",
    "    y \\mid \\mathbf{x}\n",
    "\\right) = \\mathcal{G} \\left( \n",
    "    y, \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}, \\sigma^{2} \n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e826c-f88a-4860-a725-7c3e0f10181e",
   "metadata": {},
   "source": [
    "%%markdown\n",
    "\n",
    "Given a set of instances $\\mathcal{X}$ and their labels $\\mathcal{Y}$, $\\mathbb{P}_{Y \\mid \\mathbf{X}} \\left( y \\mid \\mathbf{x} \\right)$ is a likelihood function of parameters $\\boldsymbol{\\theta}$ and thus MLE can be used to find the best parameters, which can be shown to have the optimization problem of fitting a linear function with MSE loss\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\theta}^{*}\n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n",
    "    y \\mid \\mathbf{x}\n",
    "\\right) \n",
    "\\\\\n",
    "& = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left(\n",
    "    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n",
    "\\right)^{2}\n",
    "\\\\\n",
    "& = \\arg\\min_{\\boldsymbol{\\theta}} \\lVert \\mathbf{y} - \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \\rVert^{2}_{2}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea2c40-67de-4608-9d5f-2799f251d94b",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::{admonition} Proof: $\\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left( y \\mid \\mathbf{x} \\right) = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left( y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i} \\right)^{2}$\n",
    ":class: dropdown\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\theta}^{*}\n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n",
    "    y \\mid \\mathbf{x}\n",
    "\\right) \n",
    "\\\\\n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\log \\mathcal{G} \\left(\n",
    "    y, \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}, \\sigma^{2}\n",
    "\\right)\n",
    "\\\\\n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\log - \\frac{\n",
    "    1\n",
    "}{\n",
    "    \\sqrt{2 \\pi \\sigma^{2}}\n",
    "} \\exp{ - \\frac{\n",
    "    \\left(\n",
    "        y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n",
    "    \\right)^{2}\n",
    "}{\n",
    "    2 \\sigma^{2}\n",
    "}}\n",
    "\\\\ \n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} - \\frac{1}{2} \\log \\left(\n",
    "    2 \\pi \\sigma^{2}\n",
    "\\right) - \\sum_{i=1}^{n} \\frac{\n",
    "    \\left(\n",
    "        y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n",
    "    \\right)^{2}\n",
    "}{\n",
    "    2 \\sigma^{2}\n",
    "}\n",
    "\\\\\n",
    "& = \\arg\\max_{\\boldsymbol{\\theta}} - \\sum_{i=1}^{n} \\left(\n",
    "    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n",
    "\\right)^{2}\n",
    "& [\\text{only need term with } \\boldsymbol{\\theta}]\n",
    "\\\\\n",
    "& = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left(\n",
    "    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n",
    "\\right)^{2}\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f329b9-20bf-426b-af6b-83383fa7a2fe",
   "metadata": {},
   "source": [
    "### Solving linear regression\n",
    "\n",
    "Solving linear regression is the same in both formulations by setting the gradient w.r.t $\\boldsymbol{\\theta}$ to $0$ and solve for $\\boldsymbol{\\theta}$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\boldsymbol{\\theta}} \\lVert \\mathbf{y} - \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \\lVert^{2}_{2}\n",
    "& = 0\n",
    "\\\\\n",
    "2 \\left(\n",
    "    \\hat{\\mathbf{X}}^{T} \\hat{\\mathbf{X}} \\boldsymbol{\\theta} - \\hat{\\mathbf{X}}^{T} \\mathbf{y} \n",
    "\\right)\n",
    "& = 0\n",
    "\\\\\n",
    "\\hat{\\mathbf{X}}^{T} \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \n",
    "& = \\hat{\\mathbf{X}}^{T} \\mathbf{y} \n",
    "\\\\\n",
    "\\boldsymbol{\\theta} \n",
    "& = \\left(\n",
    "    \\hat{\\mathbf{X}}^{T} \\hat{\\mathbf{X}}\n",
    "\\right)^{-1} \\hat{\\mathbf{X}}^{T} \\mathbf{y}.\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
