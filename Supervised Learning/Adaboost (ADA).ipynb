{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377dc6fb-037f-43e5-8c44-bbfa66c64240",
   "metadata": {},
   "source": [
    "# AdaBoost (Ada)\n",
    "---\n",
    "\n",
    "1. AdaBoost, or boosting in general, combines a series of weak learners into a strong learner. A weak learner is defined as any classifier that is slightly better than random guessing (>50%) which means that it has some basic understandings of the underlying distribution of the dataset. The output from the final strong learner is a combination of the weighted outputs of the weak learners. \n",
    "2. AdaBoost works by repeatedly fitting a base model on training instances with different weights. First we initialize a equal weight for each training instance and then we have M iterations. In each iteration, we fit the base model on the training instances with the current weights and get a value called error rate that evaluates what is the percentage of the weights of the incorrectly classified instances. The error rate then is used to compute the classifier coefficient that increases as the error rate decreases. In the end of each iteration, we update the weight of each instance so that misclassified instances get larger weights and correctly classified instances get lower weights. After the iterations, we get M classifiers and their coefficients. To make a prediction for an instance from the strong learner, we get the outputs from the M classifiers, sum up the product of the outputs and their coefficients and take the sign of value as the final output. \n",
    "3. AdaBoost assumes the weak learner to always have training accuracy larger than 50% and the output class to be 1 and -1. A very short decision tree called decision stump is usually used as the weak learner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca18c8-7106-4d45-8889-fd0d6dea0c2d",
   "metadata": {},
   "source": [
    "## Weak learner\n",
    "---\n",
    "\n",
    "> TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa580d6-ffd3-40ed-b9f0-e6056eeaad64",
   "metadata": {},
   "source": [
    "## AdaBoost algorithm\n",
    "---\n",
    "\n",
    "Given a dataset $\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{n} \\in \\mathbb{R}^{d}$ with labels $y_{1}, y_{2}, \\dots, y_{n} \\in \\{-1, 1\\}$.\n",
    "\n",
    "1. Initialize the observation weight $D_{1}$ for each instance. \n",
    "\n",
    "    $$ D_{1}(x_{i}) = \\frac{1}{n}, \\quad i \\in \\{1, 2, \\dots, n\\} $$\n",
    "\n",
    "2. For $t = 1 \\dots T$,\n",
    "    1. Fit a classifier $h_{t}$ to the training instances with weights $D_{t}$.\n",
    "    2. Compute the error rate of the classifier:\n",
    "    \n",
    "        $$ \\epsilon_{t} = \\sum_{i=1}^{n} D_{t}(x_{i}) \\times \\mathbb{1}(y_i \\neq h_{t}(x_i)) $$\n",
    "        \n",
    "    3. Compute the classifier weight:\n",
    "    \n",
    "        $$ \\alpha_{t} = \\frac{1}{2} \\ln \\frac{1 - \\epsilon_{t}}{\\epsilon_{t}}  $$\n",
    "        \n",
    "    4. Update the observation weight for each instance:\n",
    "    \n",
    "        $$ D_{t + 1}(x_{i}) = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)), \\quad i \\in \\{1, 2, \\dots, n\\} $$\n",
    "        \n",
    "        where $Z_{t}$ is the normalized factor that makes $\\sum_{i = 1}^{n} D_{t + 1}(x_{i}) = 1$.\n",
    "        \n",
    "        $$ {Z_{t}} = \\sum_{i = 1}^{n} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) $$\n",
    "        \n",
    "3. Final output of AdaBoost: \n",
    "\n",
    "    $$ F(x) = \\operatorname{sign} \\left( \\sum_{t=1}^{T} \\alpha_t h_{t}(x) \\right) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bda59-5f2a-4e94-968d-284aa4fe9e71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Facts about AdaBoost:\n",
    "\n",
    "1. $\\alpha_{t}$ is always positive.\n",
    "\n",
    "    :::{admonition} Proof\n",
    "    :class: dropdown\n",
    "\n",
    "    Since the requirement for the weak learner is to have the weighed training error $\\epsilon_{t} < \\frac{1}{2}$ and thus $ 1 - \\epsilon_{t} \\geq \\frac{1}{2} $,\n",
    "    \n",
    "    $$ \\frac{1 - \\epsilon_{t}}{\\epsilon_{t}} > 1 \\Rightarrow \\alpha_{t} = \\frac{1}{2} \\ln{\\frac{1 - \\epsilon_{t}}{\\epsilon_{t}}} > 0 $$\n",
    "    \n",
    "    :::\n",
    "    \n",
    "1. The smaller the classification error of $h_{t}$, the larger the weight $\\alpha_{t}$ and thus the more impact that $h_{t}$ will have on the final classifier. \n",
    "\n",
    "1. The weights of correctly classified points are reduced and the weights of incorrectly classified points are increased. Hence, the incorrectly classified points will receive more attention in the next run.\n",
    "\n",
    "    :::{admonition} Proof\n",
    "    :class: dropdown\n",
    "    \n",
    "    $$\n",
    "    \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) = \n",
    "    \\begin{cases} \n",
    "    \\exp(-\\alpha_{t}) & \\text{if } y_{i} = h_{t}(x_{i}) \\\\\n",
    "    \\exp(\\alpha_{t}) & \\text{if } y_{i} \\neq h_{t}(x_{i}) \\\\\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "    :::\n",
    "    \n",
    "1. The weighted training error of the weak rule $h_{t}$ on the reweighted training set in the next iteration with weights $D_{t + 1}$ is always 0.5. \n",
    "\n",
    "    :::{admonition} Proof\n",
    "    :class: dropdown\n",
    "    \n",
    "    1. Assume that the weighted training error of $h_{t}$ on iteration $t$ is $\\epsilon$,\n",
    "    \n",
    "        $$ \\epsilon = \\sum_{x_{i} \\in E} D_{t}(x_{i}) $$ \n",
    "    \n",
    "        where $E$ is the set of the incorrectly classified instances.\n",
    "    \n",
    "    1. By adding $h_{t}$, the new weights of the incorrectly classified training instances are\n",
    "    \n",
    "        $$ \n",
    "        \\begin{align}\n",
    "        D_{t + 1}(x_{i}) & = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\exp(-\\alpha_{t} \\times (-1)) \\\\\n",
    "        & = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\exp \\left(\\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon}{\\epsilon} \\right) \\right) \\\\\n",
    "        & = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}} \n",
    "        \\end{align}\n",
    "        $$\n",
    "    \n",
    "    1. Since the set of incorrectly classified instances $E$ doesn't change, the weighted training error of $h_{t}$ on the reweighted training set is \n",
    "    \n",
    "        $$ \n",
    "        \\begin{align}\n",
    "        \\hat{\\epsilon} & = \\sum_{x_{i} \\in E} D_{t + 1}(x_{i}) \\\\\n",
    "        & = \\frac{1}{Z_{t}} \\sum_{x_{i} \\in E} D_{t}(x_{i}) \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}} \\\\\n",
    "        & = \\frac{1}{Z_{t}} \\epsilon \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}} \\\\\n",
    "        & = \\frac{1}{Z_{t}} \\sqrt{(1 - \\epsilon) \\epsilon}\n",
    "        \\end{align}\n",
    "        $$\n",
    "    \n",
    "    1. Denote $C$ to be the set of the correctly classified instances.\n",
    "    \n",
    "        $$ \n",
    "        \\begin{align}\n",
    "        {Z_{t}} & = \\sum_{i = 1}^{n} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) \\\\\n",
    "        & = \\sum_{i \\in E} D_{t}(x_{i}) \\times \\exp(\\alpha_{t}) + \\sum_{i \\in C} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t}) \\\\\n",
    "        & = \\sqrt{(1 - \\epsilon) \\epsilon} + (1 - \\epsilon) \\sqrt{\\frac{\\epsilon}{1 - \\epsilon}} \\\\\n",
    "        & = 2 \\sqrt{(1 - \\epsilon) \\epsilon} \\\\\n",
    "        \\end{align}\n",
    "        $$\n",
    "        \n",
    "    1. Thus, \n",
    "    \n",
    "        $$ \\hat{\\epsilon} = \\frac{\\sqrt{(1 - \\epsilon) \\epsilon}}{2 \\sqrt{(1 - \\epsilon) \\epsilon}} = 0.5 $$\n",
    "        \n",
    "    :::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0b451-5e71-4abe-bc8e-842d70fb8841",
   "metadata": {},
   "source": [
    "## Convergence analysis\n",
    "---\n",
    "\n",
    "AdaBoost is a greedy algorithm that minimizes a loss function that upper bounds the classification error. \n",
    "\n",
    "1. AdaBoost is a greedy algorithm that minimizes the loss function:\n",
    "\n",
    "    $$ \\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) $$\n",
    "    \n",
    "    :::{admonition} Proof\n",
    "    :class: dropdown\n",
    "    \n",
    "    We can expand $D_{t + 1}(x_{i})$ recursively\n",
    "\n",
    "    $$ \n",
    "    \\begin{align}\n",
    "    D_{t + 1}(x_{i}) & = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) \\\\\n",
    "    & = \\frac{1}{Z_{t}} \\left( \\frac{1}{Z_{t - 1}} D_{t - 1}(x_{i}) \\times \\exp(-\\alpha_{t - 1} \\times y_{i}h_{t - 1}(x_i)) \\right) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) \\\\\n",
    "    & = \\frac{1}{Z_{t} Z_{t - 1}} D_{t - 1}(x_{i}) \\times \\exp(- y_{i}(\\alpha_{t}h_{t}(x_i) + \\alpha_{t - 1}h_{t - 1}(x_i))) \\\\\n",
    "    & = \\dots & [\\text{expand } Z_{t - 1}, Z_{t - 2}, \\dots, Z_{1}] \\\\\n",
    "    & = \\frac{1}{\\prod_{k=1}^{t} Z_{k}} D_{1}(x_{i}) \\times \\exp(- y_{i}(\\alpha_{t}h_{t}(x_i) + \\alpha_{t - 1}h_{t - 1}(x_i) + \\dots \\alpha_{1}h_{1}(x_i))) \\\\\n",
    "    & = \\frac{1}{\\prod_{k=1}^{t} Z_{k}} \\frac{1}{n} \\times \\exp(- y_{i}(\\alpha_{t}h_{t}(x_i) + \\alpha_{t - 1}h_{t - 1}(x_i) + \\dots \\alpha_{1}h_{1}(x_i))) & [D_{1}(x_{i}) = \\frac{1}{n}] \\\\\n",
    "    & = \\frac{1}{\\prod_{k=1}^{t} Z_{k}} \\frac{1}{n} \\times \\exp(- y_{i}F_{t}(x_{i})) & [F_{t}(x_{i}) = \\alpha_{t}h_{t}(x_i) + \\alpha_{t - 1}h_{t - 1}(x_i) + \\dots \\alpha_{1}h_{1}(x_i))] \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "    Since the sum of the weights is 1:\n",
    "\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    1 & = \\sum_{i = 1}^{n} D_{t + 1}(x_{i})  \\\\\n",
    "    1 & = \\sum_{i = 1}^{n} \\frac{1}{\\prod_{k=1}^{t} Z_{k}} \\frac{1}{n} \\times \\exp(- y_{i}F_{t}(x_{i})) \\\\\n",
    "    \\prod_{k=1}^{t} Z_{k} & = \\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) \\\\\n",
    "    \\end{align}\n",
    "    $$ \n",
    "\n",
    "    Thus, the loss function is the sum of the normalization terms. \n",
    "    \n",
    "    In iteration $t$, all previous $Z_{1}, Z_{2}, \\dots, Z_{t - 1}$ don't depend on $\\alpha_{t}$ and $h_{t}$. Thus the loss function that AdaBoost minimizes in iteration $t$ is just $Z_{t}$. \n",
    "    \n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) & = \\prod_{k=1}^{t} Z_{k} \\\\\n",
    "     & = Z_{t} \\prod_{k=1}^{t - 1} Z_{k}\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "    We know that $Z_{t}$ depends on the choice of $\\alpha_{t}$ and $h_{t}$. Minimizing $Z_{t}$ can be solved by taking its derivative w.r.t $\\alpha_{t}$ and $h_{t}$ and set it to 0. Here we show that the $\\alpha_{t}$ that minimizes the $Z_{t}$ is exactly the one used in AdaBoost algorithm. \n",
    "    \n",
    "    $$ \n",
    "    \\begin{align}\n",
    "    \\frac{\\partial Z_{t}}{\\partial \\alpha_{t}} & = 0 \\\\\n",
    "    \\frac{\\partial}{\\partial \\alpha_{t}} \\sum_{i \\in E} D_{t}(x_{i}) \\times \\exp(\\alpha_{t}) + \\sum_{i \\in C} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t}) & = 0 \\\\\n",
    "    \\sum_{i \\in E} D_{t}(x_{i}) \\times \\exp(\\alpha_{t}) - \\sum_{i \\in C} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t}) & = 0 \\\\\n",
    "    \\frac{\\exp(\\alpha_{t})}{\\exp(-\\alpha_{t})} \\sum_{i \\in E} D_{t}(x_{i}) & = \\sum_{i \\in C} D_{t}(x_{i}) \\\\\n",
    "    \\exp(2 \\alpha_{t}) \\epsilon & = 1 - \\epsilon \\\\\n",
    "    \\alpha_{t} & = \\frac{1}{2} \\ln \\frac{1 - \\epsilon}{\\epsilon}\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "    :::\n",
    "    \n",
    "1. The loss function is an upper bound on the weighted training error (0-1 loss) in any iteration $t$:\n",
    "\n",
    "    $$ Err(F_{t}) = \\frac{1}{n} \\sum_{i}^{n} \\mathbb{1}(y_{i} \\neq \\operatorname{sign}(F_{t}(x_{i}))) \\leq \\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) $$\n",
    "    \n",
    "    :::{admonition} Proof\n",
    "    :class: dropdown\n",
    "    \n",
    "    For all $i$, \n",
    "    \n",
    "    $$ \\mathbb{1} (y_{i} \\neq \\operatorname{sign}(F_{t}(x_{i})) = -\\operatorname{step}(y_{i} F_{t}(x_{i}) \\leq  \\exp(- y_{i}F_{t}(x_{i}) $$ \n",
    "    \n",
    "    :::\n",
    "    \n",
    "1. After $t$ iterations, the weighted training error of $F_{t}$ is bounded by \n",
    "\n",
    "    $$ Err(F_{t}) \\leq \\exp(-2 \\sum_{k = 1}^{t} \\gamma_{k}^{2}) $$\n",
    "\n",
    "    where $\\gamma_{k} = 0.5 - \\epsilon_{k}$ to denote how much better $h_{t}$ is than the random guessing,\n",
    "    \n",
    "    :::{admonition} Proof\n",
    "    :class: dropdown\n",
    "    \n",
    "    $$\n",
    "    \\begin{align}\n",
    "    Err(F_{t}) & \\leq \\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) \\\\\n",
    "    & \\leq \\prod_{k=1}^{t} Z_{k} \\\\\n",
    "    & \\leq \\prod_{k=1}^{t} 2 \\sqrt{(1 - \\epsilon_{k}) \\epsilon_{k}} \\\\\n",
    "    & \\leq \\prod_{k=1}^{t} 2 \\sqrt{(0.5 + \\gamma_{k}) (0.5 - \\gamma_{k})} & [\\epsilon_{k} = 0.5 - \\gamma_{k}] \\\\\n",
    "    & \\leq \\prod_{k=1}^{t} \\sqrt{1 - 4 \\gamma_{k}^{2}} \\\\\n",
    "    & \\leq \\prod_{k=1}^{t} \\exp(-2 \\gamma_{k}^{2}) & [\\sqrt{1 - 4 \\gamma_{t}^{2}} \\leq \\exp(-2 \\gamma^{2})] \\\\\n",
    "    & \\leq \\exp(-2 \\sum_{k=i}^{t} \\gamma_{k}^{2})\n",
    "    \\end{align}\n",
    "    $$ \n",
    "    \n",
    "    :::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f6998-9e6b-410d-aa55-781bcb76b159",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "\n",
    "1. https://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html  \n",
    "1. https://arxiv.org/pdf/1403.1452.pdf\n",
    "1. https://cse.buffalo.edu/~jcorso/t/CSE555/files/lecture_boosting.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
