{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68aafb36-4727-4209-ac10-b0370d012d5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea79bc-9c83-4d4c-ad91-da003c387bec",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### Statistical Learning\n",
    "\n",
    "- [Bayesian Decision Theory (BDT)](bayesian-decision-theory)\n",
    "\n",
    "- [Maximum Likelihood Estimation (MLE)](maximum-likelihood-estimation)\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "- [Linear Discriminant](linear-discriminant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e0ede-ec49-4975-bb2f-629658003ce6",
   "metadata": {},
   "source": [
    "## Logistic regression as a Gaussian classifier\n",
    "\n",
    "Logistic regression is a classification model that models the posterior probability of the positive class and assigns labels based on the MAP rule\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1 & \\sigma (f (\\mathbf{x})) \\geq 0.5 \\\\\n",
    "0 & \\sigma (f (\\mathbf{x}))\n",
    "< 0.5 \\\\\n",
    "\\end{cases},\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $f (\\mathbf{x})$ is a linear function on the instance $\\mathbf{x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7eefdb-2194-4da4-8d60-1ebb3ae7492e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### MAP rule and posterior probability\n",
    "\n",
    "Recall that the BDR with 0-1 loss is the MAP rule\n",
    "\n",
    "$$\n",
    "f (\\mathbf{x}) = \\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x})\n",
    "$$\n",
    "\n",
    "where $\\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x})$ is the posterior probability that the true class for instance $\\mathbf{x}$ is $y$. \n",
    "\n",
    "For a binary classification problem, the **MAP rule** can be simplified to select the class $1$ for $\\mathbf{x}$ if\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}) \n",
    "& \\geq \\mathbb{P}_{Y \\mid \\mathbf{X}} (0 \\mid \\mathbf{x}) \n",
    "\\\\\n",
    "& \\geq 1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n",
    "\\\\\n",
    "& \\geq 0.5.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using the Bayes theorem, the **posterior probability** of the positive class can be represented using the class conditional probabilities and class probabilities\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}) \n",
    "& = \\frac{\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n",
    "}{\n",
    "    \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x})\n",
    "} \n",
    "& [\\text{Bayes' theroem}]\n",
    "\\\\\n",
    "& = \\frac{\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n",
    "}{\n",
    "    \\mathbb{P}_{\\mathbf{X}, Y} (\\mathbf{x}, 0) + \\mathbb{P}_{\\mathbf{X}, Y} (\\mathbf{x}, 1)\n",
    "} \n",
    "& [\\text{Law of total probability}]\n",
    "\\\\\n",
    "& = \\frac{\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n",
    "}{\n",
    "    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 0) \\mathbb{P}_{Y} (0) + \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n",
    "} \n",
    "& [\\text{Chain rule}]\n",
    "\\\\\n",
    "& = \\left(1 + \\frac{\n",
    "        \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 0) \\mathbb{P}_{Y} (0) \n",
    "    }{\n",
    "        \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n",
    "    } \n",
    "\\right)^{-1}\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698c254-fccf-46b0-89b3-27acd52fe99e",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "The **sigmoid function** is a saturating function that maps the real number $x$ into a number that ranges from $0$ to $1$\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{\n",
    "    1\n",
    "}{\n",
    "    1 + e^{- x}\n",
    "}.\n",
    "$$\n",
    "\n",
    "The posterior probability is the result of the sigmoid function if we assume the class conditional probabilities are Gaussian distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3b4f6-cccd-411f-a2a8-dc610d31cd00",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Recall that the multivariate Gaussian with the mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{\n",
    "    1\n",
    "}{\n",
    "    \\sqrt{(2 \\pi)^{2} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert}\n",
    "} \\exp \\left(\n",
    "    -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_{1})^T \\boldsymbol{\\Sigma}_{1}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_{1}})\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "which can be compactly written as follows\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \n",
    "& = \\frac{\n",
    "    1\n",
    "}{\n",
    "    \\sqrt{(2 \\pi)^{2} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert}\n",
    "} \\exp \\left(\n",
    "    -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_{1})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_{1}})\n",
    "\\right)\n",
    "\\\\\n",
    "& = \\exp \\left(\n",
    "    \\log \\left(\n",
    "        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma} \\rvert\n",
    "    \\right)^{-\\frac{1}{2}} - \\frac{1}{2} \\left(\n",
    "        \\mathbf{x} - \\boldsymbol{\\mu}\n",
    "    \\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left(\n",
    "        \\mathbf{x} - \\boldsymbol{\\mu}\n",
    "    \\right)\n",
    "\\right)\n",
    "\\\\\n",
    "& = \\exp \\left(\n",
    "    -\\frac{1}{2} \\log \\left(\n",
    "        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma} \\rvert\n",
    "    \\right) - \\frac{1}{2} \\left(\n",
    "        \\mathbf{x} - \\boldsymbol{\\mu}\n",
    "    \\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left(\n",
    "        \\mathbf{x} - \\boldsymbol{\\mu}\n",
    "    \\right)\n",
    "\\right)\n",
    "\\\\\n",
    "& = \\exp \\left(\n",
    "    -\\frac{1}{2} \\left(\n",
    "        \\log \\left(\n",
    "            (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma} \\rvert\n",
    "        \\right) + d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\boldsymbol{\\mu}) \n",
    "    \\right)\n",
    "\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\mathbf{y}) = \\frac{1}{2} \\left(\n",
    "    \\mathbf{x} - \\mathbf{y} \n",
    "\\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left( \n",
    "    \\mathbf{x} - \\mathbf{y} \n",
    "\\right)$ is the Mahalanobis distance between $\\mathbf{x}$ and $\\mathbf{y}$ with covariance matrix $\\boldsymbol{\\Sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c420598-9be0-4725-876e-7933681768f7",
   "metadata": {},
   "source": [
    "If we assume the class conditional probabilities for both classes are Gaussian distributions:\n",
    "\n",
    "- $\\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 0) = \\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}_{0}, \\boldsymbol{\\Sigma}_{0})$\n",
    "\n",
    "- $\\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) = \\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}_{1})$,\n",
    "\n",
    "then the posterior possibility is \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n",
    "& = \\left(\n",
    "    1 + \\frac{\n",
    "        \\exp \\left(\n",
    "            -\\frac{1}{2} \\left(\n",
    "                \\log \\left(\n",
    "                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{0} \\rvert\n",
    "                \\right) + d_{\\boldsymbol{\\Sigma_{0}}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}})\n",
    "            \\right) \\mathbb{P}_{Y} (0)\n",
    "        \\right) \n",
    "    }{\n",
    "        \\exp \\left(\n",
    "            -\\frac{1}{2} \\left(\n",
    "                \\log \\left(\n",
    "                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert\n",
    "                \\right) + d_{\\boldsymbol{\\Sigma_{1}}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n",
    "            \\right) \\mathbb{P}_{Y} (1)\n",
    "        \\right) \n",
    "    }\n",
    "\\right)^{-1}\n",
    "\\\\\n",
    "& = \\left(\n",
    "    1 + \\frac{\n",
    "        \\exp \\left(\n",
    "            -\\frac{1}{2} \\left(\n",
    "                \\log \\left(\n",
    "                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{0} \\rvert\n",
    "                \\right) + d_{\\boldsymbol{\\Sigma_{0}}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}})\n",
    "            \\right) + \\log \\mathbb{P}_{Y} (0)\n",
    "        \\right)\n",
    "    }{\n",
    "        \\exp \\left(\n",
    "            -\\frac{1}{2} \\left(\n",
    "                \\log \\left(\n",
    "                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert\n",
    "                \\right) + d_{\\boldsymbol{\\Sigma_{1}}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n",
    "            \\right) + \\log \\mathbb{P}_{Y} (1)\n",
    "        \\right) \n",
    "    }\n",
    "\\right)^{-1}\n",
    "\\\\\n",
    "& = \\left(\n",
    "    1 + \\exp \\left(\n",
    "        - f (\\mathbf{x})\n",
    "    \\right)\n",
    "\\right)^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $f (\\mathbf{x}) = \\frac{1}{2} \\left(\n",
    "    \\alpha_{0} - \\alpha_{1} \n",
    "    + d_{\\boldsymbol{\\Sigma_{0}}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}}) \n",
    "    - d_{\\boldsymbol{\\Sigma_{1}}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n",
    "    + 2 \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n",
    "\\right)\n",
    "$ and $\\alpha_{i} = \\log \\left(\n",
    "    (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{i} \\rvert\n",
    "\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9798761-7817-4522-8a05-e57b436ca4c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Linear function\n",
    "\n",
    "If we further assume that the Gaussian distributions for both classes have the same covariance matrix $\\boldsymbol{\\Sigma}_{0} = \\boldsymbol{\\Sigma}_{1} = \\boldsymbol{\\Sigma}$,\n",
    "then $f (\\mathbf{x})$ is a linear function\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f (\\mathbf{x}) \n",
    "& = \\frac{1}{2} \\left(\n",
    "    \\alpha - \\alpha \n",
    "    + d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}}) \n",
    "    - d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n",
    "    + 2 \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n",
    "\\right)\n",
    "\\\\\n",
    "& = \\frac{1}{2} \\left(\n",
    "    \\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} +\n",
    "    2\\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} + \n",
    "    \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} -\n",
    "    \\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} -\n",
    "    2\\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1} -\n",
    "    \\boldsymbol{\\mu}_{1}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}\n",
    "\\right) + \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n",
    "\\\\\n",
    "& = \\left( \n",
    "    \\boldsymbol{\\mu}_{0} - \\boldsymbol{\\mu}_{1}\n",
    "\\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} + \\frac{1}{2} \\left(\n",
    "    \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} -\n",
    "    \\boldsymbol{\\mu}_{1}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}\n",
    "\\right) + \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n",
    "\\\\\n",
    "& = \\mathbf{w}^{T} \\mathbf{x} + b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\mathbf{w}^{T} = \\left( \n",
    "    \\boldsymbol{\\mu}_{0} - \\boldsymbol{\\mu}_{1}\n",
    "\\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x}$,\n",
    "\n",
    "- $b = \\frac{1}{2} \\left(\n",
    "    \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} -\n",
    "    \\boldsymbol{\\mu}_{1}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}\n",
    "\\right) + \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddaf40-064c-47c4-876c-2d0ad4f906bd",
   "metadata": {},
   "source": [
    "## Learning of logistic regression\n",
    "\n",
    "With the generative approach, parameters $\\boldsymbol{\\mu}_{0}$, $\\boldsymbol{\\mu}_{1}$, $\\boldsymbol{\\Sigma}_{0}$, and $\\boldsymbol{\\Sigma}_{1}$ are learned from the training set using MLE. \n",
    "In particular, the parameters for the conditional probability of class $j$ are learned by solving the following optimization problem\n",
    "\n",
    "$$\n",
    "\\arg\\max_{\\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}} \\prod_{y_{j} = j} \\mathbb{P}_{\\mathbf{X} \\mid Y} \\left(\n",
    "    \\mathbf{x}_{i} \\mid j\n",
    "\\right) = \\arg\\max_{\\boldsymbol{\\mu}_{j}, \\boldsymbol{\\Sigma}_{j}} \\prod_{y_{j} = j} \\mathcal{G} \\left( \n",
    "    \\mathbf{x}_{i}; \\boldsymbol{\\mu}_{j}, \\boldsymbol{\\Sigma}_{j}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "However, logistic regression is usually learned using a discriminative approach, where the parameters $\\mathbf{w}, b$ are directly learned from the data by minimizing binary cross-entropy loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0fd86-3401-4118-a18f-6ec1c567f51d",
   "metadata": {},
   "source": [
    "### Learning as a MLE problem\n",
    "\n",
    "Recall that the learning of the linear regression can be formulated as an MLE problem\n",
    "\n",
    "$$\n",
    "\\arg\\max_{\\mathbf{w}, b} \\prod_{i} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n",
    "    y_{i} \\mid \\mathbf{x}_{i}\n",
    "\\right) = \\arg\\max_{\\mathbf{w}, b} \\prod_{i} \\mathcal{G} \\left( \n",
    "    y_{i}; \\mathbf{w}^{T} \\mathbf{x}_{i} + b, \\sigma^{2} \n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where the posterior probability of the label $\\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n",
    "    y_{i} \\mid \\mathbf{x}_{i}\n",
    "\\right)$ follows a univariate Gaussian distribution with the mean $\\mathbf{w}^{T} \\mathbf{x} + b$ and a known variance $\\sigma^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e187a-d10c-453a-8b37-089df84adab3",
   "metadata": {},
   "source": [
    "For logistic regression, the posterior probability of the label should be a Bernoulli distribution \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n",
    "    y \\mid \\mathbf{x}\n",
    "\\right) \n",
    "& = \\mathcal{B} \\left( \n",
    "    y; \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n",
    "\\right)\n",
    "\\\\\n",
    "& = \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})^{y} \\left(\n",
    "    1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n",
    "\\right)^{(1 - y)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and therefore the MLE problem is defined as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\arg\\max_{\\mathbf{w}, b} \\prod_{i} \\mathcal{B} \\left( \n",
    "    y; \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n",
    "\\right)\n",
    "& = \\arg\\max_{\\mathbf{w}, b} \\sum_{i} \\log \\mathcal{B} \\left( \n",
    "    y; \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n",
    "\\right)\n",
    "\\\\\n",
    "& = \\arg\\max_{\\mathbf{w}, b} \\sum_{i} \\log \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})^{y_{i}} \\left(\n",
    "    1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n",
    "\\right)^{(1 - y_{i})}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47d497-c5da-49df-9e56-ed3c8304fc8f",
   "metadata": {},
   "source": [
    "### Binary cross-entropy (BCE) loss\n",
    "\n",
    "The binary cross-entropy loss is defined as \n",
    "\n",
    "$$\n",
    "\\text{BCE} (y, \\hat{y}) =  - y \\log \\hat{y} - (1 - y) \\log (1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "where $y \\in \\{0, 1\\}$ is the binary label and $\\hat{y} \\in [0, 1]$ is the probability of the positive class.\n",
    "\n",
    "Solving the MLE of parameters of the logistic regression problem is the same as minimizing the BCE loss\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\arg\\max_{\\mathbf{w}, b} \\sum_{i} \\log \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})^{y_{i}} \\left(\n",
    "    1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n",
    "\\right)^{(1 - y_{i})}\n",
    "\\\\\n",
    "= & \\arg\\max_{\\mathbf{w}, b} \\sum_{i} y_{i} \\log \\sigma (f (\\mathbf{x}_{i})) + (1 - y_{i}) \\log \\left(\n",
    "    1 - \\sigma (f (\\mathbf{x}_{i}))\n",
    "\\right)\n",
    "\\\\\n",
    "= & \\arg\\min_{\\mathbf{w}, b} \\sum_{i} - y_{i} \\log \\sigma (f (\\mathbf{x}_{i})) - (1 - y_{i}) \\log \\left(\n",
    "    1 - \\sigma (f (\\mathbf{x}_{i}))\n",
    "\\right)\n",
    "\\\\\n",
    "= & \\arg\\min_{\\mathbf{w}, b} \\sum_{i} \\text{BCE} (y_{i}, \\sigma (f (\\mathbf{x}_{i})).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, logistic regression can be learned by minimizing the BCE loss between the predicted labels and training labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36eca79-f347-448b-838c-ac9a98103cba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Minimizing loss with gradient descent \n",
    "\n",
    "Unlike linear regression, the optimization problem of logistic regression \n",
    "\n",
    "$$\n",
    "\\arg\\min_{\\mathbf{w}, b} \\sum_{i} - y_{i} \\log \\sigma (f (\\mathbf{x}_{i})) - (1 - y_{i}) \\log \\left(\n",
    "    1 - \\sigma (f (\\mathbf{x}_{i}))\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "can not be analytically solved to obtain a closed-form solution because of the non-linear sigmoid function. \n",
    "\n",
    "Instead, gradient descent is used to solve the optimization problem numerically. \n",
    "\n",
    "TODO: waiting for convex optimization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
