{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db54a6e0-243d-415e-b109-7524f71b8742",
   "metadata": {},
   "source": [
    "# Logistic Regression (LR)\n",
    "---\n",
    "\n",
    "1. Linear regression is a supervised machine learning model that fits a correlation linear line between input and label variables. The output value can be arbitrary continuous value and thus it is used for regression. \n",
    "2. The model has a weight vector and a bias as parameters. The output of the model is the dot product of the weight vector and the input vector plus a bias value. \n",
    "3. The linear regression model is trained by solving an optimization problem that is defined by applying a cost function that evaluates the difference between the model's output and the correct labels. The cost function for linear regression is mean squared error function that takes the mean of the squared value of each prediction's error. MSE for linear regression is proved to be convex, so solving it using convex optimization or gradient descent will get the global minimum. \n",
    "4. Logistic regression is similar to linear regression, but the output is a probability value between 0 and 1, so it is used for binary classification instead of regression.\n",
    "5. A sigmoid (logistic) function is attached after the output of linear regression to output a probability for logistic regression. Instead of using MSE, the cost function is changed to binary cross entropy such that the loss grows exponentially with the difference between outputs and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad13a63-3711-4979-a786-a3dfc090c556",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "---\n",
    "\n",
    "### Optimization\n",
    "\n",
    "#### Convex function \n",
    "> TODO\n",
    "\n",
    "#### Gradient descent \n",
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708c913-066d-4498-82c6-184e92ed46de",
   "metadata": {},
   "source": [
    "## Linear regression for regression problems\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc3f31-c37c-4fc0-9589-2d4c3b0881f8",
   "metadata": {},
   "source": [
    "### Linear function\n",
    "\n",
    "If a function with single input and output $y = f(x)$ is a linear function, it represents a straight line on the coordinate plane. Thus it has the form:\n",
    "\n",
    "$$ y = wx + b $$\n",
    "\n",
    "where $w$ is the slope and $b$ is the y-intercept of the line. \n",
    "\n",
    "A linear function can also take multiple inputs and we can represent $n$ inputs with a vector $\\mathbf{x} \\in \\mathbb{R}^{d}$. Then the equation changes to:\n",
    "\n",
    "$$ y = \\mathbf{w} \\cdot \\mathbf{x} + b $$\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^{d}$ is called **weights** or the weight vector and $b \\in \\mathbb{R}$ is called **bias**.\n",
    "\n",
    "Linear regression is a supervised regression model that simply fits a linear function between the inputs $\\mathbf{x}$ and output $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb682ff4-8dd5-401f-98d4-c0320d0a7ee7",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Loss function is a function that measures the error between the labels predicted by your current model and the true labels for a set of instances (training set).\n",
    "\n",
    "Linear regression minimizes the a particular loss function called **mean squared error (MSE)** function. Given a set of training labels $\\mathbf{y} = y_{1}, y_{2}, \\dots, y_{n}$ for $n$ training instances and the predicted labels $\\hat{\\mathbf{y}} = \\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{n}$ for the same instances, MSE is defined as: \n",
    "\n",
    "$$ \\operatorname{MSE}(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{n} \\sum_{i}^{n}(\\hat{y}_{i} - y_{i})^{2} $$\n",
    "\n",
    "Thus, fitting a linear regression model for a training set $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ with labels $\\mathbf{y} \\in \\mathbb{R}^{n}$ is a convex optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min & \\quad \\lVert (\\mathbf{X}\\mathbf{w} + b) - \\mathbf{y} \\rVert_{2}^{2} \\\\\n",
    "= \\min & \\quad \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f6eef-65c4-4518-a16f-4b9daa4e6da9",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization is a technique used to avoid over-fitting of the machine learning models. Regularization works by adding an additional penalty term in the loss function to penalize aspects of the model other than the prediction error. \n",
    "\n",
    "$$ L_{\\text{new}} = L_{\\text{old}} + \\lambda L_{\\text{reg}} $$\n",
    "\n",
    "where $\\lambda$ is a hyperparamter that adjust the trade-off between the primary objective and the regularization.\n",
    "\n",
    "For linear regression and other many models, usually $L_{2}$ regularization (Ridge) and $L_{1}$ regularization (Lasso) are used. \n",
    "- Ridge penalizes the $L_{2}$ norm of the weights\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "L_{\\text{ridge}} & = \\lVert \\mathbf{w} \\rVert_{2}^{2} \\\\\n",
    "& = \\sum_{i=1}^{\\lvert \\mathbf{w} \\rvert} w_{i}^{2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Lasso penalizes the $L_{1}$ norm of the weights\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "L_{\\text{lasso}} & = \\lVert \\mathbf{w} \\rVert_{1} \\\\\n",
    "& = \\sum_{i=1}^{\\lvert \\mathbf{w} \\rvert} \\lvert w_{i} \\rvert \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Both Ridge and Lasso penalize the magnitude of the weights. Large weights tend to overfit the training dataset because\n",
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c98541-5977-424b-895a-d7be7c7b103c",
   "metadata": {},
   "source": [
    "## Solving linear regression\n",
    "---\n",
    "\n",
    "Here we show how we can analytically solve linear regression to get a closed form solution.\n",
    "\n",
    "We first make $b$ as part of $\\mathbf{w}$ to simplify the derivation process, which is done by adding $b$ as an extra weight into $\\mathbf{w}$ vector. The result weight vector $\\mathbf{\\hat{w}} \\in \\mathbb{R}^{d + 1}$ has one extra dimension:\n",
    "\n",
    "$$ \\mathbf{\\hat{w}} = (b, w_{1}, w_{2}, \\dots, w_{d}). $$\n",
    "\n",
    "Then we add a dummy input $x_{0} = 1$ to all input instances, so that \n",
    "\n",
    "$$ \\mathbf{\\hat{x}} = (1, x_{1}, x_{2}, \\dots, x_{d}). $$\n",
    "\n",
    "As a result, we have\n",
    "\n",
    "$$ \\mathbf{\\hat{w}} \\cdot \\mathbf{\\hat{x}} = \\mathbf{w} \\cdot \\mathbf{x} + b. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1416411-0591-41a0-b3ac-dbc33ea3619f",
   "metadata": {},
   "source": [
    "The equation that we want to solve is\n",
    "\n",
    "$$ \\min \\quad \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{\\hat{w}} \\cdot \\mathbf{\\hat{x}}_{i} - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\rVert_{2}^{2} $$\n",
    "\n",
    "Since this equation is a convex function, it can be directly solved by setting its derivative w.r.t its parameters ($\\mathbf{\\hat{w}})$ to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e26a8e-d184-44f3-a494-b99b11a374fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solving linear regression\n",
    "\n",
    "Here we show how we can analytically solve linear regression with $L_{2}$ regularization to get a closed form solution.\n",
    "\n",
    "$$ \\min \\quad \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\rVert_{2}^{2} $$\n",
    "\n",
    "Since this equation is a convex function, it can be directly solved by taking its derivative w.r.t its parameters ($\\mathbf{w}$ and $b$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cada20-792c-44ca-b255-207cd2252682",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}_{j}} \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}_{j}} \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{w} \\cdot \\mathbf{x}_{i} + (b - y_{i}))^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n",
    "\\frac{2}{n} \\sum_{i=1}^{n} \\mathbf{x}_{i, j} (\\mathbf{w} \\cdot \\mathbf{x}_{i} + (b - y_{i})) + 2 \\lambda \\mathbf{w}_{j} & = 0 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd916db6-df9b-40f1-a223-a2f95d0cf0ca",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{w} \\cdot \\mathbf{x}_{i} + (b - y_{i}))^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{w} \\cdot \\mathbf{x}_{i})^{2} + 2(\\mathbf{w} \\cdot \\mathbf{x}_{i})(b - y_{i}) + (b - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} 2(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{i})\\mathbf{w} + 2\\mathbf{x}_{i}(b - y_{i}) + 2 \\lambda \\mathbf{w} & = 0\\\\\n",
    "\\frac{2 \\mathbf{w}}{n} \\sum_{i=1}^{n} \\mathbf{x}_{i} \\cdot \\mathbf{x}_{i} + \\frac{2b}{n} \\sum_{i=1}^{n} \\mathbf{x}_{i} - \\frac{2}{n} \\sum_{i=1}^{n} \\mathbf{x}_{i} y_{i} + 2 \\lambda \\mathbf{w} & = 0 \\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c0e8c-6e50-49ba-a0d5-ef8c85edd73a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial b} \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} + \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n",
    "\\frac{\\partial}{\\partial b} \\frac{1}{n} \\sum_{i=1}^{n} (b + (\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}))^{2} + \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n",
    "\\frac{\\partial}{\\partial b} \\frac{1}{n} \\sum_{i=1}^{n} b^{2} + 2b(\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}) + (\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i})^{2} + \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} 2b + 2(\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}) & = 0 \\\\\n",
    "2b + \\frac{2}{n} \\sum_{i=1}^{n} （\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}) & = 0 \\\\\n",
    "b & = - \\frac{1}{n} \\sum_{i=1}^{n} （\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369a924-e983-4361-9180-1253dff26b22",
   "metadata": {},
   "source": [
    "## Logistic regression for classification problems\n",
    "---\n",
    "\n",
    "### From regression to classification using sigmoid function\n",
    "\n",
    "How can we use linear regression on a binary classification problem where the labels are 0 and 1?\n",
    "\n",
    "Answer: take the output of a linear regression model and pass it to a **sigmoid** (logistic) function:\n",
    "\n",
    "$$ \\sigma(x) = \\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "Sigmoid function has the following characterstics that are suitable for binary classification\n",
    "1. Sigmoid function maps range $(-\\inf, \\inf)$ to range $(0, 1)$, which can be interpreted as the possibility of being class 1. \n",
    "1. Positive inputs map to output larger than 0.5 and negative inputs map to output less than 0.5. \n",
    "\n",
    "Thus, given an instance $\\mathbf{x}$, the binary output can be derived by setting a threshold $\\theta$ (usually set to $0.5$) to the output of the logistic regression,\n",
    "\n",
    "$$ \\hat{y} = \\sigma(\\mathbf{w}\\mathbf{x} + b) $$\n",
    "\n",
    "$$ \n",
    "\\hat{y}_{\\text{label}} = \n",
    "\\begin{cases}\n",
    "1, & \\hat{y} \\geq \\theta \\\\\n",
    "0, & \\hat{y} < \\theta \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note another commonly used function is **logit** function,\n",
    "\n",
    "$$ \\sigma^{-1}(x) = \\mathrm{logit}(x) = \\log \\frac{x}{1 - x} $$\n",
    "\n",
    "The inverse of the sigmoid function is the logit function, which can be derived by exchange the input and the output of the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x &= \\frac{1}{1 + e^{-y}} \\\\\n",
    "\\frac{1}{x} &= 1 + e^{-y} \\\\\n",
    "e^{-y} &= \\frac{1 - x}{x} \\\\\n",
    "e^{y} &= \\frac{x}{1 - x} \\\\\n",
    "y &= \\log\\frac{x}{1 - x} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d19c0",
   "metadata": {},
   "source": [
    "### Binary cross entropy (log loss) instead of mean squared error \n",
    "\n",
    "Although sigmoid function can work for binary classification problem, it doesn't work quite well with MSE loss. The primary reason is that MSE with sigmoid function is not a convex function anymore. \n",
    "\n",
    "$$ L_{\\text{MSE}} = \\frac{1}{n} \\sum_{i}^{n} (\\frac{1}{1 + e^{-(\\mathbf{w}\\mathbf{x} + b)}} -y_{i})^{2} = \\frac{1}{n} \\sum_{i}^{n} (\\sigma(\\mathbf{w}\\mathbf{x} + b) -y_{i})^{2} $$ \n",
    "\n",
    "To prove a function is convex or not, one way is to see if the second derivative of $L_{\\text{MSE}}$ w.r.t to $\\mathbf{w}$ is positive semidefinite. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_{\\text{MSE}}}{\\partial \\mathbf{w}} & = \\frac{\\partial L_{\\text{MSE}}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial \\mathbf{w}} & \\text{[chain rule]} \\\\\n",
    "& = \\frac{\\partial}{\\partial \\sigma} \\left( \\frac{1}{n} \\sum_{i}^{n} (\\sigma -y_{i})^{2} \\right) \\sigma(1 - \\sigma) \\mathbf{x} & \\text{[$\\sigma' = \\sigma(1 - \\sigma)$]} \\\\\n",
    "& = \\frac{2}{n} \\sum_{i}^{n} (\\sigma - y_{i}) \\sigma(1 - \\sigma) \\mathbf{x} \\\\\n",
    "& = \\frac{2 \\mathbf{x}}{n} \\sum_{i}^{n} \\sigma^{2} - \\sigma^{3} - y_{i}\\sigma - y_{i}\\sigma^{2}  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial^{2} L_{\\text{MSE}}}{\\partial \\mathbf{w}^{2}} & = \\frac{\\partial}{\\partial \\mathbf{w}} \\left( \\frac{2 \\mathbf{x}}{n} \\sum_{i}^{n} \\sigma^{2} - \\sigma^{3} - y_{i}\\sigma - y_{i}\\sigma^{2} \\right) \\\\\n",
    "& = \\frac{\\partial}{\\partial \\sigma} \\left( \\frac{2 \\mathbf{x}}{n} \\sum_{i}^{n} \\sigma^{2} - \\sigma^{3} - y_{i}\\sigma - y_{i}\\sigma^{2} \\right) \\frac{\\partial \\sigma}{\\partial \\mathbf{w}}^{T} \\\\\n",
    "& = \\left( \\frac{2 \\mathbf{x}}{n} \\sum_{i}^{n} 2\\sigma - 3\\sigma^{2} - y_{i} - 2y_{i}\\sigma \\right) \\sigma(1 - \\sigma) \\mathbf{x}^{T} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> TODO: prove the hessian matrix is not positive semidefinite.\n",
    "\n",
    "Thus, instead of MSE, **binary cross entropy** (BCE) loss (log loss) is used with sigmoid to create a convex objective.\n",
    "\n",
    "$$ \\mathrm{BCE} = -\\frac{1}{n}\\sum_{i}^{n}(y_{i}\\log(\\hat{y}_{i}) + (1-y_{i})\\log(1-\\hat{y}))) $$\n",
    "\n",
    "BCE assumes both inputs $y$ and $\\hat{y}$ are in the range $[0, 1]$. Since normally the labels $y_{i}$ are 0 or 1, BCE can be interpreted by decomposing to two cases for each prediction and label pair:\n",
    "\n",
    "$$ \n",
    "\\begin{cases}\n",
    "-\\log(\\hat{y}_{i}) &  y_{i} = 1 \\\\\n",
    "-\\log(1 - \\hat{y}_{i}) & y_{i} = 0 \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcb79d2-6872-4863-8166-098feb362958",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.desmos.com/calculator/ojpmcptvt0?embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f3c103279d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.desmos.com/calculator/ojpmcptvt0?embed\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809fa6f-3487-4ccf-b9c7-d58fc070b711",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "\n",
    "1. https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c\n",
    "1. https://www.cs.toronto.edu/~rgrosse/courses/csc311_f20/readings/notes_on_linear_regression.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
